,Article Title,Author,Journal Title,ISSN,ISBN,Publication Date,Volume,Issue,First Page,Page Count,Accession Number,DOI,Publisher,Doctype,Subjects,Keywords,Abstract,PLink
0,An Empirical Evaluation of Structured Argumentation Using the Toulmin Argument Formalism.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,May-07,37,3,340,8,24858457,10.1109/TSMCA.2007.893488,IEEE,Article,STATISTICAL sampling; PARTICIPATION; SAMPLE size (Statistics); RESEARCH methodology; MATHEMATICAL statistics; Marketing Research and Public Opinion Polling,Argument support systems; structured argumentation tools; Toulmin argument formalism,"Some structured argumentation tools employ the Toulmin argument formalism, but no research has been performed testing this formalism's effect on argument evaluation or communication. An experiment was conducted to address this need by assessing: 1) if the process of generating Toulmin structures impacted participant (re)assessment of the soundness of an argument presented in an article and 2) if other participants thought that the structured representations adequately reflected the written argument. Results were mixed. First, generating Toulmin structures did impact the assessment of argument soundness. This was noteworthy given that participants were professionals representing the population of interest and that a weak manipulation and small sample size were used in the experiment. However, the effect was limited to the article where the argument was poorly aligned with the Toulmin formalism, and second, participants reviewing these structures found them to be less sound than the argument presented in the article itself. More generally, participants did not find it easy to generate Toulmin structures. Greater perceived difficulty in structure generation (and not generation time) was significantly correlated with the amount of change in the participants' soundness ratings, suggesting the mediating role of cognitive effort on reassessment. Generated structures varied greatly. Structures that had more total elements were easier to understand and were given better soundness ratings. These findings suggest that one needs to be cautious of the claimed value of the structured argumentation tools employing the Toulmin formalism without additional empirical research, demonstrating whether and how they can be effective cognitive aids. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24858457&site=ehost-live
1,Confirmation Bias in Complex Analyses.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,May-08,38,3,584,9,31836666,10.1109/TSMCA.2008.918634,IEEE,Article,"EXPERIMENTAL design; COMPUTER systems; HEURISTIC-systematic model (Communication); HEURISTIC programming; FAILURE time data analysis; ANCHORING effect; COMPUTER software; COMPUTER simulation; ANALYSIS of variance; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); Computer systems design and related services (except video game design and development); Computer Systems Design Services",Analysis of competing hypotheses (ACH); anchoring effect; complex analysis; confirmation bias; heuristics and biases,"Most research works investigating the confirmation bias has used abstract experimental tasks where participants drew inferences from just a few items of evidence. The experiment reported in this paper investigated the confirmation bias in a complex analysis task that is more characteristic of law enforcement investigations, financial analysis, and intelligence analysis. Participants were professionals, half of whom had intelligence analysis experience. The effectiveness of a procedure designed to mitigate the confirmation bias, called analysis of competing hypotheses (ACH), was tested. Results showed a confirmation bias for both experience groups, but ACH significantly reduced bias only for participants without intelligence analysis experience. Confirmation bias manifested as a weighting bias, not as an interpretation bias. Participants tended to agree on the interpretation of evidence (i.e., whose hypothesis was supported by the evidence) but tended to disagree on the importance of the evidence-giving more weight to the evidence that supported their preferred hypothesis and less weight to evidence that disconfirmed it. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=31836666&site=ehost-live
2,Confirmation Bias in the Analysis of Remote Sensing Data.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,Jan-09,39,1,218,9,39455810,10.1109/TSMCA.2008.2006372,IEEE,Article,REMOTE sensing; DATA analysis; PREJUDICES; ELECTROMAGNETIC waves; SCIENTIFIC communication; HEURISTIC-systematic model (Communication),Alternative causes approach; confirmation bias; heuristics and biases; remote sensing; technical data analysis,"Analysis of remote sensing data requires a mix of technical data analysis and expert judgment. Although there is considerable empirical evidence that expert judgments reflect substantial biases, the impact of judgment biases in remote sensing and similar types of technical data analyses has not been investigated. In particular, judgment research suggests that experts are prone to a confirmation bias-where focus on a proposed hypothesis leads the expert to seek and overweigh confirming versus disconfirming evidence. In technical data analysis, this predicts a tendency toward false positives in interpretation-concluding that sensor data support a hypothesis when they do not. In this paper, we empirically examine confirmation bias in technical data analysis of remote sensing data, along with an approach to mitigating this bias that systematically promotes consideration of alternative causes in the analysis. Results suggest that analysts do exhibit confirmation bias in their technical data analysis of remote sensing data, and furthermore, that structured consideration of alternative causes mitigates this bias. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=39455810&site=ehost-live
3,Testing the Effectiveness of Icons for Supporting Distributed Team Decision Making Under Time Pressure.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,Mar-04,34,2,179,11,12464409,10.1109/TSMCA.2003.819492,IEEE,Article,DECISION making; TIME pressure; PSYCHOLOGICAL stress; PROBLEM solving; TEAMS in the workplace,Distributed decision making; human-computer interface; team decision making; time pressure.,"There has been minimal experimentation testing the effectiveness of icons (or interface features in general) on distributed team decision making. To overcome this deficiency, an experiment tested the effectiveness of a ""send"" icon to remind team members to send information to their teammates, and a ""receive"" icon to tell them when they had received information, for a simulated, military task. As predicted, the ""send"" icon was effective in maintaining information flow, particularly when time pressure was high and simulated teammates sent less information, because it reduced memory burden and supported proactive behavior. The ""receive"" icon was only effective in supporting decision accuracy when time pressure was low. As time pressure increased, participants' with the ""receive"" icon increasingly used a strategy of making decisions before reading the most important information, completely counter to expectations. These results illustrate the subtle, sometimes surprising way task characteristics (e.g., time pressure) can affect participants' strategies and, thereby, nullify the positive effect of displays on performance. The experiment also examined other task characteristics and working memory capacity, and showed how the lens model equation helped explain all effects on decision accuracy. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12464409&site=ehost-live
4,Using Brunswikian theory and a longitudinal design to study how hierarchical teams adapt to increasing levels of time pressure,Leonard Adelman,Acta Psychologica,16918,,Feb-03,112,2,181,26,8804129,10.1016/S0001-6918(02)00082-3,Elsevier B.V.,Article,DECISION making; TIME pressure,Adaption; Brunswikian theory; Team decision making; Time pressure,"Brunswikian theory and a longitudinal design were used to study how three-person, hierarchical teams adapted to increasing levels of time pressure and, thereby, try to understand why previous team research has not necessarily found a direct relationship between team processes and performance with increasing time pressure. We obtained four principal findings. First, team members initially adapted to increasing time pressure without showing any performance decrements by accelerating their cognitive processing, increasing the amount of their implicit coordination by sending more information without being asked and, to a lesser extent, filtering (omitting) certain activities. Second, teams began and continued to perform the task differently with increasing time pressure, yet often achieved comparable levels of performance. Third, time pressure did affect performance because there was a level of time pressure beyond which performance could not be maintained, although that level differed for different teams. And, fourth, some adaptation strategies were more effective than others at the highest time pressure level. Taken together, these findings support the Brunswikian perspective that one should not necessarily expect a direct relationship between team processes and performance with increasing time pressure because teams adapt their processes in different, yet often equally effective ways, in an effort to maintain high and stable performance. [Copyright &y& Elsevier] Copyright of Acta Psychologica is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=8804129&site=ehost-live
5,Using inferred probabilities to measure the accuracy of imprecise forecasts.,Leonard Adelman,Judgment & Decision Making,19302975,,Nov-12,7,6,728,13,83767746,,Society for Judgment & Decision Making,Article,FORECASTING; METHODOLOGY; CALIBRATION; QUANTITATIVE research; PROBABILITY theory,forecast accuracy; imprecise forecasts; imputed probability; inferred probability; judgment-based forecasting; political forecasting; probability calibration; verbal probability,"Research on forecasting is effectively limited to forecasts that are expressed with clarity; which is to say that the forecasted event must be sufficiently well-defined so that it can be clearly resolved whether or not the event occurred and forecasts certainties are expressed as quantitative probabilities. When forecasts are expressed with clarity, then quantitative measures (scoring rules, calibration, discrimination, etc.) can be used to measure forecast accuracy, which in turn can be used to measure the comparative accuracy of different forecasting methods. Unfortunately most real world forecasts are not expressed clearly. This lack of clarity extends to both the description of the forecast event and to the use of vague language to express forecast certainty. It is thus difficult to assess the accuracy of most real world forecasts, and consequently the accuracy the methods used to generate real world forecasts. This paper addresses this deficiency by presenting an approach to measuring the accuracy of imprecise real world forecasts using the same quantitative metrics routinely used to measure the accuracy of well-defined forecasts. To demonstrate applicability, the inferred probability method is applied to measure the accuracy of forecasts in fourteen documents examining complex political domains. [ABSTRACT FROM AUTHOR] Copyright of Judgment & Decision Making is the property of Society for Judgment & Decision Making and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=83767746&site=ehost-live
6,A Study of Several Classification Algorithms to Predict Students' Learning Performance.,Pouyan Ahmadi,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,2937,10,139581526,,ASEE,Article,,Classification algorithms; Learning Management System; navigational behavior; performance prediction,"Identifying students who need better pedagogical support is an invaluable asset for any academic institution. The main objective of this study is to predict the students' performance and thereby maximize their learning productivity. We focus on the students' past academic performance to predict their future results. This is done by analyzing the various factors of course material and students' online behavior from the Learning Management System (LMS). We also analyze several predictors that contribute to the overall student performance from the data collected. To determine the efficient model that is more accurate and precise, we compare the performance of four well-known machine learning classification algorithms. The 2017 and 2018 academic year data collected consists of user patterns, navigational behavior and the students' daily activities from the LMS, Blackboard (Bb) Learn of the Undergraduate IT program within the Information Sciences and Technology (IST) Department at George Mason University (GMU). This comparison effort will help us confirm the most effective algorithm to identify students' who are at risk of failing a class so that academic advisors/instructors can offer better academic guidance and support. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139581526&site=ehost-live
6,A Study of Several Classification Algorithms to Predict Students' Learning Performance.,Khondkar Islam,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,2937,10,139581526,,ASEE,Article,,Classification algorithms; Learning Management System; navigational behavior; performance prediction,"Identifying students who need better pedagogical support is an invaluable asset for any academic institution. The main objective of this study is to predict the students' performance and thereby maximize their learning productivity. We focus on the students' past academic performance to predict their future results. This is done by analyzing the various factors of course material and students' online behavior from the Learning Management System (LMS). We also analyze several predictors that contribute to the overall student performance from the data collected. To determine the efficient model that is more accurate and precise, we compare the performance of four well-known machine learning classification algorithms. The 2017 and 2018 academic year data collected consists of user patterns, navigational behavior and the students' daily activities from the LMS, Blackboard (Bb) Learn of the Undergraduate IT program within the Information Sciences and Technology (IST) Department at George Mason University (GMU). This comparison effort will help us confirm the most effective algorithm to identify students' who are at risk of failing a class so that academic advisors/instructors can offer better academic guidance and support. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139581526&site=ehost-live
7,Discovering the Top-k Unexplained Sequences in Time-Stamped Observation Data.,Massimiliano Albanese,IEEE Transactions on Knowledge & Data Engineering,10414347,,Mar-14,26,3,577,18,94339027,10.1109/TKDE.2013.33,IEEE,Article,MATHEMATICAL sequences; DATA analysis; APPLICATION software; WEBSITES; INTERNET security; ARTIFICIAL intelligence; KNOWLEDGE base; Software Publishers; Software publishers (except video game publishers); Custom Computer Programming Services; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,artificial intelligence; computing methodologies; knowledge base management; Knowledge representation formalisms and methods,"There are numerous applications where we wish to discover unexpected activities in a sequence of time-stamped observation data--for instance, we may want to detect inexplicable events in transactions at a website or in video of an airport tarmac. In this paper, we start with a known set (\cal A) of activities (both innocuous and dangerous) that we wish to monitor. However, in addition, we wish to identify ""unexplained"" subsequences in an observation sequence that are poorly explained (e.g., because they may contain occurrences of activities that have never been seen or anticipated before, i.e., they are not in (\cal A)). We formally define the probability that a sequence of observations is unexplained (totally or partially) w.r.t. (\cal A). We develop efficient algorithms to identify the top-$(k)$ Totally and partially unexplained sequences w.r.t. (\cal A). These algorithms leverage theorems that enable us to speed up the search for totally/partially unexplained sequences. We describe experiments using real-world video and cyber-security data sets showing that our approach works well in practice in terms of both running time and accuracy. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94339027&site=ehost-live
8,Fast Activity Detection: Indexing for Temporal Stochastic Automaton-Based Activity Models.,Massimiliano Albanese,IEEE Transactions on Knowledge & Data Engineering,10414347,,Feb-13,25,2,360,14,84489405,10.1109/TKDE.2011.246,IEEE,Article,"COMPUTER systems; SECURITY systems; STOCHASTIC analysis; PROBABILISTIC automata; MACHINE theory; ASYNCHRONOUS transfer mode; WEB servers; Security Systems Services (except Locksmiths); Computer systems design and related services (except video game design and development); Computer Systems Design Services; Data Processing, Hosting, and Related Services",Activity detection; Automata; Context; Hidden Markov models; indexing; Monitoring; stochastic automata; Stochastic processes; timestamped data,"Today, numerous applications require the ability to monitor a continuous stream of fine-grained data for the occurrence of certain high-level activities. A number of computerized systems—including ATM networks, web servers, and intrusion detection systems—systematically track every atomic action we perform, thus generating massive streams of timestamped observation data, possibly from multiple concurrent activities. In this paper, we address the problem of efficiently detecting occurrences of high-level activities from such interleaved data streams. A solution to this important problem would greatly benefit a broad range of applications, including fraud detection, video surveillance, and cyber security. There has been extensive work in the last few years on modeling activities using probabilistic models. In this paper, we propose a temporal probabilistic graph so that the elapsed time between observations also plays a role in defining whether a sequence of observations constitutes an activity. We first propose a data structure called “temporal multiactivity graph” to store multiple activities that need to be concurrently monitored. We then define an index called Temporal Multiactivity Graph Index Creation (tMAGIC) that, based on this data structure, examines and links observations as they occur. We define algorithms for insertion and bulk insertion into the tMAGIC index and show that this can be efficiently accomplished. We also define algorithms to solve two problems: the “evidence” problem that tries to find all occurrences of an activity (with probability over a threshold) within a given sequence of observations, and the “identification” problem that tries to find the activity that best matches a sequence of observations. We introduce complexity reducing restrictions and pruning strategies to make the problem—which is intrinsically exponential—linear to the number of observations. Our experiments confirm that tMAGIC has time and space complexity linear to the size of the input, and can efficiently retrieve instances of the monitored activities. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=84489405&site=ehost-live
9,How the Ocean Personality Model Affects the Perception of Crowds.,Jan Allbeck,IEEE Computer Graphics & Applications,2721716,,5/1/11,31,3,22,0,60216479,10.1109/MCG.2009.105,IEEE,Article,COMPUTER simulation; COMPUTER graphics; DIGITAL image processing; GRAPHIC arts; COMPUTER-generated imagery,Animation; Automation; autonomous agents; Computational modeling; computer graphics; Computer simulation; crowd simulation; graphics and multimedia; Humans; Information science; Ocean personality model; Oceans; Psychology; Telephony,"Most crowd simulators animate homogeneous crowds but include underlying parameters that users can tune to create variations in the crowd. However, these parameters are specific to the crowd models and might be difficult for animators or naïve users to use. A proposed approach maps these parameters to personality traits. It extends the HiDAC (High-Density Autonomous Crowds) system by providing each agent with a personality model based on the Ocean (openness, conscientiousness, extroversion, agreeableness, and neuroticism) personality model. Each trait has an associated nominal behavior. Specifying an agent's personality leads to automation of low-level parameter tuning. User studies validated the mapping by assessing users' perception of the traits in animations that illustrate such behaviors. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Computer Graphics & Applications is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60216479&site=ehost-live
10,Applying data redundancy to differential equation solvers.,Paul Ammann,Annals of Software Engineering,10227091,,1997,4,1,65,13,9803764,10.1023/A:1018958526071,Springer Nature,Article,REDUNDANCY in engineering; AUTOMATIC data collection systems; INFORMATION services; SOFTWARE engineering; SOFTWARE productivity; COMPUTER programming; SOFTWARE compatibility; COMPUTER software development; EQUATIONS; Custom Computer Programming Services; Computer systems design and related services (except video game design and development); All Other Information Services; Other Computer Related Services,,"Data redundancy methods evaluate the output of a program on a given input by examining the outputs produced by the same program on additional inputs. This papers explores the use of data redundancy to detect and/or tolerate failures in differential equation solvers. Our first goal is to show that data redundancy techniques are applicable to a wide class of differential equations. Our second task is to identify circumstances in which an independence model of the sort used in program checking can be exploited to build highly reliable solvers from moderately reliable components. We conclude with illustrative examples of applying various data redundancy techniques to a standard differential equation solver. The method has potential for critical systems in which the application's control laws are specified as sets of differential equations. [ABSTRACT FROM AUTHOR] Copyright of Annals of Software Engineering is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=9803764&site=ehost-live
11,Can-Follow Concurrency Control.,Paul Ammann,IEEE Transactions on Computers,189340,,Oct-07,56,10,1425,6,26820551,10.1109/TC.2007.70761,IEEE,Article,DATABASES; ELECTRONIC information resources; INTEGRATED software; INFORMATION science; INFORMATION technology; ANALYSIS of variance; ELECTRICAL engineering; NUMERICAL analysis; MATHEMATICAL analysis; Engineering Services,Can-follow; concurrency control; transaction processing,"Can-follow concurrency control permits a transaction to read (write) an item write-locked (read-locked) by another transaction with almost no delays. By combining the merits of 2PL and 2V2PL, this approach mitigates the lock contention not Only between update and read-only transactions, but also between update and update transactions. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26820551&site=ehost-live
12,The Effect of Imperfect Error Detection on Reliability Assessment via Life Testing.,Paul Ammann,IEEE Transactions on Software Engineering,985589,,Feb-94,20,2,142,7,14281380,10.1109/32.265635,IEEE,Article,"COMPUTER software; COMPUTER systems; ELECTRONIC systems; COMPUTERS; COMPUTER programming; Computer and software stores; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Software publishers (except video game publishers); Electronics Stores; Computer and peripheral equipment manufacturing; Electronic Computer Manufacturing; Other Computer Related Services; Custom Computer Programming Services; Computer systems design and related services (except video game design and development); Computer Systems Design Services",Error detection; software reliability assessment; software testing; test oracles,"Measurement of software reliability by life testing involves executing the software on large numbers of test cases and recording the results. The number of failures observed is used to bound the failure probability even if the number of failures observed is zero. Typical analyses assume that all failures that occur are observed, but, in practice, failures occur without being observed. In this paper, we examine the effect of imperfect error detection, i.e., the situation in which a failure of the software may not be observed. If a conventional analysis associated with life testing is used, the confidence in the bound on the failure probability is optimistic. Our results show that imperfect error detection does not necessarily limit the ability of life testing to bound the probability of failure to the very low values required in critical systems. However, we show that the confidence level associated with a bound on failure probability cannot necessarily be made as high as desired, unless very strong assumptions are made about the error detection mechanism. Such assumptions are unlikely to be met in practice, and so life testing is likely to be useful only for situations in which very high confidence levels are not required. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14281380&site=ehost-live
13,TRUSTED RECOVERY.,Paul Ammann,Communications of the ACM,10782,,Jul-99,42,7,71,5,12513677,10.1145/306549.306580,Association for Computing Machinery,Article,INFORMATION warfare; INFORMATION technology; COMPUTER security; COMPUTER hackers; COMPUTER networks; COUNTERTERRORISM; CYBERTERRORISM; Computer Systems Design Services,,"This article emphasizes that though prevention and detection get much attention but recovery is also an equally important phase of information warfare defense. Prevention is just one phase of information warfare process and it might be detrimental for a system if it neglects an equally important recovery phase. To protect a system against information warfare, it is of course necessary to take steps to prevent attacks from succeeding. At the same time, however, it is important to recognize that not all attacks can be averted at the outset. The goal of defense is to keep available as many of the critical system elements as possible in the face of information warfare attacks. It is undesirable to use recovery techniques that require halting system operations for repair, for denial of service may be the attacker's objective, especially if it occurs at a critical time. Once a bad system element has been detected, it is essential to proceed quickly with repairs while allowing applications to continue operating even if some of the elements have been damaged by an attack.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12513677&site=ehost-live
14,A Comparative Computer Simulation of Dendritic Morphology.,Giorgio Ascoli,PLoS Computational Biology,1553734X,,Jun-08,4,6,1,15,45123017,10.1371/journal.pcbi.1000089,Public Library of Science,Article,DENDRITIC cells; MORPHOMETRICS; CELLULAR mechanics; RELATIONSHIP quality; ONTOGENY; MORPHOLOGY,,"Computational modeling of neuronal morphology is a powerful tool for understanding developmental processes and structure-function relationships. We present a multifaceted approach based on stochastic sampling of morphological measures from digital reconstructions of real cells. We examined how dendritic elongation, branching, and taper are controlled by three morphometric determinants: Branch Order, Radius, and Path Distance from the soma. Virtual dendrites were simulated starting from 3,715 neuronal trees reconstructed in 16 different laboratories, including morphological classes as diverse as spinal motoneurons and dentate granule cells. Several emergent morphometrics were used to compare real and virtual trees. Relating model parameters to Branch Order best constrained the number of terminations for most morphological classes, except pyramidal cell apical trees, which were better described by a dependence on Path Distance. In contrast, bifurcation asymmetry was best constrained by Radius for apical, but Path Distance for basal trees. All determinants showed similar performance in capturing total surface area, while surface area asymmetry was best determined by Path Distance. Grouping by other characteristics, such as size, asymmetry, arborizations, or animal species, showed smaller differences than observed between apical and basal, pointing to the biological importance of this separation. Hybrid models using combinations of the determinants confirmed these trends and allowed a detailed characterization of morphological relations. The differential findings between morphological groups suggest different underlying developmental mechanisms. By comparing the effects of several morphometric determinants on the simulation of different neuronal classes, this approach sheds light on possible growth mechanism variations responsible for the observed neuronal diversity. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=45123017&site=ehost-live
15,A comprehensive knowledge base of synaptic electrophysiology in the rodent hippocampal formation.,Giorgio Ascoli,Hippocampus,10509631,,Apr-20,30,4,314,18,142500326,10.1002/hipo.23148,"John Wiley & Sons, Inc.",Article,SCIENTIFIC literature; KNOWLEDGE base; ELECTROPHYSIOLOGY; RODENTS; MEMBRANE potential; All other miscellaneous animal production,circuit biophysics; computational biology; information storage and retrieval; knowledge bases; models; neuron types; synapses/physiology,"The cellular and synaptic architecture of the rodent hippocampus has been described in thousands of peer‐reviewed publications. However, no human‐ or machine‐readable public catalog of synaptic electrophysiology data exists for this or any other neural system. Harnessing state‐of‐the‐art information technology, we have developed a cloud‐based toolset for identifying empirical evidence from the scientific literature pertaining to synaptic electrophysiology, for extracting the experimental data of interest, and for linking each entry to relevant text or figure excerpts. Mining more than 1,200 published journal articles, we have identified eight different signal modalities quantified by 90 different methods to measure synaptic amplitude, kinetics, and plasticity in hippocampal neurons. We have designed a data structure that both reflects the differences and maintains the existing relations among experimental modalities. Moreover, we mapped every annotated experiment to identified potential connections, that is, specific pairs of presynaptic and postsynaptic neuron types. To this aim, we leveraged Hippocampome.org, an open‐access knowledge base of morphologically, electrophysiologically, and molecularly characterized neuron types in the rodent hippocampal formation. Specifically, we have implemented a computational pipeline to systematically translate neuron type properties into formal queries in order to find all compatible potential connections. With this system, we have collected nearly 40,000 synaptic data entities covering 88% of the 3,120 potential connections in Hippocampome.org. Correcting membrane potentials with respect to liquid junction potentials significantly reduced the difference between theoretical and experimental reversal potentials, thereby enabling the accurate conversion of all synaptic amplitudes to conductance. This data set allows for large‐scale hypothesis testing of the general rules governing synaptic signals. To illustrate these applications, we confirmed several expected correlations between synaptic measurements and their covariates while suggesting previously unreported ones. We release all data open‐source at Hippocampome.org in order to further research across disciplines. [ABSTRACT FROM AUTHOR] Copyright of Hippocampus is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142500326&site=ehost-live
16,A Neural Mechanism for Background Information-Gated Learning Based on Axonal-Dendritic Overlaps.,Giorgio Ascoli,PLoS Computational Biology,1553734X,,Mar-15,11,3,1,17,101836220,10.1371/journal.pcbi.1004155,Public Library of Science,Article,"SYNAPSES; NEURAL transmission; NEUROPLASTICITY; COMPUTATIONAL biology; COMPUTER simulation of biological systems; ARTIFICIAL neural networks; Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Research Article,"Experiencing certain events triggers the acquisition of new memories. Although necessary, however, actual experience is not sufficient for memory formation. One-trial learning is also gated by knowledge of appropriate background information to make sense of the experienced occurrence. Strong neurobiological evidence suggests that long-term memory storage involves formation of new synapses. On the short time scale, this form of structural plasticity requires that the axon of the pre-synaptic neuron be physically proximal to the dendrite of the post-synaptic neuron. We surmise that such “axonal-dendritic overlap” (ADO) constitutes the neural correlate of background information-gated (BIG) learning. The hypothesis is based on a fundamental neuroanatomical constraint: an axon must pass close to the dendrites that are near other neurons it contacts. The topographic organization of the mammalian cortex ensures that nearby neurons encode related information. Using neural network simulations, we demonstrate that ADO is a suitable mechanism for BIG learning. We model knowledge as associations between terms, concepts or indivisible units of thought via directed graphs. The simplest instantiation encodes each concept by single neurons. Results are then generalized to cell assemblies. The proposed mechanism results in learning real associations better than spurious co-occurrences, providing definitive cognitive advantages. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101836220&site=ehost-live
17,Augmenting Weak Semantic Cognitive Maps with an ‘‘Abstractness’’ Dimension,Giorgio Ascoli,Computational Intelligence & Neuroscience,16875265,,2013,,,1,10,97055574,10.1155/2013/308176,Hindawi Limited,Article,,,"The emergent consensus on dimensional models of sentiment, appraisal, emotions, and values is on the semantics of the principal dimensions, typically interpreted as valence, arousal, and dominance. The notion of weak semantic maps was introduced recently as distribution of representations in abstract spaces that are not derived from human judgments, psychometrics, or any other a priori information about their semantics. Instead, they are defined entirely by binary semantic relations among representations, such as synonymy and antonymy. An interesting question concerns the ability of the antonymy-based semantic maps to capture all “universal” semantic dimensions. The present work shows that those narrow weak semantic maps are not complete in this sense and can be augmented with other semantic relations. Specifically, including hyponym-hypernym relations yields a new semantic dimension of the map labeled here “abstractness” (or ontological generality) that is not reducible to any dimensions represented by antonym pairs or to traditional affective space dimensions. It is expected that including other semantic relations (e.g., meronymy/holonymy) will also result in the addition of new semantic dimensions to the map. These findings have broad implications for automated quantitative evaluation of the meaning of text and may shed light on the nature of human subjective experience. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence & Neuroscience is the property of Hindawi Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=97055574&site=ehost-live
18,Effects of Synaptic Synchrony on the Neuronal Input-Output Relationship.,Giorgio Ascoli,Neural Computation,8997667,,Jul-08,20,7,1717,15,32132585,10.1162/neco.2008.10-06-385,MIT Press,Article,NEURONS; NERVOUS system; NEURAL transmission; SYNAPSES; NEURAL circuitry,,"The firing rate of individual neurons depends on the firing frequency of their distributed synaptic inputs, with linear and nonlinear relations subserving different computational functions. This letter explores the relationship between the degree of synchrony among excitatory synapses and the linearity of the response using detailed compartmental models of cortical pyramidal cells. Synchronous input resulted in a linear input output relationship, while asynchronous stimulation yielded sub- and supra proportional outputs at low and high frequencies, respectively. The dependence of input-output linearity on synchrony was sigmoidal and considerably robust with respect to dendritic location, stimulus irregularity, and alteration of active and synaptic properties. Moreover, synchrony affected firing rate differently at lower and higher input frequencies. A reduced integrate-and-fire model suggested a mechanism explaining these results based on spatiotemporal integration, with fundamental implications relating synchrony to memory encoding. [ABSTRACT FROM AUTHOR] Copyright of Neural Computation is the property of MIT Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=32132585&site=ehost-live
19,Efficient metadata mining of web-accessible neural morphologies.,Giorgio Ascoli,Progress in Biophysics & Molecular Biology,796107,,Jan-22,168,,94,9,154660703,10.1016/j.pbiomolbio.2021.05.005,Elsevier B.V.,Article,METADATA; FEATURE extraction; CELL analysis; VIRTUAL communities; MORPHOLOGY; Internet Publishing and Broadcasting and Web Search Portals,Application programming interface; Digital reconstructions; Glia; Morphometry; NeuroMorpho.Org; Neuron; Summary reporting,"Advancements in neuroscience research have led to steadily accelerating data production and sharing. The online community repository of neural reconstructions NeuroMorpho.Org grew from fewer than 1000 digitally traced neurons in 2006 to more than 140,000 cells today, including glia that now constitute 10.1% of the content. Every reconstruction consists of a detailed 3D representation of branch geometry and connectivity in a standardized format, from which a collection of morphometric features is extracted and stored. Moreover, each entry in the database is accompanied by rich metadata annotation describing the animal subject, anatomy, and experimental details. The rapid expansion of this resource in the past decade was accompanied by a parallel rise in the complexity of the available information, creating both opportunities and challenges for knowledge mining. Here, we introduce a new summary reporting functionality, allowing NeuroMorpho.Org users to efficiently download digests of metadata and morphometrics from multiple groups of similar cells for further analysis. We demonstrate the capabilities of the tool for both glia and neurons and present an illustrative statistical analysis of the resulting data. [ABSTRACT FROM AUTHOR] Copyright of Progress in Biophysics & Molecular Biology is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154660703&site=ehost-live
20,Explorers of the cells: Toward cross-platform knowledge integration to evaluate neuronal function.,Giorgio Ascoli,Neuron,8966273,,Nov-21,109,22,3535,3,153525357,10.1016/j.neuron.2021.10.025,Cell Press,Article,EXPLORERS; NEURONS; OPTOGENETICS; OSCILLATIONS,cell type; circuit mapping; extracellular recordings; ground truth; machine-readable; optogenetics; oscillations,"In this issue of Neuron , Petersen et al. (2021) introduce CellExplorer, an open-source tool to integrate neurophysiological metrics of neuronal activity from circuits to behavior. Together with other neuroinformatic resources, it may facilitate community-based multidisciplinary characterization of brain cell types. [ABSTRACT FROM AUTHOR] Copyright of Neuron is the property of Cell Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153525357&site=ehost-live
21,In search of a periodic table of the neurons: Axonal-dendritic circuitry as the organizing principle.,Giorgio Ascoli,BioEssays,2659247,,Oct-16,38,10,969,8,118247220,10.1002/bies.201600067,"John Wiley & Sons, Inc.",Article,NEURONS; NEURAL circuitry; AXONAL transport; DENDRITIC cells; MOLECULAR neurobiology,axons; circuits; classification; dendrites; neurons,"No one knows yet how to organize, in a simple yet predictive form, the knowledge concerning the anatomical, biophysical, and molecular properties of neurons that are accumulating in thousands of publications every year. The situation is not dissimilar to the state of Chemistry prior to Mendeleev's tabulation of the elements. We propose that the patterns of presence or absence of axons and dendrites within known anatomical parcels may serve as the key principle to define neuron types. Just as the positions of the elements in the periodic table indicate their potential to combine into molecules, axonal and dendritic distributions provide the blueprint for network connectivity. Furthermore, among the features commonly employed to describe neurons, morphology is considerably robust to experimental conditions. At the same time, this core classification scheme is suitable for aggregating biochemical, physiological, and synaptic information. [ABSTRACT FROM AUTHOR] Copyright of BioEssays is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=118247220&site=ehost-live
22,Large scale similarity search across digital reconstructions of neural morphology.,Giorgio Ascoli,Neuroscience Research,1680102,,Aug-22,181,,39,7,157523232,10.1016/j.neures.2022.05.004,Elsevier B.V.,Article,"PRINCIPAL components analysis; MORPHOLOGY; NERVOUS system; COMMUNITY development; Administration of Urban Planning and Community and Rural Development; Other local, municipal and regional public administration",Neuroinformatics; Neuronal Morphology; Principal Component Analysis; Similarity search; Software as a Service,"Most functions of the nervous system depend on neuronal and glial morphology. Continuous advances in microscopic imaging and tracing software have provided an increasingly abundant availability of 3D reconstructions of arborizing dendrites, axons, and processes, allowing their detailed study. However, efficient, large-scale methods to rank neural morphologies by similarity to an archetype are still lacking. Using the NeuroMorpho.Org database, we present a similarity search software enabling fast morphological comparison of hundreds of thousands of neural reconstructions from any species, brain regions, cell types, and preparation protocols. We compared the performance of different morphological measurements: 1) summary morphometrics calculated by L -Measure, 2) persistence vectors, a vectorized descriptor of branching structure, 3) the combination of the two. In all cases, we also investigated the impact of applying dimensionality reduction using principal component analysis (PCA). We assessed qualitative performance by gauging the ability to rank neurons in order of visual similarity. Moreover, we quantified information content by examining explained variance and benchmarked the ability to identify occasional duplicate reconstructions of the same specimen. We also compared two different methods for selecting the number of principal components using this benchmark. The results indicate that combining summary morphometrics and persistence vectors with applied PCA using maximum likelihood based automatic dimensionality selection provides an information rich characterization that enables efficient and precise comparison of neural morphology. We have deployed the similarity search as open-source online software both through a user-friendly graphical interface and as an API for programmatic access. [Display omitted] • Implemented similarity search enables fast comparison of neuronal reconstructions. • Our method combines summary morphometrics with descriptors of branching structure. • We evaluated visual similarity, information content, and duplicate detection ability. • Similarity search provided both as GUI and API for NeuroMorpho.Org database. • Code and application are released open source for further community development. [ABSTRACT FROM AUTHOR] Copyright of Neuroscience Research is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157523232&site=ehost-live
23,Mobilizing the base of neuroscience data: the case of neuronal morphologies.,Giorgio Ascoli,Nature Reviews Neuroscience,1471003X,,Apr-06,7,4,318,7,20656977,10.1038/nrn1885,Springer Nature,Article,"NEUROSCIENCES; BIOINFORMATICS; COMPUTERS in biology; MORPHOLOGY; MEDICAL sciences; Research and Development in Biotechnology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"Despite the explosive growth of bioinformatics, data sharing has not yet become routine in neuroscience, possibly because of several broad-spanning issues, from data heterogeneity to privacy regulations. We present the case of neuronal morphology as an ideal example of shareable data. Drawing from recent experience, we argue that the tremendous research potential of existing (and largely unused) digital reconstructions should diffuse any reticence to sharing this type of data. [ABSTRACT FROM AUTHOR] Copyright of Nature Reviews Neuroscience is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20656977&site=ehost-live
24,Petilla terminology: nomenclature of features of GABAergic interneurons of the cerebral cortex.,Giorgio Ascoli,Nature Reviews Neuroscience,1471003X,,Jul-08,9,7,557,12,32724001,10.1038/nrn2402,Springer Nature,journal article,NEUROSCIENCES; CLASSIFICATION; NEURONS; GABA; CEREBRAL cortex; INTERNEURONS; NERVOUS system; ACTION potentials,,"Neuroscience produces a vast amount of data from an enormous diversity of neurons. A neuronal classification system is essential to organize such data and the knowledge that is derived from them. Classification depends on the unequivocal identification of the features that distinguish one type of neuron from another. The problems inherent in this are particularly acute when studying cortical interneurons. To tackle this, we convened a representative group of researchers to agree on a set of terms to describe the anatomical, physiological and molecular features of GABAergic interneurons of the cerebral cortex. The resulting terminology might provide a stepping stone towards a future classification of these complex and heterogeneous cells. Consistent adoption will be important for the success of such an initiative, and we also encourage the active involvement of the broader scientific community in the dynamic evolution of this project. [ABSTRACT FROM AUTHOR] Copyright of Nature Reviews Neuroscience is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=32724001&site=ehost-live
25,Principal Semantic Components of Language and the Measurement of Meaning.,Giorgio Ascoli,PLoS ONE,19326203,,2010,5,6,1,17,52729557,10.1371/journal.pone.0010921,Public Library of Science,Article,SEMANTICS; MEANING (Philosophy); LATENT semantic analysis; METRIC system; LANGUAGE & languages; SYNONYMS,,"Metric systems for semantics, or semantic cognitive maps, are allocations of words or other representations in a metric space based on their meaning. Existing methods for semantic mapping, such as Latent Semantic Analysis and Latent Dirichlet Allocation, are based on paradigms involving dissimilarity metrics. They typically do not take into account relations of antonymy and yield a large number of domain-specific semantic dimensions. Here, using a novel self-organization approach, we construct a low-dimensional, context-independent semantic map of natural language that represents simultaneously synonymy and antonymy. Emergent semantics of the map principal components are clearly identifiable: the first three correspond to the meanings of ''good/bad'' (valence), ''calm/excited'' (arousal), and ''open/closed'' (freedom), respectively. The semantic map is sufficiently robust to allow the automated extraction of synonyms and antonyms not originally in the dictionaries used to construct the map and to predict connotation from their coordinates. The map geometric characteristics include a limited number (∼4) of statistically significant dimensions, a bimodal distribution of the first component, increasing kurtosis of subsequent (unimodal) components, and a U-shaped maximum-spread planar projection. Both the semantic content and the main geometric features of the map are consistent between dictionaries (Microsoft Word and Princeton's WordNet), among Western languages (English, French, German, and Spanish), and with previously established psychometric measures. By defining the semantics of its dimensions, the constructed map provides a foundational metric system for the quantitative analysis of word meaning. Language can be viewed as a cumulative product of human experiences. Therefore, the extracted principal semantic dimensions may be useful to characterize the general semantic dimensions of the content of mental states. This is a fundamental step toward a universal metric system for semantics of human experiences, which is necessary for developing a rigorous science of the mind. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=52729557&site=ehost-live
26,"Quantitative Investigations of Axonal and Dendritic Arbors: Development, Structure, Function, and Pathology.",Giorgio Ascoli,Neuroscientist,10738584,,Jun-15,21,3,241,14,103004899,10.1177/1073858414540216,Sage Publications Inc.,Article,AXONS; DENDRITIC cells; NEUROSCIENCES; NEURAL circuitry; NEUROPLASTICITY; IMAGE reconstruction,data sharing; database; neuron morphology; quantitative analysis; three-dimensional reconstructions,"The branching structures of neurons are a long-standing focus of neuroscience. Axonal and dendritic morphology affect synaptic signaling, integration, and connectivity, and their diversity reflects the computational specialization of neural circuits. Altered neuronal morphology accompanies functional changes during development, experience, aging, and disease. Technological improvements continuously accelerate high-throughput tissue processing, image acquisition, and morphological reconstruction. Digital reconstructions of neuronal morphologies allow for complex quantitative analyses that are unattainable from raw images or two-dimensional tracings. Furthermore, digitized morphologies enable computational modeling of biophysically realistic neuronal dynamics. Additionally, reconstructions generated to address specific scientific questions have the potential for continued investigations beyond the original reason for their acquisition. Facilitating multiple reuse are repositories like NeuroMorpho.Org, which ease the sharing of reconstructions. Here, we review selected scientific literature reporting the reconstruction of axonal or dendritic morphology with diverse goals including establishment of neuronal identity, examination of physiological properties, and quantification of developmental or pathological changes. These reconstructions, deposited in NeuroMorpho.Org, have since been used by other investigators in additional research, of which we highlight representative examples. This cycle of data generation, analysis, sharing, and reuse reveals the vast potential of digital reconstructions in quantitative investigations of neuronal morphology. [ABSTRACT FROM AUTHOR] Copyright of Neuroscientist is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103004899&site=ehost-live
27,Schematic memory persistence and transience for efficient and robust continual learning.,Giorgio Ascoli,Neural Networks,8936080,,Dec-21,144,,49,12,153338272,10.1016/j.neunet.2021.08.011,Elsevier B.V.,Article,LEARNING; ARTIFICIAL intelligence; MEMORY; HUMAN-artificial intelligence interaction; DECISION making; RECOLLECTION (Psychology),Continual learning; Deep learning; Deep neural networks; Memory efficiency; Robustness; Schematic memory,"Continual learning is considered a promising step toward next-generation Artificial Intelligence (AI), where deep neural networks (DNNs) make decisions by continuously learning a sequence of different tasks akin to human learning processes. It is still quite primitive, with existing works focusing primarily on avoiding (catastrophic) forgetting. However, since forgetting is inevitable given bounded memory and unbounded task loads, 'how to reasonably forget' is a problem continual learning must address in order to reduce the performance gap between AIs and humans, in terms of (1) memory efficiency, (2) generalizability, and (3) robustness when dealing with noisy data. To address this, we propose a novel ScheMAtic memory peRsistence and Transience (SMART) 1 1 Code available at: https://github.com/YuyangGao/SMART. framework for continual learning with external memory that builds on recent advances in neuroscience. The efficiency and generalizability are enhanced by a novel long-term forgetting mechanism and schematic memory, using sparsity and 'backward positive transfer' constraints with theoretical guarantees on the error bound. Robust enhancement is achieved using a novel short-term forgetting mechanism inspired by background information-gated learning. Finally, an extensive experimental analysis on both benchmark and real-world datasets demonstrates the effectiveness and efficiency of our model. [ABSTRACT FROM AUTHOR] Copyright of Neural Networks is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153338272&site=ehost-live
28,"Sharing Neuron Data: Carrots, Sticks, and Digital Records.",Giorgio Ascoli,PLoS Biology,15449173,,10/8/15,13,10,1,10,110229849,10.1371/journal.pbio.1002275,Public Library of Science,Article,NEUROSCIENCES; INFORMATION sharing; ELECTRONIC records; DATA recovery; MORPHOLOGY,Community Page,"Routine data sharing is greatly benefiting several scientific disciplines, such as molecular biology, particle physics, and astronomy. Neuroscience data, in contrast, are still rarely shared, greatly limiting the potential for secondary discovery and the acceleration of research progress. Although the attitude toward data sharing is non-uniform across neuroscience subdomains, widespread adoption of data sharing practice will require a cultural shift in the community. Digital reconstructions of axonal and dendritic morphology constitute a particularly “sharable” kind of data. The popularity of the public repository NeuroMorpho.Org demonstrates that data sharing can benefit both users and contributors. Increased data availability is also catalyzing the grassroots development and spontaneous integration of complementary resources, research tools, and community initiatives. Even in this rare successful subfield, however, more data are still unshared than shared. Our experience as developers and curators of NeuroMorpho.Org suggests that greater transparency regarding the expectations and consequences of sharing (or not sharing) data, combined with public disclosure of which datasets are shared and which are not, may expedite the transition to community-wide data sharing. [ABSTRACT FROM AUTHOR] Copyright of PLoS Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=110229849&site=ehost-live
29,Sizing up whole-brain neuronal tracing.,Giorgio Ascoli,Science Bulletin,20959273,,May-22,67,9,883,2,156589129,10.1016/j.scib.2022.01.018,Elsevier B.V.,Article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156589129&site=ehost-live
30,"Spiking neural networks and hippocampal function: A web-accessible survey of simulations, modeling methods, and underlying theories.",Giorgio Ascoli,Cognitive Systems Research,13890417,,Dec-21,70,,80,13,152740258,10.1016/j.cogsys.2021.07.008,Elsevier B.V.,Article,HIPPOCAMPUS (Brain); ARTIFICIAL neural networks; SELF-organizing maps; MINING methodology; KNOWLEDGE base; Remediation Services; Site Preparation Contractors,Computational; Hippocampus; Knowledge base; Modeling; Spiking neural network,"Computational modeling has contributed to hippocampal research in a wide variety of ways and through a large diversity of approaches, reflecting the many advanced cognitive roles of this brain region. The intensively studied neuron type circuitry of the hippocampus is a particularly conducive substrate for spiking neural models. Here we present an online knowledge base of spiking neural network simulations of hippocampal functions. First, we overview theories involving the hippocampal formation in subjects such as spatial representation, learning, and memory. Then we describe an original literature mining process to organize published reports in various key aspects, including: (i) subject area (e.g., navigation, pattern completion, epilepsy); (ii) level of modeling detail (Hodgkin-Huxley, integrate-and-fire, etc.); and (iii) theoretical framework (attractor dynamics, oscillatory interference, self-organizing maps, and others). Moreover, every peer-reviewed publication is also annotated to indicate the specific neuron types represented in the network simulation, establishing a direct link with the Hippocampome.org portal. The web interface of the knowledge base enables dynamic content browsing and advanced searches, and consistently presents evidence supporting every annotation. Moreover, users are given access to several types of statistical reports about the collection, a selection of which is summarized in this paper. This open access resource thus provides an interactive platform to survey spiking neural network models of hippocampal functions, compare available computational methods, and foster ideas for suitable new directions of research. [ABSTRACT FROM AUTHOR] Copyright of Cognitive Systems Research is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152740258&site=ehost-live
31,The Natural Frequency of Human Prospective Memory Increases With Age.,Giorgio Ascoli,Psychology & Aging,8827974,,Jun-15,30,2,209,11,103162680,10.1037/a0038876,American Psychological Association,Article,PROSPECTIVE memory; AGE factors in memory; AUTOBIOGRAPHICAL memory; EPISODIC memory; REGRESSION analysis,autobiographical memory; episodic memory; experience sampling; future-oriented thought; prospective memory,"Autobiographical memory (AM), the recollection of past experiences, and prospective memory (PM), the prospection of future events, are prominent components of subjective life, yet data on the frequencies of their occurrence are limited. Using experience sampling, we quantified the incidence of AM and PM in natural settings among various age groups. Individuals of all ages reported engaging in AM approximately 10% of the time. In contrast, whereas younger subjects recalled PMs as often as they recalled AMs, older subjects experienced PM twice as frequently. AM occurrence was positively correlated with PM occurrence, most strongly among younger individuals. AM and PM durations were also positively correlated and remarkably stable across age groups. Together, these data identify an age-associated shift in the temporal orientation of recollection and quantify the relationship between AM and PM. More broadly, this approach provides a quantitative foundation of AM and PM occurrence, a crucial yet largely unexplored dimension of recollection. [ABSTRACT FROM AUTHOR] Copyright of Psychology & Aging is the property of American Psychological Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103162680&site=ehost-live
32,Topological characterization of neuronal arbor morphology via sequence representation: I - motif analysis.,Giorgio Ascoli,BMC Bioinformatics,14712105,,Jul-15,16,1,1,15,108646013,10.1186/s12859-015-0604-2,BioMed Central,Article,MORPHOLOGY; DENDRITES; AXONS; GENETIC code; BIFURCATION theory,Motif analysis; Neuronal morphology; Tree topology,"Background: The morphology of neurons offers many insights into developmental processes and signal processing. Numerous reports have focused on metrics at the level of individual branches or whole arbors; however, no studies have attempted to quantify repeated morphological patterns within neuronal trees. We introduce a novel sequential encoding of neurite branching suitable to explore topological patterns. Results: Using all possible branching topologies for comparison we show that the relative abundance of short patterns of up to three bifurcations, together with overall tree size, effectively capture the local branching patterns of neurons. Dendrites and axons display broadly similar topological motifs (over-represented patterns) and anti-motifs (underrepresented patterns), differing most in their proportions of bifurcations with one terminal branch and in select sub-sequences of three bifurcations. In addition, pyramidal apical dendrites reveal a distinct motif profile. Conclusions: The quantitative characterization of topological motifs in neuronal arbors provides a thorough description of local features and detailed boundaries for growth mechanisms and hypothesized computational functions. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108646013&site=ehost-live
33,Towards the automatic classification of neurons.,Giorgio Ascoli,Trends in Neurosciences,1662236,,May-15,38,5,307,12,102319063,10.1016/j.tins.2015.02.004,Elsevier B.V.,Article,NEURAL physiology; NEUROSCIENCES; MACHINE learning; METADATA; ACQUISITION of data,big data; machine learning; metadata; neural classification; standardization,"The classification of neurons into types has been much debated since the inception of modern neuroscience. Recent experimental advances are accelerating the pace of data collection. The resulting growth of information about morphological, physiological, and molecular properties encourages efforts to automate neuronal classification by powerful machine learning techniques. We review state-of-the-art analysis approaches and the availability of suitable data and resources, highlighting prominent challenges and opportunities. The effective solution of the neuronal classification problem will require continuous development of computational methods, high-throughput data production, and systematic metadata organization to enable cross-laboratory integration. [ABSTRACT FROM AUTHOR] Copyright of Trends in Neurosciences is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=102319063&site=ehost-live
34,Algorithmic description of hippocampal granule cell dendritic morphology,Giorgio Ascoli,Neurocomputing,9252312,,Jun-05,65-66,,253,8,17796107,10.1016/j.neucom.2004.10.017,Elsevier B.V.,Article,DENDRITIC cells; NEURONS; ANTIGEN presenting cells; IMMUNOCOMPETENT cells; LYMPHOID tissue,3D model; Dentate gyrus; Granule cell; Hidden Markov; Morphometry,"Abstract: Recent efforts in computational neuroanatomy have aimed at accurately reproducing all relevant statistical details of dendritic morphology with stochastic models based on local rules and parameters measured from real neurons. Here we present a solution of this problem for dentate gyrus granule cells based on a hidden Markov algorithm. The correctness of the model is supported by the statistical agreement between distributions of emergent parameters measured from population of traced and virtual neurons. The algorithm relies on two local hidden variables, one of which might be associated with dendritic microtubules, and another may represent the time of development. [Copyright &y& Elsevier] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17796107&site=ehost-live
35,Algorithmic reconstruction of complete axonal arborizations in rat hippocampal neurons,Giorgio Ascoli,Neurocomputing,9252312,,Jun-05,65-66,,15,8,17796077,10.1016/j.neucom.2004.10.105,Elsevier B.V.,Article,AXONAL transport; NEURONS; AXONS; BIOLOGICAL transport,Axon; Digital tracing; Hippocampus; Morphology; Neuroanatomy,"Abstract: Three-dimensional axonal morphologies are not currently available to the neuroscience community in a format suitable for computational modeling. We have designed an algorithm to reconstruct full axonal arborizations based on sets of digitized segments extracted from raw anatomical preparations. We applied this algorithm to eight rat neurons covering the entire synaptic loop of the hippocampal formation (entorhinal cortex, dentate gyrus, CA3, CA1, and subiculum). Since no digital reconstructions of axons have been previously completed for these cell classes, we validated the algorithm by comparing the resulting dendritic arborizations (which were automatically reconstructed along the axons) of the CA1 cells with those available in several public archives. The eight axonal morphologies are quantitatively analyzed and freely distributed in SWC format (http://www.krasnow.gmu.edu/L-Neuron). [Copyright &y& Elsevier] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17796077&site=ehost-live
36,An update to Hippocampome.org by integrating single-cell phenotypes with circuit function in vivo.,Giorgio Ascoli,PLoS Biology,15449173,,5/6/21,19,5,1,28,150169266,10.1371/journal.pbio.3001213,Public Library of Science,Article,ENTORHINAL cortex; PHENOTYPES; PUBLIC records; THETA rhythm; NEURONS,,"Understanding brain operation demands linking basic behavioral traits to cell-type specific dynamics of different brain-wide subcircuits. This requires a system to classify the basic operational modes of neurons and circuits. Single-cell phenotyping of firing behavior during ongoing oscillations in vivo has provided a large body of evidence on entorhinal–hippocampal function, but data are dispersed and diverse. Here, we mined literature to search for information regarding the phase-timing dynamics of over 100 hippocampal/entorhinal neuron types defined in Hippocampome.org. We identified missing and unresolved pieces of knowledge (e.g., the preferred theta phase for a specific neuron type) and complemented the dataset with our own new data. By confronting the effect of brain state and recording methods, we highlight the equivalences and differences across conditions and offer a number of novel observations. We show how a heuristic approach based on oscillatory features of morphologically identified neurons can aid in classifying extracellular recordings of single cells and discuss future opportunities and challenges towards integrating single-cell phenotypes with circuit function. By integrating single-cell firing behavior during in vivo oscillations with morphological and molecular neuron classification in vitro, this study standardizes a framework for interpreting the functional roles of distinct circuit modules and ascribing extracellular recordings to identified neuron types. [ABSTRACT FROM AUTHOR] Copyright of PLoS Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150169266&site=ehost-live
37,"Biomedical research funding: when the game gets tough, winners start to play.",Giorgio Ascoli,BioEssays,2659247,,Sep-07,29,9,933,4,26664021,10.1002/bies.20633,"John Wiley & Sons, Inc.",Article,"MEDICAL research; FINANCE; FEDERAL aid to medical research; RESEARCH funding; GRANTS in aid (Public finance); UNITED States; NATIONAL Institutes of Health (U.S.); Administration of General Economic Programs; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,The article presents an analysis on the biomedical research funding and the National Institutes of Health (NIH) budget trends that influence long-term scientific strategies and career decisions. It was found that the success rate of grant applications submitted for funding is not in favor of the total yearly amount of NIH extramural expenditure. It is inferred that the success rate variability can be explained by an equal but delayed reaction of the number of applications to budget fluctuations.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26664021&site=ehost-live
38,Incorporating anatomically realistic cellular-level connectivity in neural network models of the rat hippocampus,Giorgio Ascoli,Biosystems,3032647,,Jan-05,79,3-Jan,173,9,22230591,10.1016/j.biosystems.2004.09.024,Elsevier B.V.,Article,ARTIFICIAL neural networks; NEUROBIOLOGY; COGNITIVE neuroscience; NEURAL circuitry,Feedforward inhibition; Hippocampus; Neural networks; Neuroanatomy; Synaptology,"Abstract: The specific connectivity patterns among neuronal classes can play an important role in the regulation of firing dynamics in many brain regions. Yet most neural network models are built based on vastly simplified connectivity schemes that do not accurately reflect the biological complexity. Taking the rat hippocampus as an example, we show here that enough quantitative information is available in the neuroanatomical literature to construct neural networks derived from accurate models of cellular connectivity. Computational simulations based on this approach lend themselves to a direct investigation of the potential relationship between cellular connectivity and network activity. We define a set of fundamental parameters to characterize cellular connectivity, and are collecting the related values for the rat hippocampus from published reports. Preliminary simulations based on these data uncovered a novel putative role for feedforward inhibitory neurons. In particular, “mopp” cells in the dentate gyrus are suitable to help maintain the firing rate of granule cells within physiological levels in response to a plausibly noisy input from the entorhinal cortex. The stabilizing effect of feedforward inhibition is further shown to depend on the particular ratio between the relative threshold values of the principal cells and the interneurons. We are freely distributing the connectivity data on which this study is based through a publicly accessible web archive (http://www.krasnow.gmu.edu/L-Neuron). [Copyright &y& Elsevier] Copyright of Biosystems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22230591&site=ehost-live
39,Itinerant complexity in networks of intrinsically bursting neurons.,Giorgio Ascoli,Chaos,10541500,,Jun-20,30,6,1,11,144345634,10.1063/5.0010334,American Institute of Physics,Article,NEURONS; NEURAL circuitry; CIRCUIT complexity; INTERNEURONS; SYNCHRONIC order,,"Active neurons can be broadly classified by their intrinsic oscillation patterns into two classes characterized by spiking or bursting. Here, we show that networks of identical bursting neurons with inhibitory pulsatory coupling exhibit itinerant dynamics. Using the relative phases of bursts between neurons, we numerically demonstrate that the network exhibits endogenous transitions between multiple modes of transient synchrony. This is true even for bursts consisting of two spikes. In contrast, our simulations reveal that networks of identical singlet-spiking neurons do not exhibit such complexity. These results suggest a role for bursting dynamics in realizing itinerant complexity in neural circuits. [ABSTRACT FROM AUTHOR] Copyright of Chaos is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144345634&site=ehost-live
40,Local Control of Postinhibitory Rebound Spiking in CA1 Pyramidal Neuron Dendrites.,Giorgio Ascoli,Journal of Neuroscience,2706474,,5/5/10,30,18,6434,9,50936319,10.1523/JNEUROSCI.4066-09.2010,Society for Neuroscience,Article,BRAIN research; NEURONS; HIPPOCAMPUS (Brain); NEURAL transmission; DENDRITES; RESPONSE inhibition,,"Postinhibitory rebound spiking is characteristic of several neuron types and brain regions, where it sustains spontaneous activity and central pattern generation. However, rebound spikes are rarely observed in the principal cells of the hippocampus under physiological conditions. We report that CA1 pyramidal neurons support rebound spikes mediated by hyperpolarization-activated inward current (Ih), and normally masked by A-type potassium channels (KA). In both experiments and computational models, KA blockage or reduction consistently resulted in a somatic action potential upon release from hyperpolarizing injections in the soma or main apical dendrite. Rebound spiking was systematically abolished by the additional blockage or reduction of Ih. Since the density of both KA and Ih increases in these cells with the distance from the soma, such ""latent"" mechanism may be most effective in the distal dendrites, which are targeted by a variety of GABAergic interneurons. Detailed computer simulations, validated against the experimental data, demonstrate that rebound spiking can result from activation of distal inhibitory synapses. In particular, partial KA reduction confined to one or few branches of the apical tuft may be sufficient to elicit a local spike following a train of synaptic inhibition. Moreover, the spatial extent and amount of KA reduction determines whether the dendritic spike propagates to the soma. These data suggest that the plastic regulation of KA can provide a dynamic switch to unmask postinhibitory spiking in CA1 pyramidal neurons. This newly discovered local modulation of postinhibitory spiking further increases the signal processing power of the CA1 synaptic microcircuitry. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroscience is the property of Society for Neuroscience and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=50936319&site=ehost-live
41,Metrics for comparing neuronal tree shapes based on persistent homology.,Giorgio Ascoli,PLoS ONE,19326203,,8/15/17,12,8,1,24,124613118,10.1371/journal.pone.0182184,Public Library of Science,Article,NEUROANATOMY; HOMOLOGY (Biology); NEURAL circuitry; THEORY of knowledge; ELECTROPHYSIOLOGY,Algebra; Animal cells; Biology and life sciences; Cell biology; Cellular neuroscience; Cellular types; Computer and information sciences; Data management; Dendritic structure; Evolutionary biology; Evolutionary systematics; Geodesics; Geometry; Linear algebra; Mathematics; Neuronal dendrites; Neuronal morphology; Neurons; Neuroscience; Phylogenetic analysis; Phylogenetics; Physical sciences; Research Article; Taxonomy; Topology; Vector spaces,"As more and more neuroanatomical data are made available through efforts such as NeuroMorpho.Org and FlyCircuit.org, the need to develop computational tools to facilitate automatic knowledge discovery from such large datasets becomes more urgent. One fundamental question is how best to compare neuron structures, for instance to organize and classify large collection of neurons. We aim to develop a flexible yet powerful framework to support comparison and classification of large collection of neuron structures efficiently. Specifically we propose to use a topological persistence-based feature vectorization framework. Existing methods to vectorize a neuron (i.e, convert a neuron to a feature vector so as to support efficient comparison and/or searching) typically rely on statistics or summaries of morphometric information, such as the average or maximum local torque angle or partition asymmetry. These simple summaries have limited power in encoding global tree structures. Based on the concept of topological persistence recently developed in the field of computational topology, we vectorize each neuron structure into a simple yet informative summary. In particular, each type of information of interest can be represented as a descriptor function defined on the neuron tree, which is then mapped to a simple persistence-signature. Our framework can encode both local and global tree structure, as well as other information of interest (electrophysiological or dynamical measures), by considering multiple descriptor functions on the neuron. The resulting persistence-based signature is potentially more informative than simple statistical summaries (such as average/mean/max) of morphometric quantities—Indeed, we show that using a certain descriptor function will give a persistence-based signature containing strictly more information than the classical Sholl analysis. At the same time, our framework retains the efficiency associated with treating neurons as points in a simple Euclidean feature space, which would be important for constructing efficient searching or indexing structures over them. We present preliminary experimental results to demonstrate the effectiveness of our persistence-based neuronal feature vectorization framework. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124613118&site=ehost-live
42,Morphological homeostasis in cortical dendrites.,Giorgio Ascoli,Proceedings of the National Academy of Sciences of the United States of America,278424,,1/31/06,103,5,1569,6,20097055,10.1073/pnas.0510057103,National Academy of Sciences,Article,HOMEOSTASIS; PHYSIOLOGICAL control systems; GENETIC regulation; NEUROPLASTICITY; GENE expression; NERVOUS system; NEURONS,dendritic size; neuronal morphometry; pyramidal cells; rat hippocampus,"Neurons have significant potential for the homeostatic regulation of a broad range of functional features, from gene expression to synaptic excitability. In this article, we show that dendritic morphology may also be under intrinsic homeostatic control. We present the results from a statistical analysis of a large collection of digitally reconstructed neurons, demonstrating that fluctuations in dendritic size in one given portion of a neuron are systematically counterbalanced by the remaining dendrites in the same cell. As a result, the total dendritic measure (e.g., number of branches, length, and surface area) of each neuron in a given morphological class is, on average, significantly less random than would be expected if trees (and their parts) were regulated independently during development. This observation is general across scales that range from gross basal/apical subdivisions to individual branches and bifurcations, and its statistical significance is robust among various brain regions, cell types, and experimental conditions. Given the pivotal dendritic role in signal integration, synaptic plasticity, and network connectivity, these findings add a dimension to the functional characterization of neuronal homeostasis. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the National Academy of Sciences of the United States of America is the property of National Academy of Sciences and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20097055&site=ehost-live
43,NeuroMorpho.Org: A Central Resource for Neuronal Morphologies.,Giorgio Ascoli,Journal of Neuroscience,2706474,,8/29/07,27,35,9247,5,26653307,10.1523/JNEUROSCI.2055-07.2007,Society for Neuroscience,Article,ARCHIVES; WEBSITES; INFORMATION resources; NEUROSCIENCES; MORPHOLOGY; INFORMATION science; Libraries and Archives; Archives; Internet Publishing and Broadcasting and Web Search Portals,,"The article focuses on NeuroMorpho.Org, a web-accessible archive of digital reconstructions of neuronal morphology. The resource also contains additional information, including an introduction page, a QuickStart guide and answers to frequently asked questions. It provides several examples of the principal capabilities enabled by the resource. It provides an overview of data in terms of raw file availability and potential applications.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26653307&site=ehost-live
44,Neuronal Morphology Goes Digital: A Research Hub for Cellular and System Neuroscience,Giorgio Ascoli,Neuron,8966273,,Mar-13,77,6,1017,22,86370540,10.1016/j.neuron.2013.03.008,Cell Press,Article,NEURONS; CELL morphology; BRAIN function localization; BRAIN imaging; ELECTROPHYSIOLOGY; COMPUTATIONAL neuroscience; MATHEMATICAL models,,"The importance of neuronal morphology in brain function has been recognized for over a century. The broad applicability of “digital reconstructions” of neuron morphology across neuroscience subdisciplines has stimulated the rapid development of numerous synergistic tools for data acquisition, anatomical analysis, three-dimensional rendering, electrophysiological simulation, growth models, and data sharing. Here we discuss the processes of histological labeling, microscopic imaging, and semiautomated tracing. Moreover, we provide an annotated compilation of currently available resources in this rich research “ecosystem” as a central reference for experimental and computational neuroscience. [ABSTRACT FROM AUTHOR] Copyright of Neuron is the property of Cell Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=86370540&site=ehost-live
45,Normalized unitary synaptic signaling of the hippocampus and entorhinal cortex predicted by deep learning of experimental recordings.,Giorgio Ascoli,Communications Biology,23993642,,5/5/22,5,1,1,19,156802878,10.1038/s42003-022-03329-5,Springer Nature,Article,DEEP learning; ENTORHINAL cortex; HIPPOCAMPUS (Brain); DECAY constants; LATENT variables; COMPUTER simulation; DATA analysis,,"Biologically realistic computer simulations of neuronal circuits require systematic data-driven modeling of neuron type-specific synaptic activity. However, limited experimental yield, heterogeneous recordings conditions, and ambiguous neuronal identification have so far prevented the consistent characterization of synaptic signals for all connections of any neural system. We introduce a strategy to overcome these challenges and report a comprehensive synaptic quantification among all known neuron types of the hippocampal-entorhinal network. First, we reconstructed >2600 synaptic traces from ∼1200 publications into a unified computational representation of synaptic dynamics. We then trained a deep learning architecture with the resulting parameters, each annotated with detailed metadata such as recording method, solutions, and temperature. The model learned to predict the synaptic properties of all 3,120 circuit connections in arbitrary conditions with accuracy approaching the intrinsic experimental variability. Analysis of data normalized and completed with the deep learning model revealed that synaptic signals are controlled by few latent variables associated with specific molecular markers and interrelating conductance, decay time constant, and short-term plasticity. We freely release the tools and full dataset of unitary synaptic values in 32 covariate settings. Normalized synaptic data can be used in brain simulations, and to predict and test experimental hypothesis. A deep learning model trained on roughly 2,600 synaptic traces from hippocampal electrophysiology datasets demonstrates how specific covariates influence synaptic signals. [ABSTRACT FROM AUTHOR] Copyright of Communications Biology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156802878&site=ehost-live
46,"On Synaptic Circuits, Memory, and Kumquats.",Giorgio Ascoli,New England Journal of Medicine,284793,,9/17/15,373,12,1170,3,109469792,10.1056/NEJMcibr1509692,New England Journal of Medicine,journal article,ELECTRON microscopes; NEOCORTEX; SYNAPSES; COGNITION; MEMORY; Analytical Laboratory Instrument Manufacturing,,The article reviews a study that used the electron microscope in capturing nuerotransmitter vesicle in the neocortex of a mouse and the synaptic circuit formation for memory and cognition. Topics include multiple synapses in the cortical region allowing axons to encode the sweer-sour taste and future memories that can be obtained through optical microscopy. Also mentioned are axonal-dendrite overlapping resulting to synaptic formation and recording neuronal activity through optical microscopy.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109469792&site=ehost-live
47,Operations research methods for estimating the population size of neuron types.,Giorgio Ascoli,Annals of Operations Research,2545330,,Jun-20,289,1,33,18,143492539,10.1007/s10479-020-03542-7,Springer Nature,Article,"OPERATIONS research; NEURONS; NUMBERS of species; SCIENTIFIC community; DEFINITIONS; Process, Physical Distribution, and Logistics Consulting Services",Cell census; Constraint optimization; Hippocampus; Neurons,"Understanding brain computation requires assembling a complete catalog of its architectural components. Although the brain is organized into several anatomical and functional regions, it is ultimately the neurons in every region that are responsible for cognition and behavior. Thus, classifying neuron types throughout the brain and quantifying the population sizes of distinct classes in different regions is a key subject of research in the neuroscience community. The total number of neurons in the brain has been estimated for multiple species, but the definition and population size of each neuron type are still open questions even in common model organisms: the so called ""cell census"" problem. We propose a methodology that uses operations research principles to estimate the number of neurons in each type based on available information on their distinguishing properties. Thus, assuming a set of neuron type definitions, we provide a solution to the issue of assessing their relative proportions. Specifically, we present a three-step approach that includes literature search, equation generation, and numerical optimization. Solving computationally the set of equations generated by literature mining yields best estimates or most likely ranges for the number of neurons in each type. While this strategy can be applied towards any neural system, we illustrate its usage on the rodent hippocampus. [ABSTRACT FROM AUTHOR] Copyright of Annals of Operations Research is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143492539&site=ehost-live
48,Passive dendritic integration heavily affects spiking dynamics of recurrent networks,Giorgio Ascoli,Neural Networks,8936080,,Jun-03,16,6-May,657,7,10117884,10.1016/S0893-6080(03)00090-X,Elsevier B.V.,Article,SYNAPSES; BIOLOGICAL neural networks,Asynchronous; Dendritic integration; Neural network model,"According to dendritic cable theory, proximal synapses give rise to inputs with short delay, high amplitude, and short duration. In contrast, inputs from distal synapses have long delays, low amplitude, and long duration. Nevertheless, large scale neural networks are seldom built with realistically layered synaptic architectures and corresponding electrotonic parameters. Here, we use a simple model to investigate the spike response dynamics of networks with different electrotonic structures. The networks consist of a layer of neurons receiving a sparse feedforward projection from a set of inputs, as well as sparse recurrent connections from within the layer. Firing patterns are set in the inputs, and recorded from the neuron (output) layer. The feedforward and recurrent synapses are independently set as proximal or distal, representing dendritic connections near or far from the soma, respectively. Analyses of firing dynamics indicate that recurrent distal synapses tend to concentrate network activity in fewer neurons, while proximal recurrent synapses result in a more homogeneous activity distribution. In addition, when the feedforward input is regular (spiking or bursting) and asynchronous, the output is regular if recurrent synapses are more distal than feedforward ones, and irregular in the opposite configuration. Finally, the amplitude of network fluctuations in response to asynchronous input is lower if feedforward and recurrent synapses are electrotonically distant from one another (in either configuration). In conclusion, electrotonic effects reflecting different dendritic positions of synaptic inputs significantly influence network dynamics. [Copyright &y& Elsevier] Copyright of Neural Networks is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=10117884&site=ehost-live
49,Science of the Conscious Mind.,Giorgio Ascoli,Biological Bulletin,63185,,Dec-08,215,3,204,12,35946370,10.2307/25470706,University of Chicago Press,Article,"CONSCIOUSNESS; ENZYME kinetics; LIFE sciences; NEUROBIOLOGY; PHYSICS; METRIC system; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"Human beings have direct access to their own mental states, but can only indirectly observe cosmic radiation and enzyme kinetics. Why then can we measure the temperature of far away galaxies and the activation constant of kinases to the third digit, yet we only gauge our happiness on a scale from 1 to 7? Here we propose a radical research paradigm shift to embrace the subjective conscious mind into the realm of objective empirical science. Key steps are the axiomatic acceptance of first-person experiences as scientific observables; the definition of a quantitative, reliable metric system based on natural language; and the careful distinction of subjective mental states (e.g., interpretation and intent) from physically measurable sensory and motor behaviors (input and output). Using this approach, we pro- pose a series of reproducible experiments that may help define a still largely unexplored branch of science. We speculate that the development of this new discipline will be initially parallel to, and eventually converging with, neuro-biology and physics. [ABSTRACT FROM AUTHOR] Copyright of Biological Bulletin is the property of University of Chicago Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35946370&site=ehost-live
50,Statistical determinants of dendritic morphology in hippocampal pyramidal neurons: A hidden Markov model.,Giorgio Ascoli,Hippocampus,10509631,,2005,15,2,166,18,64910912,10.1002/hipo.20041,"John Wiley & Sons, Inc.",Article,,,"Dendritic structure is traditionally characterized by distributions and interrelations of morphometric parameters, such as Sholl-like plots of the number of branches versus dendritic path distance. However, how much of a given morphology is effectively captured by any statistical description is generally unknown. In this work, we assemble a small number of standard geometrical parameters measured from experimental data in a simple stochastic algorithm to describe the dendrograms of hippocampal pyramidal cells. The model, consistent with the hidden Markov framework, is feedforward, local, and causal. It relies on two 'hidden' local variables: the expected number of terminal tips in a given subtree, and the current path distance from the soma. The algorithm generates dendrograms that statistically reproduce all morphological essentials of dendrites observed in real neurons, including the distributions of branching and termination points, branch lengths, membrane area, topological asymmetry, and (assuming passive membrane parameters within physiological range) electrotonic characteristics. Thus, this algorithm and the small number of its morphometric parameters constitute a remarkably complete description of the dendrograms of hippocampal pyramidal cells. Specifically, it is found that CA3 and CA1 basal dendrites and CA3 apical dendrites can each be described as homogeneous morphological classes. In contrast, the accurate generation of CA1 apical dendrites necessitates the separate sampling of two types of branches, main and oblique, suggesting their derivations from different developmental mechanisms (terminal and interstitial growth, respectively). We further offer a plausible biophysical interpretation of the model hidden variables, relating them to microtubules and other intracellular resources. © 2004 Wiley-Liss, Inc. [ABSTRACT FROM AUTHOR] Copyright of Hippocampus is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=64910912&site=ehost-live
51,Successful grant fishing in funding droughts.,Giorgio Ascoli,Nature Cell Biology,14657392,,Aug-07,9,8,856,2,25997618,10.1038/ncb0807-856,Springer Nature,Letter,"LETTERS to the editor; MEDICAL research; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,A letter to the editor is presented in response to the article on the role of the National Institutes of Health for biomedical research.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=25997618&site=ehost-live
52,Topological characterization of neuronal arbor morphology via sequence representation: II - global alignment.,Giorgio Ascoli,BMC Bioinformatics,14712105,,2015,16,1,209,17,103690498,10.1186/s12859-015-0605-1,BioMed Central,Article,,Multiple sequence alignment; Neuronal morphology; Sequence alignment; Tree topology,"Background: The increasing abundance of neuromorphological data provides both the opportunity and the challenge to compare massive numbers of neurons from a wide diversity of sources efficiently and effectively. We implemented a modified global alignment algorithm representing axonal and dendritic bifurcations as strings of characters. Sequence alignment quantifies neuronal similarity by identifying branch-level correspondences between trees. Results: The space generated from pairwise similarities is capable of classifying neuronal arbor types as well as, or better than, traditional topological metrics. Unsupervised cluster analysis produces groups that significantly correspond with known cell classes for axons, dendrites, and pyramidal apical dendrites. Furthermore, the distinguishing consensus topology generated by multiple sequence alignment of a group of neurons reveals their shared branching blueprint. Interestingly, the axons of dendritic-targeting interneurons in the rodent cortex associates with pyramidal axons but apart from the (more topologically symmetric) axons of perisomatic-targeting interneurons. Conclusions: Global pairwise and multiple sequence alignment of neurite topologies enables detailed comparison of neurites and identification of conserved topological features in alignment-defined clusters. The methods presented also provide a framework for incorporation of additional branch-level morphological features. Moreover, comparison of multiple alignment with motif analysis shows that the two techniques provide complementary information respectively revealing global and local features. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103690498&site=ehost-live
53,Topological characterization of neuronal arbor morphology via sequence representation: II - global alignment.,Giorgio Ascoli,BMC Bioinformatics,14712105,,Jul-15,16,1,1,17,108646014,10.1186/s12859-015-0605-1,BioMed Central,Article,MORPHOLOGY; AXONS; DENDRITES; SEQUENCE alignment; CLUSTER analysis (Statistics); TOPOLOGY; INTERNEURONS,Multiple sequence alignment; Neuronal morphology; Sequence alignment; Tree topology,"Background: The increasing abundance of neuromorphological data provides both the opportunity and the challenge to compare massive numbers of neurons from a wide diversity of sources efficiently and effectively. We implemented a modified global alignment algorithm representing axonal and dendritic bifurcations as strings of characters. Sequence alignment quantifies neuronal similarity by identifying branch-level correspondences between trees. Results: The space generated from pairwise similarities is capable of classifying neuronal arbor types as well as, or better than, traditional topological metrics. Unsupervised cluster analysis produces groups that significantly correspond with known cell classes for axons, dendrites, and pyramidal apical dendrites. Furthermore, the distinguishing consensus topology generated by multiple sequence alignment of a group of neurons reveals their shared branching blueprint. Interestingly, the axons of dendritic-targeting interneurons in the rodent cortex associates with pyramidal axons but apart from the (more topologically symmetric) axons of perisomatic-targeting interneurons. Conclusions: Global pairwise and multiple sequence alignment of neurite topologies enables detailed comparison of neurites and identification of conserved topological features in alignment-defined clusters. The methods presented also provide a framework for incorporation of additional branch-level morphological features. Moreover, comparison of multiple alignment with motif analysis shows that the two techniques provide complementary information respectively revealing global and local features. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108646014&site=ehost-live
54,A new bursting model of CA3 pyramidal cell physiology suggests multiple locations for spike initiation,Giorgio Ascoli,Biosystems,3032647,,Oct-02,67,3-Jan,129,9,8571713,10.1016/S0303-2647(02)00071-0,Elsevier B.V.,Article,COMPUTER simulation; DENDRITES,Burst; CA3; Computer simulation; Dendritic spikes; Modeling; NEURON,"We introduce a novel computational model of hippocampal pyramidal cells physiology based on an up-to-date, detailed description of passive and active biophysical properties and real dendritic morphology. This model constitutes a modification of a previous (1995) model which included complex calcium dynamics and Na+, K+, and Ca2+ currents. Changes reflect recently acquired experimental knowledge regarding the types and spatial distributions of these currents. The updated model responds to simulated somatic current clamp stimulation with a train of spikes (burst). The shape of the burst reproduces the characteristic behavior observed experimentally, similarly to the previous model. However, an analysis of dendritic membrane voltage distribution during the burst shows that the mechanisms underlying this somatic behavior are dramatically different in the two models. In the previous model, all spikes were generated in the soma and backpropagated in the dendrites. In the updated model, in contrast, only the first spike is initiated somatically. The second somatic spike is preceded by a dendritic spike (triggered by the first spike backpropagation), which propagates both backward and forward, reaching the soma just before the rise of the second somatic spike. The third and fourth spikes are similarly caused by a complex spatio-temporal interplay between somatic and dendritic depolarization. These results suggest that the distribution of ionic currents recently characterized in hippocampal pyramidal cells can support both somatic and dendritic spike initiation. In addition, these simulations demonstrate that models with considerably different distributions of active conductances can reproduce the same experimental bursting behavior with distinct biophysical mechanisms. [Copyright &y& Elsevier] Copyright of Biosystems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=8571713&site=ehost-live
55,A real-scale anatomical model of the dentate gyrus based on single cell reconstructions and 3D rendering of a brain atlas,Giorgio Ascoli,Neurocomputing,9252312,,Jun-02,44-46,,629,6,7826525,10.1016/S0925-2312(02)00450-2,Elsevier B.V.,Article,DENTATE gyrus; NEUROANATOMY,Anatomical reconstruction; Computational neuroanatomy; Dentate gyrus; Granule cells,"As a first step towards the creation of a cellular model of dentate gyrus (DG) anatomy, we distributed 1,000,000 digitized granule cells (gcs) in 3D in a virtual reality reconstruction of Swanson''s brain atlas. DG coronal sections were assembled into 3D surfaces using implicit function generation. The resulting file included hilar, granular, and molecular boundaries. 20,000 replicas of each of 50 reconstructed gcs were added to the model by packing the somata in the appropriate layer and then radially orienting the dendritic tree axes. The model can be used to evaluate stereologic parameters such as dendritic overlap probability, space occupancy, and exposure to incoming fibers. [Copyright &y& Elsevier] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=7826525&site=ehost-live
56,Automated image computing reshapes computational neuroscience.,Giorgio Ascoli,BMC Bioinformatics,14712105,,2013,14,1,1,5,131628226,10.1186/1471-2105-14-293,BioMed Central,Article,,,"We briefly identify several critical issues in current computational neuroscience, and present our opinions on potential solutions based on bioimage informatics, especially automated image computing. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131628226&site=ehost-live
57,"Cell numbers, distribution, shape, and regional variation throughout the murine hippocampal formation from the adult brain Allen Reference Atlas.",Giorgio Ascoli,Brain Structure & Function,18632653,,Nov-19,224,8,2883,15,138982611,10.1007/s00429-019-01940-7,Springer Nature,Article,"HIPPOCAMPUS (Brain); ENTORHINAL cortex; DENTATE gyrus; IMAGE recognition (Computer vision); ATLASES; IMAGE processing; Book, periodical and newspaper merchant wholesalers; Other printing; Commercial Printing (except Screen and Books); One-Hour Photofinishing; Photofinishing Laboratories (except One-Hour)",Cell counts; Hippocampus; Image analysis; Mouse,"Quantifying the distribution of cells in every brain region is fundamental to attaining a comprehensive census of distinct neuronal and glial types. Until recently, estimating neuron numbers involved time-consuming procedures that were practically limited to stereological sampling. Progress in open-source image recognition software, growth in computing power, and unprecedented neuroinformatics developments now offer the potentially paradigm-shifting alternative of comprehensive cell-by-cell analysis in an entire brain region. The Allen Brain Atlas provides free digital access to complete series of raw Nissl-stained histological section images along with regional delineations. Automated cell segmentation of these data enables reliable and reproducible high-throughput quantification of regional variations in cell count, density, size, and shape at whole-system scale. While this strategy is directly applicable to any regions of the mouse brain, we first deploy it here on the closed-loop circuit of the hippocampal formation: the medial and lateral entorhinal cortices; dentate gyrus (DG); areas Cornu Ammonis 3 (CA3), CA2, and CA1; and dorsal and ventral subiculum. Using two independent image processing pipelines and the adult mouse reference atlas, we report the first cellular-level soma segmentation in every sub-region and non-principal layer of the left hippocampal formation through the full rostral-caudal extent. It is important to note that our techniques excluded the layers with the largest number of cells, DG granular and CA pyramidal, due to dense packing. The numerical estimates for the remaining layers are corroborated by traditional stereological sampling on a data subset and well match sparse published reports. [ABSTRACT FROM AUTHOR] Copyright of Brain Structure & Function is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138982611&site=ehost-live
58,Comprehensive Estimates of Potential Synaptic Connections in Local Circuits of the Rodent Hippocampal Formation by Axonal-Dendritic Overlap.,Giorgio Ascoli,Journal of Neuroscience,2706474,,2/24/21,41,8,1665,19,148962271,10.1523/JNEUROSCI.1193-20.2020,Society for Neuroscience,Article,POSTSYNAPTIC potential; DENDRITES; HIPPOCAMPUS (Brain); SYNAPTOGENESIS; EPISODIC memory; IMAGE analysis,CA1; CA3; dentate gyrus; entorhinal cortex; interneuron; network,"A quantitative description of the hippocampal formation synaptic architecture is essential for understanding the neural mechanisms of episodic memory. Yet the existing knowledge of connectivity statistics between different neuron types in the rodent hippocampus only captures a mere 5% of this circuitry. We present a systematic pipeline to produce first-approximation estimates for most of the missing information. Leveraging the www.Hippocampome.org knowledge base, we derive local connection parameters between distinct pairs of morphologically identified neuron types based on their axonal-dendritic overlap within every layer and subregion of the hippocampal formation. Specifically, we adapt modern image analysis technology to determine the parcel-specific neurite lengths of every neuron type from representative morphologic reconstructions obtained from either sex. We then compute the average number of synapses per neuron pair using relevant anatomic volumes from the mouse brain atlas and ultrastructurally established interaction distances. Hence, we estimate connection probabilities and number of contacts for >1900 neuron type pairs, increasing the available quantitative assessments more than 11-fold. Connectivity statistics thus remain unknown for only a minority of potential synapses in the hippocampal formation, including those involving long-range (23%) or perisomatic (6%) connections and neuron types without morphologic tracings (7%). The described approach also yields approximate measurements of synaptic distances from the soma along the dendritic and axonal paths, which may affect signal attenuation and delay. Overall, this dataset fills a substantial gap in quantitatively describing hippocampal circuits and provides useful model specifications for biologically realistic neural network simulations, until further direct experimental data become available. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroscience is the property of Society for Neuroscience and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148962271&site=ehost-live
59,Effects of ß-Catenin on Dendritic Morphology and Simulated Firing Patterns in Cultured Hippocampal Neurons.,Giorgio Ascoli,Biological Bulletin,63185,,Aug-06,211,1,31,13,22292891,10.2307/4134575,University of Chicago Press,Article,MOLECULES; MORPHOGENESIS; ELECTROPHYSIOLOGY; TOPOLOGY; NEURONS,,"β-catenin is an intracellular signaling molecule that has been shown to be important in activity-dependent dendritic morphogenesis. Here, we investigate the detailed morphological changes elicited in dendritic arbors of cultured hippocampal neurons by overexpression of β-catenin, and we simulate the electrophysiological consequences of these changes. Compared to control neurons, cells overexpressing β-catenin have dendritic arbors with significantly greater surface area and more branches, as well as different topological characteristics. To investigate possible effects of β-catenin expression on the electrophysiological properties of neurons, we converted confocal images of neurons expressing β-catenin into computational simulator formats using parameters that evenly distributed voltage-dependent channels across the cells' membranes. In simulated current clamp experiments, somata were injected with a normalized current such that the observed electrophysiological differences in the neurons would be due only to morphological differences. We found that the morphology of β-catenin-expressing neurons contributes to significantly smaller action potential amplitude and greater sensitivity than seen in control neurons. As a consequence, beta;-catenin-expressing neurons tended to exhibit higher spike rates and needed less excitation to induce firing. These findings show that/3-catenin, by modifying dendritic arborization, could have profound influences on the electrophysiological behavior of neurons. [ABSTRACT FROM AUTHOR] Copyright of Biological Bulletin is the property of University of Chicago Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22292891&site=ehost-live
60,Formin 3 directs dendritic architecture via microtubule regulation and is required for somatosensory nociceptive behavior.,Giorgio Ascoli,Development (09501991),9501991,,8/15/21,148,16,1,19,152390363,10.1242/dev.187609,Company of Biologists Ltd.,Article,FUNCTIONAL connectivity; MICROTUBULES; DENDRITES; DROSOPHILA; SENSORY neurons; MITOCHONDRIA; NEURONS,CMT; Dendrite; Drosophila; Formin 3; INF2; Microtubule; Nociception,"Dendrite shape impacts functional connectivity and is mediated by organization and dynamics of cytoskeletal fibers. Identifying the molecular factors that regulate dendritic cytoskeletal architecture is therefore important in understanding the mechanistic links between cytoskeletal organization and neuronal function. We identified Formin 3 (Form3) as an essential regulator of cytoskeletal architecture in nociceptive sensory neurons in Drosophila larvae. Time course analyses reveal that Form3 is cell-autonomously required to promote dendritic arbor complexity. We show that form3 is required for the maintenance of a population of stable dendritic microtubules (MTs), and mutants exhibit defects in the localization of dendritic mitochondria, satellite Golgi, and the TRPA channel Painless. Form3 directly interacts with MTs via FH1-FH2 domains. Mutations in human inverted formin 2 (INF2; orthologof form3) have been causally linked to Charcot-Marie-Tooth (CMT) disease. CMT sensory neuropathies lead to impaired peripheral sensitivity. Defects in form3 function in nociceptive neurons result in severe impairment of noxious heatevoked behaviors. Expression of the INF2 FH1-FH2 domains partially recovers form3 defects in MTs and nocifensive behavior, suggesting conserved functions, thereby providing putative mechanistic insights into potential etiologies of CMT sensory neuropathies. [ABSTRACT FROM AUTHOR] Copyright of Development (09501991) is the property of Company of Biologists Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152390363&site=ehost-live
61,Functional Impact of Dendritic Branch-Point Morphology.,Giorgio Ascoli,Journal of Neuroscience,2706474,,1/30/13,33,5,2156,10,85323542,10.1523/JNEUROSCI.3495-12.2013,Society for Neuroscience,Article,BRAIN function localization; DENDRITIC cells; SOMATIC cells; GLUTAMIC acid; CELL membranes; PYRAMIDAL tract; SYNAPSES; MORPHOLOGY,,"Cortical pyramidal cells store multiple features of complex synaptic input in individual dendritic branches and independently regulate the coupling between dendritic and somatic spikes. Branch points in apical trees exhibit wide ranges of sizes and shapes, and the large diameter ratio between trunk and oblique dendrites exacerbates impedance mismatch. The morphological diversity of dendritic bifur-cations could thus locally tune neuronal excitability and signal integration. However, these aspects have never been investigated. Here, we first quantified the morphological variability of branch points from two-photon images of rat CA1 pyramidal neurons. We then investi-gated the geometrical features affecting spike initiation, propagation, and timing with a computational model validated by glutamate uncaging experiments. The results suggest that even subtle membrane readjustments at branch points could drastically alter the ability of synaptic input to generate, propagate, and time action potentials. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroscience is the property of Society for Neuroscience and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=85323542&site=ehost-live
62,Highlights from the Era of Open Source Web-Based Tools.,Giorgio Ascoli,Journal of Neuroscience,2706474,,2/3/21,41,5,927,10,148532365,10.1523/JNEUROSCI.1657-20.2020,Society for Neuroscience,Article,GENES; LANDSCAPE changes; NEUROSCIENCES,neuroscience; online repositories; open access; open science; open source; web-based tools,"High digital connectivity and a focus on reproducibility are contributing to an open science revolution in neuroscience. Repositories and platforms have emerged across the whole spectrum of subdisciplines, paving the way for a paradigm shift in the way we share, analyze, and reuse vast amounts of data collected across many laboratories. Here, we describe how open access web-based tools are changing the landscape and culture of neuroscience, highlighting six free resources that span subdisciplines from behavior to whole-brain mapping, circuits, neurons, and gene variants. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroscience is the property of Society for Neuroscience and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148532365&site=ehost-live
63,Molecular expression profiles of morphologically defined hippocampal neuron types: Empirical evidence and relational inferences.,Giorgio Ascoli,Hippocampus,10509631,,May-20,30,5,472,16,142724648,10.1002/hipo.23165,"John Wiley & Sons, Inc.",Article,SCIENTIFIC literature; ENTORHINAL cortex; NEURONS; BIOMARKERS; DENTATE gyrus,calcium‐binding proteins; molecular clusters; neuropeptides; receptors,"Gene and protein expressions are key determinants of cellular function. Neurons are the building blocks of brain circuits, yet the relationship between their molecular identity and the spatial distribution of their dendritic inputs and axonal outputs remains incompletely understood. The open‐source knowledge base Hippocampome.org amasses such transcriptomic data from the scientific literature for morphologically defined neuron types in the rodent hippocampal formation: dentate gyrus, CA3, CA2, CA1, subiculum, and entorhinal cortex. Positive, negative, or mixed expression reports were initially obtained from published articles directly connecting molecular evidence to neurons with known axonal and dendritic patterns across hippocampal layers. Here, we supplement this information by collating, formalizing, and leveraging relational expression inferences that link a gene or protein expression or lack thereof to that of another molecule or to an anatomical location. With these additional interpretations, we freely release online a comprehensive human‐ and machine‐readable molecular profile for more than 100 neuron types in Hippocampome.org. Analysis of these data ascertains the ability to distinguish unequivocally most neuron types in each of the major subdivisions of the hippocampus based on currently known biochemical markers. Moreover, grouping neuron types by expression similarity reveals eight superfamilies characterized by a few defining molecules. [ABSTRACT FROM AUTHOR] Copyright of Hippocampus is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142724648&site=ehost-live
64,"Morphometric, geographic, and territorial characterization of brain arterial trees.",Giorgio Ascoli,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Jul-14,30,7,755,12,96924700,10.1002/cnm.2627,Wiley-Blackwell,Article,MORPHOMETRICS; BRAIN blood-vessels; COMPUTER simulation; ANGIOGRAPHY; MAGNETIC resonance imaging of the brain,atlasing; brain vasculature; circle of Willis; morphometry; vascular territories,"SUMMARY Morphometric information of the brain vascularization is valuable for a variety of clinical and scientific applications. In particular, this information is important when creating arterial tree models for imposing boundary conditions in numerical simulations of the brain hemodynamics. The purpose of this work is to provide quantitative descriptions of arterial branches, bifurcation patterns, shape, and geographical distribution of the arborization of the main cerebral arteries as well as estimations of the corresponding vascular territories. For this purpose, subject-specific digital reconstructions of the brain vascular network created from 3T magnetic resonance angiography images of healthy volunteers are used to derive population-averaged morphometric characteristics of the cerebral arterial trees. Copyright © 2014 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96924700&site=ehost-live
64,"Morphometric, geographic, and territorial characterization of brain arterial trees.",Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Jul-14,30,7,755,12,96924700,10.1002/cnm.2627,Wiley-Blackwell,Article,MORPHOMETRICS; BRAIN blood-vessels; COMPUTER simulation; ANGIOGRAPHY; MAGNETIC resonance imaging of the brain,atlasing; brain vasculature; circle of Willis; morphometry; vascular territories,"SUMMARY Morphometric information of the brain vascularization is valuable for a variety of clinical and scientific applications. In particular, this information is important when creating arterial tree models for imposing boundary conditions in numerical simulations of the brain hemodynamics. The purpose of this work is to provide quantitative descriptions of arterial branches, bifurcation patterns, shape, and geographical distribution of the arborization of the main cerebral arteries as well as estimations of the corresponding vascular territories. For this purpose, subject-specific digital reconstructions of the brain vascular network created from 3T magnetic resonance angiography images of healthy volunteers are used to derive population-averaged morphometric characteristics of the cerebral arterial trees. Copyright © 2014 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96924700&site=ehost-live
65,Potential connectomics complements the endeavour of 'no synapse left behind' in the cortex.,Giorgio Ascoli,Journal of Physiology,223751,,Feb-12,590,4,651,2,71687004,10.1113/jphysiol.2011.225664,Wiley-Blackwell,Article,NEUROANATOMY; NEUROBIOLOGY; ANATOMY; BRAIN; NEURAL circuitry,,"The author comments on the paper by Ramaswamy et al. which showed that simple stochastic selection s among axo-dendritic spatial proximities captures the statistics of somatically recorded physiological signals. He states that it is still too early to determine how the results of the study will generalize across cortical regions, layers and pre- and postsynaptic cell types. He also noted the importance of neuroanatomy.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=71687004&site=ehost-live
66,Quantification of neuron types in the rodent hippocampal formation by data mining and numerical optimization.,Giorgio Ascoli,European Journal of Neuroscience,0953816X,,Apr-22,55,7,1724,18,156277479,10.1111/ejn.15639,Wiley-Blackwell,Article,ENTORHINAL cortex; HIPPOCAMPUS (Brain); DATA mining; DENTATE gyrus; NEURONS; NUMERICAL analysis,cell census; entorhinal cortex; hippocampome.Org; hippocampus; neuroinformatics; operations research,"Quantifying the population sizes of distinct neuron types in different anatomical regions is an essential step towards establishing a brain cell census. Although estimates exist for the total neuronal populations in different species, the number and definition of each specific neuron type are still intensively investigated. Hippocampome.org is an open‐source knowledge base with morphological, physiological and molecular information for 122 neuron types in the rodent hippocampal formation. While such framework identifies all known neuron types in this system, their relative abundances remain largely unknown. This work quantitatively estimates the counts of all Hippocampome.org neuron types by literature mining and numerical optimization. We report the number of neurons in each type identified by main neurotransmitter (glutamate or GABA) and axonal‐dendritic patterns throughout 26 subregions and layers of the dentate gyrus, Ammon's horn, subiculum and entorhinal cortex. We produce by sensitivity analysis reliable numerical ranges for each type and summarize the amounts across broad neuronal families defined by biomarkers expression and firing dynamics. Study of density distributions indicates that the number of dendritic‐targeting interneurons, but not of other neuronal classes, is independent of anatomical volumes. All extracted values, experimental evidence and related software code are released on Hippocampome.org. [ABSTRACT FROM AUTHOR] Copyright of European Journal of Neuroscience is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156277479&site=ehost-live
67,Quantitative Measurements of Autobiographical Memory Content.,Giorgio Ascoli,PLoS ONE,19326203,,Sep-12,7,9,1,14,82446691,10.1371/journal.pone.0044809,Public Library of Science,Article,AUTOBIOGRAPHICAL memory; INTROSPECTION; RECOLLECTION (Psychology); MEMORY testing; PROBABILITY theory; RESEARCH; MEMORY research,,"Autobiographical memory (AM), subjective recollection of past experiences, is fundamental in everyday life. Nevertheless, characterization of the spontaneous occurrence of AM, as well as of the number and types of recollected details, remains limited. The CRAM (Cue-Recalled Autobiographical Memory) test (http://cramtest.info) adapts and combines the cue-word method with an assessment that collects counts of details recalled from different life periods. The SPAM (Spontaneous Probability of Autobiographical Memories) protocol samples introspection during everyday activity, recording memory duration and frequency. These measures provide detailed, naturalistic accounts of AM content and frequency, quantifying essential dimensions of recollection. AM content (~20 details/recollection) decreased with the age of the episode, but less drastically than the probability of reporting remote compared to recent memories. AM retrieval was frequent (~20/hour), each memory lasting ~30 seconds. Testable hypotheses of the specific content retrieved in a fixed time from given life periods are presented. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=82446691&site=ehost-live
68,Scarcity begets addiction.,Giorgio Ascoli,Behavioral & Brain Sciences,0140525X,,Apr-06,29,2,178,0,20906077,10.1017/S0140525X06249045,Cambridge University Press,Article,FOOD; MONEY; PSYCHOBIOLOGY; HUMAN behavior; NEUROSCIENCES,,"As prototypical incentive with biological meaning, food illustrates the distinction between money as tool and money as drug. However, consistent neuroscience results challenge this view of food as intrinsic value and opposite to drugs of abuse. The scarce availability over evolutionary time of both food and money may explain their similar drug-like non-satiability, suggesting an integrated mechanism for generalized reinforcers. [ABSTRACT FROM AUTHOR] Copyright of Behavioral & Brain Sciences is the property of Cambridge University Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20906077&site=ehost-live
69,Self-sustaining non-repetitive activity in a large scale neuronal-level model of the hippocampal circuit,Giorgio Ascoli,Neural Networks,8936080,,Oct-08,21,8,1153,11,34743982,10.1016/j.neunet.2008.05.006,Elsevier B.V.,Article,HIPPOCAMPUS (Brain); NEURONS; ANATOMY; SYNAPSES; STOCHASTIC processes; COGNITIVE ability; SIMULATION methods & models,Integrate and fire; McCullogh-Pitts; Modeling; Network; Rat hippocampus; Rhythmic activity,"The mammalian hippocampus is involved in spatial representation and memory storage and retrieval, and much research is ongoing to elucidate the cellular and system-level mechanisms underlying these cognitive functions. Modeling may be useful to link network-level activity patterns to the relevant features of hippocampal anatomy and electrophysiology. Investigating the effects of circuit connectivity requires simulations of a number of neurons close to real scale. To this end, we construct a model of the hippocampus with 16 distinct neuronal classes (including both local and projection cells) and 200,000 individual neurons. The number of neurons in each class and their interconnectivity are drawn from rat anatomy. Here we analyze the emergent network activity and how it is affected by reducing either the size or the connectivity diversity of the model. When the model is run with a simple variation of the McCulloch–Pitts formalism, self-sustaining non-repetitive activity patterns consistently emerge. Specific firing threshold values are narrowly constrained for each cell class upon multiple runs with different stochastic wiring and initial conditions, yet these values do not directly affect network stability. Analysis of the model at different network sizes demonstrates that a scale reduction of one order of magnitude drastically alters network dynamics, including the variability of the output range, the distribution of firing frequencies, and the duration of self-sustained activity. Moreover, comparing the model to a control condition with an equivalent number of (excitatory/inhibitory balanced) synapses, but removing all class-specific information (i.e. collapsing the network to homogeneous random connectivity) has surprisingly similar effects to downsizing the total number of neurons. The reduced-scale model is also compared directly with integrate-and-fire simulations, which capture considerably more physiological detail at the single-cell level, but still fail to reproduce the full behavioral complexity of the large-scale model. Thus network size, cell class diversity, and connectivity details may all be critical to generate self-sustained non-repetitive activity patterns. [Copyright &y& Elsevier] Copyright of Neural Networks is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34743982&site=ehost-live
70,Self-sustaining non-repetitive activity in a large scale neuronal-level model of the hippocampal circuit.,Giorgio Ascoli,BMC Neuroscience,14712202,,2008 Supplement 1,9,,1,2,38599917,10.1186/1471-2202-9-S1-P62,BioMed Central,Article,HIPPOCAMPUS (Brain); NEURONS; BIOLOGICAL neural networks; MEMORY; NEURAL circuitry; NEUROSCIENCES,,"The article focuses on a study on self-sustaining non-repetitive activity in a huge scale model of the hippocampus with 16 distinct neuronal classes. It notes that the mammalian hippocampus is included in spatial representation and memory storage and retrieval. It cites that investigating the effects of circuit connectivity needs simulations of several neurons close to real scale. It states that neuron network size, cell class diversity, and connectivity details may all be vital to produce self-sustained non-repetitive activity patterns.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=38599917&site=ehost-live
71,Simple models of quantitative firing phenotypes in hippocampal neurons: Comprehensive coverage of intrinsic diversity.,Giorgio Ascoli,PLoS Computational Biology,1553734X,,10/28/19,15,10,1,27,139351291,10.1371/journal.pcbi.1007462,Public Library of Science,Article,NEURONS; EVOLUTIONARY algorithms; HIPPOCAMPUS (Brain); PHENOTYPES; ACTION potentials; KNOWLEDGE base,,"Patterns of periodic voltage spikes elicited by a neuron help define its dynamical identity. Experimentally recorded spike trains from various neurons show qualitatively distinguishable features such as delayed spiking, spiking with or without frequency adaptation, and intrinsic bursting. Moreover, the input-dependent responses of a neuron not only show different quantitative features, such as higher spike frequency for a stronger input current injection, but can also exhibit qualitatively different responses, such as spiking and bursting under different input conditions, thus forming a complex phenotype of responses. In previous work, the comprehensive knowledge base of hippocampal neuron types Hippocampome.org systematically characterized various spike pattern phenotypes experimentally identified from 120 neuron types/subtypes. In this paper, we present a complete set of simple phenomenological models that quantitatively reproduce the diverse and complex phenotypes of hippocampal neurons. In addition to point-neuron models, we created compact multi-compartment models with up to four compartments, which will allow spatial segregation of synaptic integration in network simulations. Electrotonic compartmentalization observed in our compact multi-compartment models is qualitatively consistent with experimental observations. The models were created using an automated pipeline based on evolutionary algorithms. This work maps 120 neuron types/subtypes in the rodent hippocampus to a low-dimensional model space and adds another dimension to the knowledge accumulated in Hippocampome.org. Computationally efficient representations of intrinsic dynamics, along with other pieces of knowledge available in Hippocampome.org, provide a biologically realistic platform to explore the large-scale interactions of various neuron types at the mesoscopic level. The neurons in the hippocampus show enormous diversity in their intrinsic activity patterns. A comprehensive characterization of various intrinsic types using a neuronal modeling system is necessary to simulate biologically realistic networks of brain regions. Morphologically detailed neuronal modeling frameworks often limit the scalability of such network simulations due to the specification of hundreds of equations governing each neuron's intrinsic dynamics. In this work, we have accomplished a comprehensive mapping of experimentally identified intrinsic dynamics in a simple class of models with only two governing equations. We have created over a hundred point-neuron models that reflect the intrinsic differences among the hippocampal neuron types both qualitatively and quantitatively. In addition, we compactly extended our point-neurons to include up to four compartments, which will allow anatomically finer-grained connections among the neurons in a network. Our point-neuron and compact model representations, freely available in Hippocampome.org, allow researchers to investigate dynamical interactions among various intrinsic types and emergent integrative properties using scalable, yet biologically realistic network simulations. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139351291&site=ehost-live
72,The importance of metadata to assess information content in digital reconstructions of neuronal morphology.,Giorgio Ascoli,Cell & Tissue Research,0302766X,,Apr-15,360,1,121,7,101805666,10.1007/s00441-014-2103-6,Springer Nature,Article,NEURONS; CELL morphology; NEUROMORPHICS; DATA mining; IMAGE reconstruction; DIGITAL images; QUANTITATIVE research,Completeness; Data standards; Digital reconstruction; Metadata; Neuron morphology,"Digital reconstructions of axonal and dendritic arbors provide a powerful representation of neuronal morphology in formats amenable to quantitative analysis, computational modeling, and data mining. Reconstructed files, however, require adequate metadata to identify the appropriate animal species, developmental stage, brain region, and neuron type. Moreover, experimental details about tissue processing, neurite visualization and microscopic imaging are essential to assess the information content of digital morphologies. Typical morphological reconstructions only partially capture the underlying biological reality. Tracings are often limited to certain domains (e.g., dendrites and not axons), may be incomplete due to tissue sectioning, imperfect staining, and limited imaging resolution, or can disregard aspects irrelevant to their specific scientific focus (such as branch thickness or depth). Gauging these factors is critical in subsequent data reuse and comparison. NeuroMorpho.Org is a central repository of reconstructions from many laboratories and experimental conditions. Here, we introduce substantial additions to the existing metadata annotation aimed to describe the completeness of the reconstructed neurons in NeuroMorpho.Org. These expanded metadata form a suitable basis for effective description of neuromorphological data. [ABSTRACT FROM AUTHOR] Copyright of Cell & Tissue Research is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101805666&site=ehost-live
73,Weighing the Evidence in Peters’ Rule: Does Neuronal Morphology Predict Connectivity?,Giorgio Ascoli,Trends in Neurosciences,1662236,,Feb-17,40,2,63,9,121006204,10.1016/j.tins.2016.11.007,Elsevier B.V.,Article,NEURAL circuitry; SYNAPSES; NEUROBIOLOGY; HIPPOCAMPUS physiology; AXONS,,"Although the importance of network connectivity is increasingly recognized, identifying synapses remains challenging relative to the routine characterization of neuronal morphology. Thus, researchers frequently employ axon–dendrite colocations as proxies of potential connections. This putative equivalence, commonly referred to as Peters’ rule, has been recently studied at multiple levels and scales, fueling passionate debates regarding its validity. Our critical literature review identifies three conceptually distinct but often confused applications: inferring neuron type circuitry, predicting synaptic contacts among individual cells, and estimating synapse numbers within neuron pairs. Paradoxically, at the originally proposed cell-type level, Peters’ rule remains largely untested. Leveraging Hippocampome.org, we validate and refine the relationship between axonal–dendritic colocations and synaptic circuits, clarifying the interpretation of existing and forthcoming data. [ABSTRACT FROM AUTHOR] Copyright of Trends in Neurosciences is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121006204&site=ehost-live
74,A neuronal blueprint for directional mechanosensation in larval zebrafish.,Giorgio Ascoli,Current Biology,9609822,,Apr-21,31,7,1463,1,149712459,10.1016/j.cub.2021.01.045,Cell Press,Article,BRACHYDANIO; WATER currents; SPATIAL orientation; PARSIMONIOUS models; TOPOGRAPHIC maps,hair cells; lateral line; mechanosensation; neurons; projectome; rheotaxis; single-cell tracing; somatotopy,"Animals have a remarkable ability to use local cues to orient in space in the absence of a panoramic fixed reference frame. Here we use the mechanosensory lateral line in larval zebrafish to understand rheotaxis, an innate oriented swimming evoked by water currents. We generated a comprehensive light-microscopy cell-resolution projectome of lateralis afferent neurons (LANs) and used clustering techniques for morphological classification. We find surprising structural constancy among LANs. Laser-mediated microlesions indicate that precise topographic mapping of lateral-line receptors is not essential for rheotaxis. Recording neuronal-activity during controlled mechanical stimulation of neuromasts reveals unequal representation of water-flow direction in the hindbrain. We explored potential circuit architectures constrained by anatomical and functional data to suggest a parsimonious model under which the integration of lateralized signals transmitted by direction-selective LANs underlies the encoding of water-flow direction in the brain. These data provide a new framework to understand how animals use local mechanical cues to orient in space. [Display omitted] • Lateralis afferent neurons are structurally homogeneous in larval zebrafish • Topographic representation of lateral-line receptors is not essential for rheotaxis • Calcium imaging reveals unequal representation of water-flow direction in the brain Mechanosensation enables spatial orientation in the absence of visual references. Valera et al. produce a lateral-line projectome to generate a parsimonious circuit underlying rheotaxis in larval zebrafish. Integration of lateralized signals transmitted by direction-selective neurons underlies the encoding of water-flow direction in the brain. [ABSTRACT FROM AUTHOR] Copyright of Current Biology is the property of Cell Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149712459&site=ehost-live
75,Axonal morphometry of hippocampal pyramidal neurons semi-automatically reconstructed after in vivo labeling in different CA3 locations.,Giorgio Ascoli,Brain Structure & Function,18632653,,Mar-11,216,1,1,15,58132822,10.1007/s00429-010-0291-8,Springer Nature,Article,AXONS; MORPHOMETRICS; HIPPOCAMPUS (Brain); BIOLOGICAL neural networks; QUANTITATIVE research; NEURONS; NEUROSCIENCES,Axonal arbors; CA3b; CA3c; Digital morphology; Hippocampus; Principal neuron; Schaffer collateral,"xonal arbors of principal neurons form the backbone of neuronal networks in the mammalian cortex. Three-dimensional reconstructions of complete axonal trees are invaluable for quantitative analysis and modeling. However, digital data are still sparse due to labor intensity of reconstructing these complex structures. We augmented conventional tracing techniques with computational approaches to reconstruct fully labeled axonal morphologies. We digitized the axons of three rat hippocampal pyramidal cells intracellularly filled in vivo from different CA3 sub-regions: two from areas CA3b and CA3c, respectively, toward the septal pole, and one from the posterior/ventral area (CA3pv) near the temporal pole. The reconstruction system was validated by comparing the morphology of the CA3c neuron with that traced from the same cell by a different operator on a standard commercial setup. Morphometric analysis revealed substantial differences among neurons. Total length ranged from 200 (CA3b) to 500 mm (CA3c), and axonal branching complexity peaked between 1 (CA3b and CA3pv) and 2 mm (CA3c) of Euclidean distance from the soma. Length distribution was analyzed among sub-regions (CA3a,b,c and CA1a,b,c), cytoarchitectonic layers, and longitudinal extent within a three-dimensional template of the rat hippocampus. The CA3b axon extended thrice more collaterals within CA3 than into CA1. On the contrary, the CA3c projection was double into CA1 than within CA3. Moreover, the CA3b axon extension was equal between strata oriens and radiatum, while the CA3c axon displayed an oriens/radiatum ratio of 1:6. The axonal distribution of the CA3pv neuron was intermediate between those of the CA3b and CA3c neurons both relative to sub-regions and layers, with uniform collateral presence across CA3/CA1 and moderate preponderance of radiatum over oriens. In contrast with the dramatic sub-region and layer differences, the axon longitudinal spread around the soma was similar for the three neurons. To fully characterize the axonal diversity of CA3 principal neurons will require higher-throughput reconstruction systems beyond the threefold speed-up of the method adopted here. [ABSTRACT FROM AUTHOR] Copyright of Brain Structure & Function is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=58132822&site=ehost-live
76,BigNeuron: Large-Scale 3D Neuron Reconstruction from Optical Microscopy Images.,Giorgio Ascoli,Neuron,8966273,,Jul-15,87,2,252,5,108317767,10.1016/j.neuron.2015.06.036,Cell Press,Article,"NEURAL physiology; MICROSCOPY; NEURAL circuitry; BIOINFORMATICS; BRAIN imaging; Research and Development in Biotechnology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"Understanding the structure of single neurons is critical for understanding how they function within neural circuits. BigNeuron is a new community effort that combines modern bioimaging informatics, recent leaps in labeling and microscopy, and the widely recognized need for openness and standardization to provide a community resource for automated reconstruction of dendritic and axonal morphology of single neurons. [ABSTRACT FROM AUTHOR] Copyright of Neuron is the property of Cell Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108317767&site=ehost-live
77,Corrigendum to “Differential Arc expression in the hippocampus and striatum during the transition from attentive to automatic navigation on a plus maze” [Neurobiol. Learn. Mem. 131 (2016) 36–45].,Giorgio Ascoli,Neurobiology of Learning & Memory,10747427,,Jul-16,132,,67,1,115917294,10.1016/j.nlm.2016.05.013,Academic Press Inc.,Correction Notice,HIPPOCAMPUS (Brain); PROTEIN expression; PROSENCEPHALON,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=115917294&site=ehost-live
78,Dendritic Cytoskeletal Architecture Is Modulated by Combinatorial Transcriptional Regulation in Drosophila melanogaster.,Giorgio Ascoli,Genetics,166731,,Dec-17,207,4,1401,21,126716803,10.1534/genetics.117.300393,Oxford University Press / USA,Article,DROSOPHILA melanogaster; TRANSCRIPTION factors; ACTIN; MICROTUBULES; CYTOSKELETON; BIOMARKERS,cytoskeleton; dendrite; Drosophila; neurogenomics; transcription factors,"Transcription factors (TFs) have emerged as essential cell autonomous mediators of subtype specific dendritogenesis; however, the downstream effectors of these TFs remain largely unknown, as are the cellular events that TFs control to direct morphological change. As dendritic morphology is largely dictated by the organization of the actin and microtubule (MT) cytoskeletons, elucidating TF-mediated cytoskeletal regulatory programs is key to understanding molecular control of diverse dendritic morphologies. Previous studies in Drosophila melanogaster have demonstrated that the conserved TFs Cut and Knot exert combinatorial control over aspects of dendritic cytoskeleton development, promoting actin and MT-based arbor morphology, respectively. To investigate transcriptional targets of Cut and/or Knot regulation, we conducted systematic neurogenomic studies, coupled with in vivo genetic screens utilizing multi-fluor cytoskeletal and membrane marker reporters. These analyses identified a host of putative Cut and/or Knot effector molecules, and a subset of these putative TF targets converge on modulating dendritic cytoskeletal architecture, which are grouped into three major phenotypic categories, based upon neuromorphometric analyses: complexity enhancer, complexity shifter, and complexity suppressor. Complexity enhancer genes normally function to promote higher order dendritic growth and branching with variable effects on MT stabilization and F-actin organization, whereas complexity shifter and complexity suppressor genes normally function in regulating proximal-distal branching distribution or in restricting higher order branching complexity, respectively, with spatially restricted impacts on the dendritic cytoskeleton. Collectively, we implicate novel genes and cellular programs by which TFs distinctly and combinatorially govern dendritogenesis via cytoskeletal modulation. [ABSTRACT FROM AUTHOR] Copyright of Genetics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126716803&site=ehost-live
79,Differential Arc expression in the hippocampus and striatum during the transition from attentive to automatic navigation on a plus maze.,Giorgio Ascoli,Neurobiology of Learning & Memory,10747427,,May-16,131,,36,10,115212029,10.1016/j.nlm.2016.03.008,Academic Press Inc.,Article,"SPATIAL memory; NEOSTRIATUM; HIPPOCAMPUS physiology; NAVIGATION; GENE expression; BEHAVIORAL assessment; Search, Detection, Navigation, Guidance, Aeronautical, and Nautical System and Instrument Manufacturing",Activity-regulated gene expression; Hippocampus; Place and response navigation; Spatial memory; Striatum; Vicarious trial and error,"The strategies utilized to effectively perform a given task change with practice and experience. During a spatial navigation task, with relatively little training, performance is typically attentive enabling an individual to locate the position of a goal by relying on spatial landmarks. These (place) strategies require an intact hippocampus. With task repetition, performance becomes automatic; the same goal is reached using a fixed response or sequence of actions. These (response) strategies require an intact striatum. The current work aims to understand the activation patterns across these neural structures during this experience-dependent strategy transition. This was accomplished by region-specific measurement of activity-dependent immediate early gene expression among rats trained to different degrees on a dual-solution task (i.e., a task that can be solved using either place or response navigation). As expected, rats increased their reliance on response navigation with extended task experience. In addition, dorsal hippocampal expression of the immediate early gene Arc was considerably reduced in rats that used a response strategy late in training (as compared with hippocampal expression in rats that used a place strategy early in training). In line with these data, vicarious trial and error, a behavior linked to hippocampal function, also decreased with task repetition. Although Arc mRNA expression in dorsal medial or lateral striatum alone did not correlate with training stage, the ratio of expression in the medial striatum to that in the lateral striatum was relatively high among rats that used a place strategy early in training as compared with the ratio among over-trained response rats. Altogether, these results identify specific changes in the activation of dissociated neural systems that may underlie the experience-dependent emergence of response-based automatic navigation. [ABSTRACT FROM AUTHOR] Copyright of Neurobiology of Learning & Memory is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=115212029&site=ehost-live
80,Digital Morphometry of Rat Cerebellar Climbing Fibers Reveals Distinct Branch and Bouton Types.,Giorgio Ascoli,Journal of Neuroscience,2706474,,10/17/12,32,42,14670,15,82756217,10.1523/JNEUROSCI.2018-12.2012,Society for Neuroscience,Article,MORPHOMETRICS; PURKINJE cells; CEREBELLAR cortex; CELLULAR signal transduction; DEXTRAN; BRAIN function localization; LABORATORY rats; All Other Animal Production,,"Cerebellar climbing fibers (CFs) provide powerful excitatory input to Purkinje cells (PCs), which represent the sole output of the cerebellar cortex. Recent discoveries suggest that CFs have information-rich signaling properties important for cerebellar function, beyond eliciting the well known all-or-none PC complex spike. CF morphology has not been quantitatively analyzed at the same level of detail as its biophysical properties. Because morphology can greatly influence function, including the capacity for information processing, it is important to understand CF branching structure in detail, as well as its variability across and within arbors. We have digitally reconstructed 68 rat CFs labeled using biotinylated dextran amine injected into the inferior olive and comprehensively quantified their morphology. CF structure was considerably diverse even within the same anatomical regions. Distinctly identifiable primary, tendril, and distal branches could be operationally differentiated by the relative size of the subtrees at their initial bifurcations. Additionally, primary branches were more directed toward the cortical surface and had fewer and less pronounced synaptic boutons, suggesting they prioritize efficient and reliable signal propagation. Tendril and distal branches were spatially segregated and bouton dense, indicating specialization in signal transmission. Furthermore, CFs systematically targeted molecular layer interneuron cell bodies, especially at terminal boutons, potentially instantiating feedforward inhibition on PCs. This study offers the most detailed and comprehensive characterization of CF morphology to date. The reconstruction files and metadata are publicly distributed at NeuroMorpho.org. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroscience is the property of Society for Neuroscience and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=82756217&site=ehost-live
81,Distinct classes of pyramidal cells exhibit mutually exclusive firing patterns in hippocampal area CA3b.,Giorgio Ascoli,Hippocampus,10509631,,Apr-08,18,4,411,14,64911267,10.1002/hipo.20404,"John Wiley & Sons, Inc.",Article,,,"It is thought that CA3 pyramidal neurons communicate mainly through bursts of spikes rather than so-called trains of regular firing action potentials. Reports of both burst firing and nonburst firing CA3 cells suggest that they may fire with more than one output pattern. With the use of whole-cell recording methods we studied the firing properties of rat hippocampal pyramidal neurons in vitro within the CA3b subregion and found three distinct types of firing patterns. Approximately 37% of cells were regular firing where spikes generated by minimal current injection (rheobase) were elicited with a short latency and with stronger current intensities trains of spikes exhibited spike frequency adaptation (SFA). Another 46% of neurons exhibited a delayed onset at rheobase with a weakly-adapting firing pattern upon stronger stimulation. The remaining 17% of cells showed a burst-firing pattern, though only elicited in response to strong current injection and spontaneous bursts were never observed. Control experiments indicated that the distinct firing patterns were not due to our particular slicing methods or recording techniques. Finally, computer modeling was used to identify how relative differences in K+ conductances, specifically KC, KM, and KD, between cells contribute to the different characteristics of the three types of firing patterns observed experimentally. © 2008 Wiley-Liss, Inc. [ABSTRACT FROM AUTHOR] Copyright of Hippocampus is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=64911267&site=ehost-live
82,Morphological characterization of electrophysiologically and immunohistochemically identified basal forebrain cholinergic and neuropeptide Y-containing neurons.,Giorgio Ascoli,Brain Structure & Function,18632653,,Jul-07,212,1,55,19,26284891,10.1007/s00429-007-0143-3,Springer Nature,Article,PROSENCEPHALON; NEUROPEPTIDES; NEURONS; NEUROPHYSIOLOGY; NEUROTRANSMITTERS,Axon collaterals; Dendritic morphometry; Rat; Three-dimensional reconstruction,"The basal forebrain (BF) contains cholinergic as well as different types of non-cholinergic corticopetal neurons and interneurons, including neuropeptide Y (NPY) containing cells. BF corticopetal neurons constitute an extrathalamic route to the cortex and their activity is associated with an increase in cortical release of the neurotransmitter acetylcholine, concomitant with low voltage fast cortical EEG activity. It has been shown in previous studies (Duque et al. in J Neurophysiol 84:1627–1635, ) that in anesthetized rats BF cholinergic neurons fire mostly during low voltage fast cortical EEG epochs, while increased NPY neuronal firing is accompanied by cortical slow waves. In this paper, electrophysiologically and neurochemically characterized cholinergic and NPY-containing neurons were 3D reconstructed from serial sections and morphometrically analyzed. Cholinergic and NPY-containing neurons, although having roughly the same dendritic surface areas and lengths, were found to differ in dendritic thickness and branching structure. They also have distinct patterns of dendritic endings. The subtle differences in dendritic arborization pattern may have an impact on how synaptic integration takes place in these functionally distinct neuronal populations. Cholinergic neurons exhibited cortically projecting axons and extensive local axon collaterals. Elaborate local axonal arbors confined to the BF also originated from NPY-containing neurons. The presence of local axon collaterals in both cholinergic and NPY neurons indicates that the BF is not a mere conduit for various brainstem inputs to the cortex, but a site where substantial local processing must take place. [ABSTRACT FROM AUTHOR] Copyright of Brain Structure & Function is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26284891&site=ehost-live
83,Morphological determinants of dendritic arborization neurons in <italic>Drosophila</italic> larva.,Giorgio Ascoli,Brain Structure & Function,18632653,,Apr-18,223,3,1107,14,128701789,10.1007/s00429-017-1541-9,Springer Nature,Article,CELL morphology; DENDRITIC cells; CELL growth; DROSOPHILA as laboratory animals; INSECT larvae; RNA interference,Computational modeling; Confocal microscopy; Molecular neurogenetics; Morphological reconstructions; Neuronal development,"Pairing in vivo imaging and computational modeling of dendritic arborization (da) neurons from the fruit fly larva provides a unique window into neuronal growth and underlying molecular processes. We image, reconstruct, and analyze the morphology of wild-type, RNAi-silenced, and mutant da neurons. We then use local and global rule-based stochastic simulations to generate artificial arbors, and identify the parameters that statistically best approximate the real data. We observe structural homeostasis in all da classes, where an increase in size of one dendritic stem is compensated by a reduction in the other stems of the same neuron. Local rule models show that bifurcation probability is determined by branch order, while branch length depends on path distance from the soma. Global rule simulations suggest that most complex morphologies tend to be constrained by resource optimization, while simpler neuron classes privilege path distance conservation. Genetic manipulations affect both the local and global optimal parameters, demonstrating functional perturbations in growth mechanisms. [ABSTRACT FROM AUTHOR] Copyright of Brain Structure & Function is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128701789&site=ehost-live
84,Quantitative firing pattern phenotyping of hippocampal neuron types.,Giorgio Ascoli,Scientific Reports,20452322,,11/29/19,9,1,N.PAG,1,139922628,10.1038/s41598-019-52611-w,Springer Nature,Article,NEURONS; GENETIC markers; ENTORHINAL cortex; DENTATE gyrus; QUANTITATIVE research,,"Systematically organizing the anatomical, molecular, and physiological properties of cortical neurons is important for understanding their computational functions. Hippocampome.org defines 122 neuron types in the rodent hippocampal formation based on their somatic, axonal, and dendritic locations, putative excitatory/inhibitory outputs, molecular marker expression, and biophysical properties. We augmented the electrophysiological data of this knowledge base by collecting, quantifying, and analyzing the firing responses to depolarizing current injections for every hippocampal neuron type from published experiments. We designed and implemented objective protocols to classify firing patterns based on 5 transients (delay, adapting spiking, rapidly adapting spiking, transient stuttering, and transient slow-wave bursting) and 4 steady states (non-adapting spiking, persistent stuttering, persistent slow-wave bursting, and silence). This automated approach revealed 9 unique (plus one spurious) families of firing pattern phenotypes while distinguishing potential new neuronal subtypes. Novel statistical associations emerged between firing responses and other electrophysiological properties, morphological features, and molecular marker expression. The firing pattern parameters, experimental conditions, spike times, references to the original empirical evidences, and analysis scripts are released open-source through Hippocampome.org for all neuron types, greatly enhancing the existing search and browse capabilities. This information, collated online in human- and machine-accessible form, will help design and interpret both experiments and model simulations. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139922628&site=ehost-live
85,"The Great Society, Reagan's Revolution, and Generations of Presidential Voting.",Jonathan L. Auerbach,"American Journal of Political Science (John Wiley & Sons, Inc.)",925853,,Sep-22,,,1,18,158800248,10.1111/ajps.12713,"John Wiley & Sons, Inc.",Article,,,"We build a model of American presidential voting in which the cumulative impression left by political events determines the preferences of voters. The impression varies by voter, depending on their age at the time the events took place. We use the Gallup presidential approval_rating time series to reflect the major events that influence voter preferences, with the most influential occurring during a voter's teenage and early adult years. Our fitted model is predictive, explaining more than 80% of the variation in voting trends over the last half_century. It is also interpretable, dividing voters into five meaningful generations: New Deal Democrats, Eisenhower Republicans, 1960s Liberals, Reagan Conservatives, and Millennials. We present each generation in context of the political events that shaped its preferences, beginning in 1940 and ending with the 2016 election. [ABSTRACT FROM AUTHOR] Copyright of American Journal of Political Science (John Wiley & Sons, Inc.) is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158800248&site=ehost-live
86,Covariate Balancing through Naturally Occurring Strata.,Ivan Avramovic,Health Services Research,179124,,Feb-18,53,1,273,20,127564141,10.1111/1475-6773.12628,Wiley-Blackwell,journal article,PROPENSITY score matching; NURSING care facilities; PUBLIC health; MEDICAL care; COMPARATIVE studies; COMPUTER simulation; RESEARCH methodology; MEDICAL cooperation; PROBABILITY theory; RESEARCH; STATISTICS; SYSTEM analysis; COMORBIDITY; LOGISTIC regression analysis; DATA analysis; EVALUATION research; SENIOR housing; UNITED States; UNITED States. Dept. of Veterans Affairs; Nursing Care Facilities (Skilled Nursing Facilities); Community care facilities for the elderly; Health and Welfare Funds; Administration of Veterans' Affairs; Assisted Living Facilities for the Elderly,Balancing databases; causal impact; confounding; prognosis; propensity scoring,"<bold>Objective: </bold>To provide an alternative to propensity scoring (PS) for the common situation where there are interacting covariates.<bold>Setting: </bold>We used 1.3 million assessments of residents of the United States Veterans Affairs nursing homes, collected from January 1, 2000, through October 9, 2012.<bold>Design: </bold>In stratified covariate balancing (SCB), data are divided into naturally occurring strata, where each stratum is an observed combination of the covariates. Within each stratum, cases with, and controls without, the target event are counted; controls are weighted to be as frequent as cases. This weighting procedure guarantees that covariates, or combination of covariates, are balanced, meaning they occur at the same rate among cases and controls. Finally, impact of the target event is calculated in the weighted data. We compare the performance of SCB, logistic regression (LR), and propensity scoring (PS) in simulated and real data. We examined the calibration of SCB and PS in predicting 6-month mortality from inability to eat, controlling for age, gender, and nine other disabilities for 296,051 residents in Veterans Affairs nursing homes. We also performed a simulation study, where outcomes were randomly generated from treatment, 10 covariates, and increasing number of covariate interactions. The accuracy of SCB, PS, and LR in recovering the simulated treatment effect was reported.<bold>Findings: </bold>In simulated environment, as the number of interactions among the covariates increased, SCB and properly specified LR remained accurate but pairwise LR and pairwise PS, the most common applications of these tools, performed poorly. In real data, application of SCB was practical. SCB was better calibrated than linear PS, the most common method of PS.<bold>Conclusions: </bold>In environments where covariates interact, SCB is practical and more accurate than common methods of applying LR and PS. [ABSTRACT FROM AUTHOR] Copyright of Health Services Research is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127564141&site=ehost-live
87,An Incremental Server for Scheduling Overloaded Real-Time Systems.,Hakan Aydin,IEEE Transactions on Computers,189340,,Oct-03,52,10,1347,15,11034849,10.1109/TC.2003.1234531,IEEE,Article,"COMPUTER systems; COMPUTER algorithms; WORKLOAD of computer networks; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Computer Systems Design Services; Computer systems design and related services (except video game design and development)",,Proposes a novel scheduling framework for a real-time environment that experiences dynamic workload changes. Capability of adjusting the system workload in incremental steps under overloaded conditions; Assignment of criticality value to each task; Process of selecting tasks to discard.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=11034849&site=ehost-live
88,Dynamic modulation scaling enabled multi-hop topology control for time critical wireless sensor networks.,Hakan Aydin,Wireless Networks (10220038),10220038,,Feb-20,26,2,1203,24,141475264,10.1007/s11276-019-02146-9,Springer Nature,Article,WIRELESS sensor networks; ELECTRIC network topology; SOFTWARE radio; TOPOLOGY; NETWORK performance; POLYNOMIAL time algorithms; SOFTWARE measurement,,"The previous work on connection driven topology control has shown that it has significant potential to reduce energy consumption of Wireless Sensor Networks (WSNs). Dynamic Modulation Scaling (DMS) which is a technique that manages transmission power levels in order to change the number of bits encoded per symbol has a direct impact on connection driven topology control. In this paper we investigate the transmission scheduling of multi-hop real-time WSNs equipped with DMS enabled radio chips while taking the effect of DMS on topology control into account. To our best knowledge, this is the first paper that addresses this issue. The current work on DMS enabled WSN tend to rely on theoretical DMS models to predict network performance metrics. However, there is little, if any, work that is based upon empirically verified network performance outcomes using DMS especially on its effect on connection driven topology control. This paper fills this gap by using GNU Radio and Software Defined Radio hardware to show how to emulate DMS in low power wireless systems and measure the impact of varying Signal-to-Noise levels, distance and elevation on throughput and delivery rates for different DMS control strategies. Next, we present the Mixed Integer Nonlinear Optimization Problem of minimizing energy consumption of DMS enabled connection driven topology control on real-time WSNs. Lastly, we present two polynomial time heuristics and compare their performance against the optimal solution. [ABSTRACT FROM AUTHOR] Copyright of Wireless Networks (10220038) is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141475264&site=ehost-live
89,Evaluation framework for energy-aware multiprocessor scheduling in real-Time systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,Sep-19,98,,388,15,138270784,10.1016/j.sysarc.2019.01.018,Elsevier B.V.,Article,VERNACULAR architecture; MULTIPROCESSORS; ENERGY consumption; COMPUTER scheduling; PAPER arts; ARCHITECTURE; Other Construction Material Merchant Wholesalers; Folding Paperboard Box Manufacturing,Energy management; Multicore architectures; Real-time systems; Scheduling,"Multiprocessor and multicore architectures are fast becoming the platform of choice for deploying workloads, as they have higher computing capabilities and energy efficiency than traditional architectures. In addition to time constraints, a number of real-time applications are required to operate in systems working with limited power supplies, which also imposes tight energy constraints on their execution. Therefore, it is desirable for the system to minimize its energy consumption while still achieving a satisfactory performance. Several energy-aware scheduling techniques addressing this issue have been proposed over the past few years. Unfortunately, few aspects of implementation are seldom considered in theoretical work, and only a tiny fraction of these techniques have been implemented in an actual hardware platform and evaluated by analytical methods. The work presented in this paper thus attempts to provide a prototyping and evaluation framework in which energy-aware multiprocessor scheduling algorithms can translate into full-fledged practical realizations, where their power consumption profiles can be properly measured. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138270784&site=ehost-live
90,Exact Fault-Sensitive Feasibility Analysis of Real-Time Tasks.,Hakan Aydin,IEEE Transactions on Computers,189340,,Oct-07,56,10,1372,15,26820547,10.1109/TC.2007.70739,IEEE,Article,ALGORITHMS; LINEAR programming; MATHEMATICAL programming; DYNAMIC programming; INTEGER programming; MATHEMATICAL optimization; SYSTEMS engineering; COMPUTER algorithms; ANALYSIS of variance,deadline-driven systems; fault tolerance; processor demand analysis; Real-time scheduling; real-time systems; recovery blocks,"In this paper, we consider the problem of checking the feasibility of a set of n real-time tasks while provisioning for timely recovery from (at most) k transient faults. We extend the well-known processor demand approach to take into account the extra overhead that may be induced by potential recovery operations under Earliest-Deadline-First scheduling. We develop a necessary and sufficient test using a dynamic programming technique. An improvement upon the previous solutions is to address and efficiently solve the case where the recovery blocks associated with a given task do not necessarily have the same execution time. We also provide an online version of the algorithm that does not require a priori knowledge of release times. The online algorithm runs in O(m ∙ k²) time, where m is the number of ready tasks. We extend the framework to periodic execution settings: We derive a sufficient condition that can be checked efficiently for the feasibility of periodic tasks in the presence of faults. Finally, we analyze the case where the recovery blocks are to be executed nonpreemptively and we formally show that the problem becomes intractable under that assumption. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26820547&site=ehost-live
91,Exploiting primary/backup mechanism for energy efficiency in dependable real-time systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,Aug-17,78,,68,13,124270695,10.1016/j.sysarc.2017.06.008,Elsevier B.V.,Article,ENERGY consumption; SAVINGS; ELECTRIC potential; ARRAY processors; ALGORITHMS,DPM; DVFS; Energy management; Fault tolerance; Multiprocessor; Primary/backup; Real-time systems,"Primary/Backup has been well studied as an effective fault-tolerance technique. In this paper, with the objectives of tolerating a single permanent fault and maintaining system reliability with respect to transient faults, we study dynamic-priority based energy-efficient fault-tolerance scheduling algorithms for periodic real-time tasks running on multiprocessor systems by exploiting the primary/backup technique while considering the negative effects of the widely deployed Dynamic Voltage and Frequency Scaling (DVFS) on transient faults. Specifically, by separating primary and backup tasks on their dedicated processors, we first devise two schemes based on the idea of Standby-Sparing (SS) : For Paired-SS , processors are organized as groups of two (i.e., pairs) and the existing SS scheme is applied within each pair of processors after partitioning tasks to the pairs. In Generalized-SS , processors are divided into two groups (of potentially different sizes), which are denoted as primary and secondary processor groups, respectively. The main (backup) tasks are scheduled on the primary (secondary) processor group under the partitioned-EDF ( partitioned-EDL ) with DVFS (DPM) to save energy. Moreover, we propose schemes that allocate primary and backup tasks in a mixed manner to better utilize system slack on all processors for more energy savings. On each processor, the Preference-Oriented Earliest Deadline (POED) scheduler is adopted to run primary tasks at scaled frequencies as soon as possible (ASAP) and backup tasks at the maximum frequency as late as possible (ALAP) to save energy. Our empirical evaluations show that, for systems with a given number of processors, there normally exists a configuration for Generalized-SS with different number of processors in primary and backup groups, which leads to better energy savings when compared to that of the Paired-SS scheme. Moreover, the POED-based schemes normally have more stable performance and can achieve better energy savings. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124270695&site=ehost-live
92,Multicore Mixed-Criticality Systems: Partitioned Scheduling and Utilization Bound.,Hakan Aydin,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,2780070,,Jan-18,37,1,21,14,126963990,10.1109/TCAD.2017.2697955,IEEE,Article,COMPUTER scheduling; PARALLEL algorithms; MULTICORE processors; LINUX operating systems; EMBEDDED computer systems; Computer and peripheral equipment manufacturing; Electronic Computer Manufacturing; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers,Computer science; Embedded systems; Engines; mixed-criticality (MC); Multicore processing; multicore systems; partitioned scheduling; Partitioning algorithms; Program processors; Scheduling algorithms; utilization bound,"In mixed-criticality (MC) systems, multiple activities with various certification requirements (thus with different criticality levels) can co-exist on shared hardware platforms, where multicore processors have emerged as the de facto computing engines. In this paper, by using the partitioned earliest-deadline-first with virtual deadlines (EDF-VDs) scheduler for a set of periodic MC tasks running on multicore systems, we derive a criticality-aware utilization bound for efficient feasibility tests and then identify its characteristics. Our analysis shows that the bound increases with increasing number of cores and decreasing system criticality level. We show that, since the utilizations of MC tasks at different criticality levels can vary considerably, the utilization contribution of a task on different cores may have large variations and thus can significantly affect the system schedulability under the EDF-VD scheduler. Based on these observations, we propose a novel and efficient criticality-aware task partitioning algorithm (CA-TPA) to compensate for the inherent pessimism of the utilization bound. In order to improve the system schedulability, the task priorities are determined according to their utilization contributions to the system in CA-TPA. Moreover, by analyzing the utilization variations of tasks at different levels, we develop several heuristics to minimize the utilization increment and balance the workload on cores. The simulation results show that the CA-TPA scheme is very effective in achieving higher schedulability ratio and yielding balanced workloads. The actual implementation in Linux operating system further demonstrates the applicability of CA-TPA with lower run-time overhead, compared to the existing partitioning schemes. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126963990&site=ehost-live
93,On Reliability Management of Energy-Aware Real-Time Systems Through Task Replication.,Hakan Aydin,IEEE Transactions on Parallel & Distributed Systems,10459219,,Mar-17,28,3,813,13,121301737,10.1109/TPDS.2016.2600595,IEEE,Article,RELIABILITY in engineering; REAL-time computing; MULTICORE processors; POWER aware computing; EMBEDDED computer systems; COMPUTER scheduling; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and peripheral equipment manufacturing; Electronic Computer Manufacturing,Energy consumption; Energy-aware systems; Management; Multicore processing; multicore systems; real-time and embedded systems; Real-time systems; reliability; scheduling; Transient analysis; Voltage control,"On emerging multicore systems, task replication is a powerful way to achieve high reliability targets. In this paper, we consider the problem of achieving a given reliability target for a set of periodic real-time tasks running on a multicore system with minimum energy consumption. Our framework explicitly takes into account the coverage factor of the fault detection techniques and the negative impact of Dynamic Voltage Scaling (DVS) on the rate of transient faults leading to soft errors. We characterize the subtle interplay between the processing frequency, replication level, reliability, fault coverage, and energy consumption on DVS-enabled multicore systems. We first develop static solutions and then propose dynamic adaptation schemes in order to reduce the concurrent execution of the replicas of a given task and to take advantage of early completions. Our simulation results indicate that through our algorithms, a very broad spectrum of reliability targets can be achieved with minimum energy consumption thanks to the judicious task replication and frequency assignment. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121301737&site=ehost-live
94,On the Interplay of Voltage/Frequency Scaling and Device Power Management for Frame-Based Real-Time Embedded Applications.,Hakan Aydin,IEEE Transactions on Computers,189340,,Jan-12,61,1,31,0,67669908,10.1109/TC.2010.248,IEEE,Article,EMBEDDED computer systems; ENERGY management; QUALITY of service; CENTRAL processing units; PERFORMANCE evaluation; ELECTRIC potential; REAL-time programming; Electronic Computer Manufacturing; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and peripheral equipment manufacturing; Facilities Support Services; Nonresidential Property Managers; Other Services to Buildings and Dwellings,Algorithm design and analysis; Benchmark testing; device power management; energy management; Performance evaluation; Power demand; Quality of service; Real-time systems; Security; Throughput; voltage/frequency scaling,"Voltage/Frequency Scaling (VFS) and Device Power Management (DPM) are two popular techniques commonly employed to save energy in real-time embedded systems. VFS policies aim at reducing the CPU energy, while DPM-based solutions involve putting the system components (e.g., memory or I/O devices) to low-power/sleep states at runtime, when sufficiently long idle intervals can be predicted. Despite numerous research papers that tackled the energy minimization problem using VFS or DPM separately, the interactions of these two popular techniques are not yet well understood. In this paper, we undertake an exact analysis of the problem for a real-time embedded application running on a VFS-enabled CPU and using multiple devices. Specifically, by adopting a generalized system-level energy model, we characterize the variations in different components of the system energy as a function of the CPU processing frequency. Then, we propose a provably optimal and efficient algorithm to determine the optimal CPU frequency as well as device state transition decisions to minimize the system-level energy. We also extend our solution to deal with workload variability. The experimental evaluations confirm that substantial energy savings can be obtained through our solution that combines VFS and DPM optimally under the given task and energy models. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=67669908&site=ehost-live
95,Power-Aware Scheduling for Periodic Real-Time Tasks.,Hakan Aydin,IEEE Transactions on Computers,189340,,May-04,53,5,584,17,12966080,10.1109/TC.2004.1275298,IEEE,Article,"POWER resources; ENERGY consumption; ENERGY auditing; ENERGY policy; ALGORITHMS; WORKLOAD of computer networks; WORKLOAD of computers; Regulation and Administration of Communications, Electric, Gas, and Other Utilities",,"In this paper, we address power-aware scheduling of periodic tasks to reduce CPU energy consumption in hard real-time systems through dynamic voltage scaling. Our intertask voltage scheduling solution includes three components: 1) a static (offline) solution to compute the optimal speed, assuming worst-case workload for each arrival, 2) an online speed reduction mechanism to reclaim energy by adapting to the actual workload, and 3) an online, adaptive and speculative speed adjustment mechanism to anticipate early completions of future executions by using the average-case workload information. All these solutions still guarantee that all deadlines are met. Our simulation results show that our reclaiming algorithm alone outperforms other recently proposed intertask voltage scheduling schemes. Our speculative techniques are shown to provide additional gains, approaching the theoretical lower-bound by a margin of 10 percent. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12966080&site=ehost-live
96,Preference-oriented fixed-priority scheduling for periodic real-time tasks.,Hakan Aydin,Journal of Systems Architecture,13837621,,Sep-16,69,,1,14,117895229,10.1016/j.sysarc.2016.07.005,Elsevier B.V.,Article,"REAL-time control; ALGORITHMS; MONOTONIC functions; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); ASAP (Computer software)",Fixed-priority scheduling; Periodic tasks; Preference-oriented executions; Real-time systems,"Traditionally, real-time scheduling algorithms prioritize tasks solely based on their timing parameters and cannot effectively handle tasks that have different execution preferences . In this paper, for a set of periodic real-time tasks running on a single processor, where some tasks are preferably executed as soon as possible (ASAP) and others as late as possible (ALAP) , we investigate Preference-Oriented Fixed-Priority (POFP) scheduling techniques. First, based on Audsley’s Optimal Priority Assignment (OPA) , we study a Preference Priority Assignment (PPA) scheme that attempts to assign ALAP (ASAP) tasks lower (higher) priorities, whenever possible. Then, by considering the non-work-conserving strategy, we exploit the promotion times of ALAP tasks and devise an online dual-queue based POFP scheduling algorithm. Basically, with the objective of fulfilling the execution preferences of all tasks, the POFP scheduler retains ALAP tasks in the delay queue until their promotion times while putting ASAP tasks into the ready queue right after their arrivals. In addition, to further expedite (delay) the executions of ASAP (ALAP) tasks using system slack, runtime techniques based on dummy and wrapper tasks are investigated. The proposed schemes are evaluated through extensive simulations. The results show that, compared to the classical fixed-priority Rate Monotonic Scheduling (RMS) algorithm, the proposed priority assignment scheme and POFP scheduler can achieve significant improvement in terms of fulfilling the execution preferences of both ASAP and ALAP tasks, which can be further enhanced at runtime with the wrapper-task based slack management technique. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117895229&site=ehost-live
97,Preference-oriented partitioning for multiprocessor real-time systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,May-22,126,,N.PAG,1,156519722,10.1016/j.sysarc.2022.102468,Elsevier B.V.,Article,PARALLEL algorithms; MULTIPROCESSORS,Fixed-priority; Multiprocessor; Partitioned scheduling; Preference-oriented tasks; Real-time systems,"Recently, real-time application models where tasks may have as early as possible (ASAP) or as late as possible (ALAP) execution preferences, while meeting their deadlines, have been proposed and studied. In this work we consider the preference-oriented (PO) real-time scheduling problem for multiprocessor systems. Specifically, we focus on partitioned scheduling of fixed-priority preference-oriented real-time tasks on multiprocessor platforms. Firstly, we explore the use of the Reverse-Preference Priority Assignment (RPPA) scheme, where ALAP tasks are assigned higher priority compared to ASAP tasks on a given processor. Counter-intuitively, this helps the tasks to better fulfill their execution preferences when deployed in the context of the Preference-Oriented Fixed-Priority (POFP) scheduler. Then, considering the complementary execution preferences of the ASAP and ALAP tasks, we propose a preference-oriented partitioning algorithm to allocate tasks to processors. Finally, we extend the algorithm to exploit the period information about the ALAP and ASAP tasks when making the task allocation decisions. The proposed RPPA and preference-oriented partitioning algorithms are evaluated through extensive simulations. The results show that, when the POFP scheduler is adopted, RPPA can significantly improve the execution preferences of ALAP tasks with only marginal impact on ASAP tasks compared to the state-of-the-art priority assignment schemes. Furthermore, compared to the classical utilization based worst-fit-decreasing (WFD) partitioning scheme, the proposed PO partitioning schemes provide more opportunities for both ASAP and ALAP tasks to better fulfill their execution preferences, especially when combined with RPPA and POFP scheduling on each processor. • A novel Reverse-Preference Priority Assignment scheme for POFP scheduler is proposed • New Preference-Oriented partitioning algorithms are devised and analyzed • The schemes are evaluated via simulations and the results show their effectiveness [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156519722&site=ehost-live
98,Preference-oriented real-time scheduling and its application in fault-tolerant systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,Feb-15,61,2,127,13,101342200,10.1016/j.sysarc.2014.12.001,Elsevier B.V.,Article,PRODUCTION scheduling; FAULT tolerance (Engineering); REAL-time computing; ALGORITHMS; COMPUTER simulation,Fault-tolerant systems; Periodic real-time tasks; Preference-oriented execution; Scheduling algorithms,"In this paper, we consider a set of real-time periodic tasks where some tasks are preferably executed as soon as possible (ASAP) and others as late as possible (ALAP) while still meeting their deadlines. After introducing the idea of preference-oriented (PO) execution , we formally define the concept of PO-optimality . For fully-loaded systems (with 100% utilization), we first propose a PO-optimal scheduler, namely ASAP-Ensured Earliest Deadline (SEED) , by focusing on ASAP tasks where the optimality of ALAP tasks’ preference is achieved implicitly due to the harmonicity of the PO-optimal schedules for such systems. Then, for under-utilized systems (with less than 100% utilization), we show the discrepancies between different PO-optimal schedules. By extending SEED, we propose a generalized Preference-Oriented Earliest Deadline (POED) scheduler that can obtain a PO-optimal schedule for any schedulable task set. The application of the POED scheduler in a dual-processor fault-tolerant system is further illustrated. We evaluate the proposed PO-optimal schedulers through extensive simulations. The results show that, comparing to that of the well-known EDF scheduler, the scheduling overheads of SEED and POED are higher (but still manageable) due to the additional consideration of tasks’ preferences. However, SEED and POED can achieve the preference-oriented execution objectives in a more successful way than EDF. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101342200&site=ehost-live
99,A study on the least squares estimator of multivariate isotonic regression function.,Pramita Bagchi,Scandinavian Journal of Statistics,3036898,,Dec-20,47,4,1192,30,147049916,10.1111/sjos.12459,Wiley-Blackwell,Article,ISOTONIC regression; LEAST squares; ASYMPTOTIC distribution; NONPARAMETRIC estimation; REGRESSION analysis; CONVEX functions,consistency; convex function; cumulative sum diagram; nonstandard asymptotic distribution; rate of convergence,"Consider the problem of pointwise estimation of f in a multivariate isotonic regression model Z=f(X1,...,Xd)+ϵ, where Z is the response variable, f is an unknown nonparametric regression function, which is isotonic with respect to each component, and ϵ is the error term. In this article, we investigate the behavior of the least squares estimator of f. We generalize the greatest convex minorant characterization of isotonic regression estimator for the multivariate case and use it to establish the asymptotic distribution of properly normalized version of the estimator. Moreover, we test whether the multivariate isotonic regression function at a fixed point is larger (or smaller) than a specified value or not based on this estimator, and the consistency of the test is established. The practicability of the estimator and the test are shown on simulated and real data as well. [ABSTRACT FROM AUTHOR] Copyright of Scandinavian Journal of Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147049916&site=ehost-live
100,Circulating microRNAs in cellular and antibody-mediated heart transplant rejection.,Pramita Bagchi,Journal of Heart & Lung Transplantation,10532498,,Oct-22,41,10,1401,13,159361101,10.1016/j.healun.2022.06.019,Elsevier B.V.,Article,HEART transplantation; GRAFT rejection; HEART transplant recipients; RECEIVER operating characteristic curves; MICRORNA; CD30 antigen; STANFORD University,allograft rejection; biomarker; heart transplantation; microRNA,"Noninvasive monitoring of heart allograft health is important to improve clinical outcomes. MicroRNAs (miRs) are promising biomarkers of cardiovascular disease and limited studies suggest they can be used to noninvasively diagnose acute heart transplant rejection. The Genomic Research Alliance for Transplantation (GRAfT) is a multicenter prospective cohort study that phenotyped heart transplant patients from 5 mid-Atlantic centers. Patients who had no history of rejection after transplant were compared to patients with acute cellular rejection (ACR) or antibody-mediated rejection (AMR). Small RNA sequencing was performed on plasma samples collected at the time of an endomyocardial biopsy. Differential miR expression was performed with adjustment for clinical covariates. Regression was used to develop miR panels with high diagnostic accuracy for ACR and AMR. These panels were then validated in independent samples from GRAfT and Stanford University. Receiver operating characteristic curves were generated and area under the curve (AUC) statistics calculated. Distinct ACR and AMR clinical scores were developed to translate miR expression data for clinical use. The GRAfT cohort had a median age of 52 years, with 35% females and 45% Black patients. Between GRAfT and Stanford, we included 157 heart transplant patients: 108 controls and 49 with rejection (50 ACR and 38 AMR episodes). After differential miR expression and regression analysis, we identified 12 miRs that accurately discriminate ACR and 17 miRs in AMR. Independent validation of the miR panels within GRAfT led to an ACR AUC 0.92 (95% confidence interval [CI]: 0.86-0.98) and AMR AUC 0.82 (95% CI: 0.74-0.90). The externally validated ACR AUC was 0.72 (95% CI: 0.59-0.82). We developed distinct ACR and AMR miR clinical scores (range 0-100), a score ≥ 65, identified ACR with 86% sensitivity, 76% specificity, and 98% negative predictive value, for AMR score performance was 82%, 84% and 97%, respectively. We identified novel miRs that had excellent performance to noninvasively diagnose acute rejection after heart transplantation. Once rigorously validated, the unique clinical ACR and AMR scores usher in an era whereby genomic biomarkers can be used to screen and diagnose the subtype of rejection. These novel biomarkers may potentially alleviate the need for an endomyocardial biopsy while facilitating the initiation of targeted therapy based on the noninvasive diagnosis of ACR or AMR. [ABSTRACT FROM AUTHOR] Copyright of Journal of Heart & Lung Transplantation is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159361101&site=ehost-live
101,Guest Editorial Special Issue on Blockchain and Economic Knowledge Automation.,Angelos Stavrou,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Jan-20,50,1,2,7,141082994,10.1109/TSMC.2019.2959929,IEEE,Article,CRYPTOCURRENCIES; BLOCKCHAINS; TRADITIONAL knowledge; AUTOMATION; HOSPITALITY; BITCOIN,,"Blockchain, as an emerging decentralized architecture and distributed computing paradigm underlying Bitcoin and other cryptocurrencies, has attracted intensive attention in both research and applications recently. Blockchain, especially powered by chain-coded smart contracts, has the full potential of revolutionizing increasingly centralized cyber-physical-social systems (CPSSs) for constructions and applications, and reshaping traditional knowledge automation workflows. The key advantage of blockchain technology lies in the fact that it can enable the establishment of secured, trusted, and decentralized autonomous ecosystems for various scenarios, especially for better usage of the legacy devices, infrastructure, and resources. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141082994&site=ehost-live
101,Guest Editorial Special Issue on Blockchain and Economic Knowledge Automation.,Foteini Baldimtsi,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Jan-20,50,1,2,7,141082994,10.1109/TSMC.2019.2959929,IEEE,Article,CRYPTOCURRENCIES; BLOCKCHAINS; TRADITIONAL knowledge; AUTOMATION; HOSPITALITY; BITCOIN,,"Blockchain, as an emerging decentralized architecture and distributed computing paradigm underlying Bitcoin and other cryptocurrencies, has attracted intensive attention in both research and applications recently. Blockchain, especially powered by chain-coded smart contracts, has the full potential of revolutionizing increasingly centralized cyber-physical-social systems (CPSSs) for constructions and applications, and reshaping traditional knowledge automation workflows. The key advantage of blockchain technology lies in the fact that it can enable the establishment of secured, trusted, and decentralized autonomous ecosystems for various scenarios, especially for better usage of the legacy devices, infrastructure, and resources. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141082994&site=ehost-live
102,16S rRNA metagenome clustering and diversity estimation using locality sensitive hashing.,Huzefa Rangwala,BMC Systems Biology,17520509,,2013 Suppl 4,7,,1,13,131728572,10.1186/1752-0509-7-S4-S11,BioMed Central,Article,,,"Background: Advances in biotechnology have changed the manner of characterizing large populations of microbial communities that are ubiquitous across several environments.""Metagenome” sequencing involves decoding the DNA of organisms co-existing within ecosystems ranging from ocean, soil and human body. Several researchers are interested in metagenomics because it provides an insight into the complex biodiversity across several environments. Clinicians are using metagenomics to determine the role played by collection of microbial organisms within human body with respect to human health wellness and disease. Results: We have developed an efficient and scalable, species richness estimation algorithm that uses locality sensitive hashing (LSH). Our algorithm achieves efficiency by approximating the pairwise sequence comparison operations using hashing and also incorporates matching of fixed-length, gapless subsequences criterion to improve the quality of sequence comparisons. We use LSH-based similarity function to cluster similar sequences and make individual groups, called operational taxonomic units (OTUs). We also compute different species diversity/richness metrics by utilizing OTU assignment results to further extend our analysis. Conclusion: The algorithm is evaluated on synthetic samples and eight targeted 16S rRNA metagenome samples taken from seawater. We compare the performance of our algorithm with several competing diversity estimation algorithms. We show the benefits of our approach with respect to computational runtime and meaningful OTU assignments. We also demonstrate practical significance of the developed algorithm by comparing bacterial diversity and structure across different skin locations. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131728572&site=ehost-live
102,16S rRNA metagenome clustering and diversity estimation using locality sensitive hashing.,Daniel Barbará,BMC Systems Biology,17520509,,2013 Suppl 4,7,,1,13,131728572,10.1186/1752-0509-7-S4-S11,BioMed Central,Article,,,"Background: Advances in biotechnology have changed the manner of characterizing large populations of microbial communities that are ubiquitous across several environments.""Metagenome” sequencing involves decoding the DNA of organisms co-existing within ecosystems ranging from ocean, soil and human body. Several researchers are interested in metagenomics because it provides an insight into the complex biodiversity across several environments. Clinicians are using metagenomics to determine the role played by collection of microbial organisms within human body with respect to human health wellness and disease. Results: We have developed an efficient and scalable, species richness estimation algorithm that uses locality sensitive hashing (LSH). Our algorithm achieves efficiency by approximating the pairwise sequence comparison operations using hashing and also incorporates matching of fixed-length, gapless subsequences criterion to improve the quality of sequence comparisons. We use LSH-based similarity function to cluster similar sequences and make individual groups, called operational taxonomic units (OTUs). We also compute different species diversity/richness metrics by utilizing OTU assignment results to further extend our analysis. Conclusion: The algorithm is evaluated on synthetic samples and eight targeted 16S rRNA metagenome samples taken from seawater. We compare the performance of our algorithm with several competing diversity estimation algorithms. We show the benefits of our approach with respect to computational runtime and meaningful OTU assignments. We also demonstrate practical significance of the developed algorithm by comparing bacterial diversity and structure across different skin locations. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131728572&site=ehost-live
103,Improving the recognition of grips and movements of the hand using myoelectric signals.,Gene Shuman,BMC Medical Informatics & Decision Making,14726947,,7/21/16,16,,65,19,116978119,10.1186/s12911-016-0308-1,BioMed Central,journal article,GRIPS (Persons); HUMAN mechanics; ELECTROMYOGRAPHY; PROSTHETICS; COMPUTER interfaces; ACTIVITIES of daily living; HAND physiology; HAND; INFORMATION science; SIGNAL processing; BODY movement; All Other Health and Personal Care Stores,ADLs; Classification; Dynamic time warping; Electromyograms; Machine learning; Prehensile patterns; SAX,"<bold>Background: </bold>People want to live independently, but too often disabilities or advanced age robs them of the ability to do the necessary activities of daily living (ADLs). Finding relationships between electromyograms measured in the arm and movements of the hand and wrist needed to perform ADLs can help address performance deficits and be exploited in designing myoelectrical control systems for prosthetics and computer interfaces.<bold>Methods: </bold>This paper reports on several machine learning techniques employed to discover the electromyogram patterns present when performing 24 typical fine motor functional activities of the hand and the rest position used to accomplish ADLs. Accelerometer data is collected from the hand as an aid in identifying the start and end of movements and to help in labeling the signal data. Techniques employed include classification of 100 ms individual signal instances, using a symbolic representation to approximate signal streams, and the use of nearest neighbor in two specific situations: creation of an affinity matrix to model learning instances and classify based on multiple adjacent signal values, and using Dynamic Time Warping (DTW) as a distance measure to classify entire activity segments.<bold>Results: </bold>Results show the patterns can be learned to an accuracy of 76.64 % for a 25 class problem when classifying 100 ms instances, 83.63 % with the affinity matrix approach with symbolic representation, and 85.22 % with Dynamic Time Warping. Classification errors are, with a few exceptions, concentrated within particular grip action groups.<bold>Conclusion: </bold>The findings reported here support the view that grips and movements of the hand can be distinguished by combining electrical and mechanical properties of the task to an accuracy of 85.22 % for a 25 class problem. Converting the signals to a symbolic representation and classifying based on larger portions of the signal stream improve classification accuracy. This is both clinically useful and opens the way for an approach to help simulate hand functional activities. With improvements it may also prove useful in real time control applications. [ABSTRACT FROM AUTHOR] Copyright of BMC Medical Informatics & Decision Making is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116978119&site=ehost-live
103,Improving the recognition of grips and movements of the hand using myoelectric signals.,Daniel Barbará,BMC Medical Informatics & Decision Making,14726947,,7/21/16,16,,65,19,116978119,10.1186/s12911-016-0308-1,BioMed Central,journal article,GRIPS (Persons); HUMAN mechanics; ELECTROMYOGRAPHY; PROSTHETICS; COMPUTER interfaces; ACTIVITIES of daily living; HAND physiology; HAND; INFORMATION science; SIGNAL processing; BODY movement; All Other Health and Personal Care Stores,ADLs; Classification; Dynamic time warping; Electromyograms; Machine learning; Prehensile patterns; SAX,"<bold>Background: </bold>People want to live independently, but too often disabilities or advanced age robs them of the ability to do the necessary activities of daily living (ADLs). Finding relationships between electromyograms measured in the arm and movements of the hand and wrist needed to perform ADLs can help address performance deficits and be exploited in designing myoelectrical control systems for prosthetics and computer interfaces.<bold>Methods: </bold>This paper reports on several machine learning techniques employed to discover the electromyogram patterns present when performing 24 typical fine motor functional activities of the hand and the rest position used to accomplish ADLs. Accelerometer data is collected from the hand as an aid in identifying the start and end of movements and to help in labeling the signal data. Techniques employed include classification of 100 ms individual signal instances, using a symbolic representation to approximate signal streams, and the use of nearest neighbor in two specific situations: creation of an affinity matrix to model learning instances and classify based on multiple adjacent signal values, and using Dynamic Time Warping (DTW) as a distance measure to classify entire activity segments.<bold>Results: </bold>Results show the patterns can be learned to an accuracy of 76.64 % for a 25 class problem when classifying 100 ms instances, 83.63 % with the affinity matrix approach with symbolic representation, and 85.22 % with Dynamic Time Warping. Classification errors are, with a few exceptions, concentrated within particular grip action groups.<bold>Conclusion: </bold>The findings reported here support the view that grips and movements of the hand can be distinguished by combining electrical and mechanical properties of the task to an accuracy of 85.22 % for a 25 class problem. Converting the signals to a symbolic representation and classifying based on larger portions of the signal stream improve classification accuracy. This is both clinically useful and opens the way for an approach to help simulate hand functional activities. With improvements it may also prove useful in real time control applications. [ABSTRACT FROM AUTHOR] Copyright of BMC Medical Informatics & Decision Making is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116978119&site=ehost-live
103,Improving the recognition of grips and movements of the hand using myoelectric signals.,Jessica Lin,BMC Medical Informatics & Decision Making,14726947,,7/21/16,16,,65,19,116978119,10.1186/s12911-016-0308-1,BioMed Central,journal article,GRIPS (Persons); HUMAN mechanics; ELECTROMYOGRAPHY; PROSTHETICS; COMPUTER interfaces; ACTIVITIES of daily living; HAND physiology; HAND; INFORMATION science; SIGNAL processing; BODY movement; All Other Health and Personal Care Stores,ADLs; Classification; Dynamic time warping; Electromyograms; Machine learning; Prehensile patterns; SAX,"<bold>Background: </bold>People want to live independently, but too often disabilities or advanced age robs them of the ability to do the necessary activities of daily living (ADLs). Finding relationships between electromyograms measured in the arm and movements of the hand and wrist needed to perform ADLs can help address performance deficits and be exploited in designing myoelectrical control systems for prosthetics and computer interfaces.<bold>Methods: </bold>This paper reports on several machine learning techniques employed to discover the electromyogram patterns present when performing 24 typical fine motor functional activities of the hand and the rest position used to accomplish ADLs. Accelerometer data is collected from the hand as an aid in identifying the start and end of movements and to help in labeling the signal data. Techniques employed include classification of 100 ms individual signal instances, using a symbolic representation to approximate signal streams, and the use of nearest neighbor in two specific situations: creation of an affinity matrix to model learning instances and classify based on multiple adjacent signal values, and using Dynamic Time Warping (DTW) as a distance measure to classify entire activity segments.<bold>Results: </bold>Results show the patterns can be learned to an accuracy of 76.64 % for a 25 class problem when classifying 100 ms instances, 83.63 % with the affinity matrix approach with symbolic representation, and 85.22 % with Dynamic Time Warping. Classification errors are, with a few exceptions, concentrated within particular grip action groups.<bold>Conclusion: </bold>The findings reported here support the view that grips and movements of the hand can be distinguished by combining electrical and mechanical properties of the task to an accuracy of 85.22 % for a 25 class problem. Converting the signals to a symbolic representation and classifying based on larger portions of the signal stream improve classification accuracy. This is both clinically useful and opens the way for an approach to help simulate hand functional activities. With improvements it may also prove useful in real time control applications. [ABSTRACT FROM AUTHOR] Copyright of BMC Medical Informatics & Decision Making is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116978119&site=ehost-live
104,An investigation into the mechanistic origin of thermal stability in thermal-microstructural-engineered additively manufactured Inconel 718.,Ali Beheshti,Vacuum,0042207X,,May-22,199,,N.PAG,1,155725035,10.1016/j.vacuum.2022.110971,Elsevier B.V.,Article,THERMAL stability; LASER peening; INCONEL; RESIDUAL stresses; HEAT treatment; LASER annealing; Metal Heat Treating,Laser peening; Microstructure engineering; Nickel-based superalloys; Thermal stability,"An obstacle hindering the applicability of surface modification techniques such as laser peening (LP) in high temperature systems stems from thermally-driven degradation of desirable, strain-induced material modifications. Illustrated in this work is a novel LP scheme termed laser peening plus thermal microstructure engineering (LP + TME) comprised of cyclic LP and the addition of intermittent 600 °C (0.55T m) heat treatments designed to impart thermally-stable microstructural modifications in additively manufactured (AM) Inconel 718 (In718). Instrumented microindentation uncovered significant surface and sub-surface hardness enhancements exceeding 600 HV following LP + TME, a 20% increase over the as-built material. High magnitude compressive residual stresses exceeding −310 MPa were also measured following a 350-h 600 °C thermal exposure; a 25% increase compared to the material subjected to only a single laser shot. Thermal stabilization and overall material enhancement were determined to be the result of the formation of thermally-stable subgrains, subgrain and grain growth regulation through pinning effects, and dislocation-precipitate interactions. • Enhanced thermal stability of microstructural enhancements induced by cyclic laser peening and annealing (LP + TME). • Increased microhardness of 20% over the non-treated material. • LP + TME treated material retained 25% greater residual stresses than the traditionally laser peened material. • Greater stability of the LP + TME is attributed to subgrain formation and pinning effects of precipitates. [ABSTRACT FROM AUTHOR] Copyright of Vacuum is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155725035&site=ehost-live
105,Elevated temperature contact creep and friction of nickel-based superalloys using machine learning assisted finite element analysis.,Ali Beheshti,Mechanics of Materials,1676636,,Aug-22,171,,N.PAG,1,157418515,10.1016/j.mechmat.2022.104346,Elsevier B.V.,Article,FINITE element method; HIGH temperatures; HEAT resistant alloys; MACHINE learning; FRICTION; Iron and Steel Mills and Ferroalloy Manufacturing,Artificial neural network; Elastic-plastic-viscoplastic contact; High temperatures; Machine learning; Static friction; Superalloys,"Nickel-based superalloys with superior thermochemical, mechanical, and tribological properties are highly utilized for critical components in several high temperature applications such as gas turbines and nuclear reactors. Inconel 617, in particular, is considered as one of the main candidate superalloys for tribo-components in very-high-temperature gas-cooled nuclear reactors. Recent findings indicate that this alloy grows unique surface oxide especially in a high-temperature helium environment with distinctive wear, friction, and contact properties. This study investigates the high temperature contact area evolution and frictional behavior of Inconel 617 using finite element simulation and provides predictive models for the contact and friction performance at different normal loads, dwell times, and temperatures. High temperature helium-aged Inconel 617 top surface properties (up to 600 °C) are utilized along with a single asperity-based deformable elastic-plastic contact model under combined normal and tangential loading. Machine learning is used to assist the finite element results and to predict friction coefficient as well as contact area evolution. While a small difference is observed in the instantaneous friction coefficient (no dwell time) for all temperatures, friction coefficient increases considerably with dwell time. This shows that the effect of contact creep for longer dwell times significantly dominants the effect of high temperature variation in basic mechanical parameters such as modulus and yield strength. It is found that increasing temperature and dwell times lead to the friction coefficient increase, yet the dominance of dwell time effects decreases at higher temperatures and loads. While the analysis is presented for Inconel 617, the methodology is easy to be generalized and can be applied to other HT alloys. • High temperature contact area and frictional behavior of Inconel 617 is studied. • A single asperity-based elastic-plastic finite element simulation is developed. • Machine learning is used to assist simulations and predict friction/contact area. • Friction coefficient and contact area increase considerably with dwell time. • Method is easy to be generalized and can be used for other high temperature alloys. [ABSTRACT FROM AUTHOR] Copyright of Mechanics of Materials is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157418515&site=ehost-live
106,Elevated temperature mechanical properties of Inconel 617 surface oxide using nanoindentation.,Ali Beheshti,Materials Science & Engineering: A,9215093,,Jun-20,788,,N.PAG,1,143780433,10.1016/j.msea.2020.139539,Elsevier B.V.,Article,"NANOINDENTATION; HIGH temperatures; GAS cooled reactors; INCONEL; BULK solids; FINITE element method; Dry bulk materials trucking, local; Dry bulk materials trucking, long distance",Creep; Elevated temperature; Finite element analysis; Inconel 617; Nanoindentation,"Inconel 617 is a principal candidate material for helium gas cooled very-high-temperature reactors with outlet temperatures of 700–950 °C. Recent findings showed that this alloy develops unique surface oxide layers especially at high temperature (HT) helium environment with distinctive wear, friction and contact properties. This study investigates the elevated temperature mechanical properties of Inconel 617 top surface layers aged in HT helium environment. Nanoindentation technique is used to obtain load-displacement graphs of the alloy top surface oxide in temperatures ranging from 25 °C up to 600 °C. In addition, using finite element analysis along with an iterative regression technique, a semi-numerical method is developed to further measure and quantify the material parameters and, in particular, time-independent and creep characteristics of the oxide. While Young's modulus of the oxide is found to be relatively close to the bulk for the tested temperatures, the yield strength and hardness, in comparison to the bulk material, increase significantly as the material is oxidized after aging. The oxide exhibits significant softening as the temperature increases to 600 °C. Unlike the bulk material, diffusion through the grains is found to be the dominant creep mechanism for the oxide. Considerable difference between the mechanical properties of the oxide and the bulk material shows the need for accurate measurements of near surface mechanical properties, if reliable predictive contact and tribological models are sought at elevated temperatures. [ABSTRACT FROM AUTHOR] Copyright of Materials Science & Engineering: A is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143780433&site=ehost-live
107,Elevated temperature nanoscratch of Inconel 617 Superalloy.,Ali Beheshti,Mechanics Research Communications,936413,,Apr-22,121,,N.PAG,1,155846532,10.1016/j.mechrescom.2022.103875,Elsevier B.V.,Article,HIGH temperatures; INCONEL; HEAT resistant alloys; FINITE element method; TEMPERATURE effect; Iron and Steel Mills and Ferroalloy Manufacturing,Adhesion; High temperature; Inconel 617; Nicke-based Superalloys; Plowing; Scratch friction,"• High temperature nanoscratch and finite element analysis to extract friction. • Friction coefficient shows no dependence on sliding velocity for the range investigated. • Elastic recovery after nanoscratch decreases with increasing load and temperature. • Adhesion component of friction significantly increases with temperature rise. • Plowing contribution to the overall friction is noticeable above 400 °C. Inconel 617 superalloy is a main candidate to be used for mechanical and tribo-components in high temperature helium-cooled reactors. Recent findings show that it grows a unique surface oxide, especially under high temperature helium with distinct wear, friction, and contact properties. This study reports the elevated temperature nanoscratch behavior of Inconel 617 and further utilizes it to understand the effect of temperature on contact friction constituent contributors, adhesion and plowing at small scales. Inconel 617 is aged in high temperature helium, and consequently, the total kinetic friction coefficient of the alloy surface oxide is obtained in temperatures ranging from 25 °C to 400 °C. A finite element model is developed and validated based on the experimental results. The model is then utilized along with previously established techniques to determine the adhesion and plowing components of the friction coefficient. At small scale, the experimental results show that with increasing temperature the friction coefficient increases. It was inferred that this increase is mainly due to the increased contribution of plowing friction at high levels of deformation. [Display omitted] [ABSTRACT FROM AUTHOR] Copyright of Mechanics Research Communications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155846532&site=ehost-live
108,Elevated temperature tribology of Ni alloys under helium environment for nuclear reactor applications.,Ali Beheshti,Tribology International,0301679X,,Jul-18,123,,372,13,128945093,10.1016/j.triboint.2018.03.021,Elsevier B.V.,Article,"TRIBOLOGY; NICKEL alloys; NUCLEAR reactors; HELIUM; HIGH temperature physics; Industrial Gas Manufacturing; Non-ferrous metal (except copper and aluminum) rolling, drawing, extruding and alloying; Power Boiler and Heat Exchanger Manufacturing",Alloy 800HT; Elevated temperature helium tribology; Inconel 617; Nuclear reactor,"The current study investigates the friction and wear behavior of two primary candidate materials, Inconel 617 and alloy 800HT for high-temperature gas cooled nuclear reactors/very-high-temperature reactors. Using a custom-built high temperature tribometer, helium cooled reactor environment was simulated at room and 800 °C temperatures. Microscopy and chemical analyses were carried out to explain the tribological performance of the alloys. At elevated temperatures, both alloys show higher friction in helium, compared to air environment. Both alloys exhibit high wear resistance in all experimental conditions, except at high temperature helium environment. The formation of glazed and mechanically mixed layers of oxides were found to be important causes for the lower friction and wear in high temperature air atmosphere. [ABSTRACT FROM AUTHOR] Copyright of Tribology International is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128945093&site=ehost-live
109,Helium Tribology of Inconel 617 at Elevated Temperatures up to 950°C: Parametric Study.,Ali Beheshti,Nuclear Science & Engineering,295639,,Sep-19,193,9,998,15,138051627,10.1080/00295639.2019.1582315,Taylor & Francis Ltd,Article,HELIUM; NUCLEAR reactors; HIGH temperatures; Industrial Gas Manufacturing; Power Boiler and Heat Exchanger Manufacturing,friction; high temperature; Inconel 617; Nuclear reactor; wear,"This study investigates the friction and wear behavior of Inconel 617, one of the primary candidate materials for high-temperature gas-cooled nuclear reactors. Using a custom-built, high-temperature tribometer, a helium (He)-cooled reactor environment was simulated up to 950°C. To obtain a comprehensive understanding of the Inconel 617 tribological response, the effects of contact load, temperature, air and He environments, sliding speed, and sliding distance were studied. From the conditions investigated, the coefficient of friction and wear values are the highest in a high-temperature He atmosphere. Scanning electron microscopy, energy dispersive spectroscopy, and X-ray diffraction techniques were used to analyze the Inconel 617 oxide layer. Analysis of the samples tested in the He atmosphere showed the presence of Cr-rich oxide with a lower presence of Co-Ni-Mo compared to the samples tested in air. Characterization also revealed the existence of a very hard protective glaze layer in air while such layer was not observed in the He environment, which was associated with higher wear/friction values. [ABSTRACT FROM AUTHOR] Copyright of Nuclear Science & Engineering is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138051627&site=ehost-live
110,Helium tribology of Inconel 617 subjected to laser peening for high temperature nuclear reactor applications.,Ali Beheshti,Applied Surface Science,1694332,,Mar-22,577,,N.PAG,1,154297827,10.1016/j.apsusc.2021.151961,Elsevier B.V.,Abstract,LASER peening; TRIBOLOGY; NUCLEAR reactors; INCONEL; GAS cooled reactors; HEAT resistant materials; Power Boiler and Heat Exchanger Manufacturing,Helium thermal aging; High temperature tribology; Inconel 617; Laser peening; Thermal microstructure engineering,"[Display omitted] • Laser peening enhances the microhardness of Inconel 617 by 62%. • Laser peening sustains structural integrity on Inconel 617 after thermal exposure. • Significant wear and friction reduction in helium due to peening and aging processes. Inconel 617 is among the best candidates for utilization in high temperature gas cooled reactor tribo-components. However, the combined effects of sliding contact, along with intermittent idle times and very high temperature material degradation, deteriorates the alloy tribological performance, especially under a helium atmosphere. Laser peening is a surface treatment technique which can enhance the properties at the surface and subsurface by generating deep residual stresses and enhanced microstructure. Herein, we report the tribological behavior of regular laser peened as well as thermally-engineered laser peened Inconel 617 under helium and air atmospheres at 800 °C. In addition to friction and wear studies, the specimens are characterized by different analytical techniques to further understand the mechanisms involved in the peening process and sliding contact. Regardless of the peening process and post-process treatment types, it is observed that laser peening improves the tribological characteristics of Inconel 617. Interestingly, laser peening followed by helium thermal aging shows highly enhanced tribological behavior. This is attributed to the strengthening effect of the laser peening on the surface oxides providing an excellent and lasting protective and lubricating film under helium exposure. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154297827&site=ehost-live
111,Microfabricated Biomimetic placoid Scale-Inspired surfaces for antifouling applications.,Ali Beheshti,Applied Surface Science,1694332,,Sep-18,453,,166,7,130073164,10.1016/j.apsusc.2018.05.030,Elsevier B.V.,Article,ANTIFOULING paint; ELASTOMERS; POLYDIMETHYLSILOXANE; ESCHERICHIA coli; DURABILITY; MICROFABRICATION; CHEMICAL molding; Resin and synthetic rubber manufacturing; Plastics Material and Resin Manufacturing; Synthetic Rubber Manufacturing,Antifouling; Biomimetic; Microfabrication; Microtopographies; Surface durability,"Marine biological fouling or ‘biofouling’, the unwanted aggregation of aquatic organisms such as algae, barnacles, and marine microorganisms has been a detriment to maritime industries. Previous antifouling strategies included the use of toxic, biocide-containing paints; however, as humanity strives to lessen its environmental impact, a shift towards ecological deterrents which draw inspiration from nature are starting to be developed. In this work, the manipulation of the surface topography of nontoxic polydimethylsiloxane elastomer (PDMSe) was performed in order to mimic natural-occurring antifouling surfaces like those found on shark skin by means of a micro-molding technique using etched silicon molds. Previous polymer-based antifouling patterns which draw influence from shark skin have been successful in mitigating microorganism settlement; however, they lack a degree of biological accuracy as the features are of constant height. Our novel designs utilize various microfabrication techniques and micro-molding to generate placoid scale patterns with an engineered height gradient to deter organism settlement. Surface durability studies showed that the patterns can effectively keep their integrity under external sliding motion. Significant decreases in Escherichia coli ( E. coli ) settlement up to 75% were observed when measuring the effectiveness of pristine patterns, and up to 56% when patterns underwent extreme mechanical wear. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=130073164&site=ehost-live
112,Performance of additively manufactured polylactic acid (PLA) in prolonged marine environments.,Ali Beheshti,Polymer Degradation & Stability,1413910,,May-22,199,,N.PAG,1,156520445,10.1016/j.polymdegradstab.2022.109903,Elsevier B.V.,Article,HOPKINSON bars (Testing); ARTIFICIAL seawater; POLYLACTIC acid; NANOINDENTATION; STRAIN rate; OCEAN temperature; TENSILE tests,Additive manufacturing; Impact; Marine environment; Mechanical properties; Nanoindentation; PLA,"• An accelerated aging model was developed to predict life of additively manufactured PLA over years with only weeks or days of exposure for the first time. • A wide range of mechanical tests such as quasi-static tensile, nanoindentation, and direct impact Hopkinson pressure bar tests were conducted to deduce different mechanical properties after aging. • High strain rate behavior of the aged material was studied for the first time. • Using the results gathered from tensile testing, data was extrapolated to predict that PLA will lose 0.5% ductility at around 5166 h (30.75 weeks) and will suffer a 10 MPa drop in yield strength at around 2084 h (12.4 weeks) while submerged in 17 °C seawater. Additive manufacturing has seen rapid growth in a variety of sectors due to its advantages in cost and lead-time reductions. However, there is still skepticism regarding the ability for additively manufactured materials to withstand extreme conditions. In this report, AM polylactic acid (PLA) was submerged in seawater at several temperatures to deduce long-term effects of marine environments for these materials. An accelerated aging model was developed to predict life of PLA over years with only weeks or days of exposure. Samples were submerged in artificial seawater between one and ten weeks with temperatures ranging from 22 °C to 60 °C. Several mechanical tests were then conducted on the submerged samples, such as quasi-static tensile, nanoindentation, and direct impact Hopkinson pressure bar tests to deduce different mechanical properties after aging. For the seawater samples, diffusion data was collected to measure water absorbed by the specimens. An Arrhenius relationship was then studied, and a model for accelerated aging was developed with resulting acceleration factors. Results indicated clear aging in PLA over the timeframe studied, and it was deduced that the acceleration factors of aging between 22 °C and 40 °C, 22 °C and 50 °C, and 22 °C and 60 °C, were 1.91, 3.56, and 5.57, respectively. Results demonstrated a clear aging in PLA; it can be predicted that PLA will lose 0.5% ductility at around 5166 h (30.75 weeks) and will suffer a 10 MPa drop in yield strength at around 2084 h (12.4 weeks) while submerged in 17 °C seawater. [ABSTRACT FROM AUTHOR] Copyright of Polymer Degradation & Stability is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156520445&site=ehost-live
113,Substrate-regulated nanoscale friction of graphene.,Ali Beheshti,Materials Letters,0167577X,,Jun-18,221,,54,3,128852560,10.1016/j.matlet.2018.03.078,Elsevier B.V.,Article,GRAPHENE; NANOCHEMISTRY; ATOMIC force microscopy; NANOELECTROMECHANICAL systems; NANOELECTRONICS,Atomic force microscopy; Friction; Graphene; Substrate-regulation,"In the present study, nanotribological measurements were performed via atomic force microscopy on Si/SiO 2 -supported graphene monolayers with varying oxide layer thicknesses. The observations uncovered significant discrepancies in resulting friction forces between each graphene sample. Nanoscale interfacial friction forces were observed to increase from ∼0.49 nN to ∼1.00 nN when the oxide layer thickness was increased from 90 nm to 300 nm. The findings were determined to be the result of increased phonon scattering which is responsible for the removal of the vibrational reduction of nanoscale friction. Such discrepancies in friction forces points toward the potential tunability of nanoscale friction in supported graphene. [ABSTRACT FROM AUTHOR] Copyright of Materials Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128852560&site=ehost-live
114,Tribology of incoloy 800HT for nuclear reactors under helium environment at elevated temperatures.,Ali Beheshti,Wear,431648,,Oct-19,436,,N.PAG,1,138726399,10.1016/j.wear.2019.203022,Elsevier B.V.,Article,NUCLEAR reactors; GAS cooled reactors; HIGH temperatures; HELIUM; SCANNING electron microscopy; Industrial Gas Manufacturing; Power Boiler and Heat Exchanger Manufacturing,Elevated temperature; Friction and wear; Helium tribology; Incoloy 800HT; Nuclear reactor,"Nickel-based alloy 800HT is considered one of the main candidate alloys for nuclear reactors with gas cooled high-temperature environment and, therefore, it is necessary to have a thorough understanding of the alloy tribological response for obtaining optimum operating and loading conditions. The current study investigated the wear and friction behavior of alloy 800HT using a customized high temperature tribometer to simulate the environment of helium cooled reactor up to 750 °C. The effect of temperature, contact load, environment, sliding distance and sliding speed on the alloy friction and wear were studied. To generalize the study for other applications, an investigation is also performed in air environment. The friction and wear coefficients have the highest values at high temperature helium atmosphere where the formation and stability of the oxide scales play an important role. Optical and contact profiling, scanning electron microscopy, as well as energy dispersive spectroscopy techniques were utilized to study the surface oxide. The analysis showed the presence of Fe-Cr-Ni rich oxide both in air and helium. The protective glazed layer did not form in helium in any condition, whereas in air and under specific conditions a stable protective oxide layer was observed. • Tribological experiments of Incoloy 800HT in He environment up to 750 °C. • At 500 °C and 750 °C He, friction and wear stay high for all conditions. • Glazed oxides reduce friction and wear in high temperature air, depending on the contact pressure. [ABSTRACT FROM AUTHOR] Copyright of Wear is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138726399&site=ehost-live
115,Wear in superelastic shape memory alloys: A thermomechanical analysis.,Ali Beheshti,Wear,431648,,Jan-22,488,,N.PAG,1,153848026,10.1016/j.wear.2021.204139,Elsevier B.V.,Article,ALLOY analysis; SHAPE memory alloys; SLIDING wear; MATERIAL plasticity; WEAR resistance; PHASE transitions,Indentation; Shape memory alloy; Thermomechanical behavior; Wear,"Shape Memory Alloys (SMAs) have shown superior mechanical properties including improved wear resistance compared to their conventional counterparts of the same surface hardness. Available wear models cannot estimate a wear coefficient for SMAs, since they do not consider the effects of phase transformation and the related phenomena. This study presents a methodology considering thermomechanical behavior of shape memory alloys in sliding wear. Wear analysis is carried out for a superelastic SMA under different loads and temperatures, and the results are validated by experimental findings. The proposed approach introduces a constant wear coefficient at different loads and temperatures supporting its reliability for quantitative description of wear in an SMA. • A novel thermomechanical model is presented for sliding wear in shape memory alloys. • Phase transformation and plastic deformations are quantified to predict wear mass. • A unified wear coefficient for any arbitrary load and temperature is achieved. • Theoretical results are validated by empirical findings for NiTi shape memory alloy. [ABSTRACT FROM AUTHOR] Copyright of Wear is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153848026&site=ehost-live
116,An accelerated algorithm for discrete stochastic simulation of reaction-diffusion systems using gradient-based diffusion and tau-leaping.,Kim Blackwell,Journal of Chemical Physics,219606,,4/21/11,134,15,154103,13,60119337,10.1063/1.3572335,American Institute of Physics,Article,ALGORITHMS; STOCHASTIC processes; SIMULATION methods & models; CYTOLOGY; REACTION-diffusion equations; POISSON processes; NUMERICAL analysis; DISTRIBUTION (Probability theory),,"Stochastic simulation of reaction-diffusion systems enables the investigation of stochastic events arising from the small numbers and heterogeneous distribution of molecular species in biological cells. Stochastic variations in intracellular microdomains and in diffusional gradients play a significant part in the spatiotemporal activity and behavior of cells. Although an exact stochastic simulation that simulates every individual reaction and diffusion event gives a most accurate trajectory of the system's state over time, it can be too slow for many practical applications. We present an accelerated algorithm for discrete stochastic simulation of reaction-diffusion systems designed to improve the speed of simulation by reducing the number of time-steps required to complete a simulation run. This method is unique in that it employs two strategies that have not been incorporated in existing spatial stochastic simulation algorithms. First, diffusive transfers between neighboring subvolumes are based on concentration gradients. This treatment necessitates sampling of only the net or observed diffusion events from higher to lower concentration gradients rather than sampling all diffusion events regardless of local concentration gradients. Second, we extend the non-negative Poisson tau-leaping method that was originally developed for speeding up nonspatial or homogeneous stochastic simulation algorithms. This method calculates each leap time in a unified step for both reaction and diffusion processes while satisfying the leap condition that the propensities do not change appreciably during the leap and ensuring that leaping does not cause molecular populations to become negative. Numerical results are presented that illustrate the improvement in simulation speed achieved by incorporating these two new strategies. [ABSTRACT FROM AUTHOR] Copyright of Journal of Chemical Physics is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60119337&site=ehost-live
117,An efficient stochastic diffusion algorithm for modeling second messengers in dendrites and spines,Kim Blackwell,Journal of Neuroscience Methods,1650270,,Oct-06,157,1,142,12,22131184,10.1016/j.jneumeth.2006.04.003,Elsevier B.V.,Article,SKELETON; SOLUTION (Chemistry); BONES; NERVOUS system,Computer software; Diffusion; Second messenger; Stochastic,"Abstract: Intracellular signaling pathways, which encompass both biochemical reactions and second messenger diffusion, interact non-linearly with neuronal membrane properties in their role as essential intermediaries for synaptic plasticity and neuromodulation. Computational modeling is a productive approach for investigating these phenomena; however, most current strategies for modeling neurons exclude signaling pathways. To overcome this deficiency, a new algorithm is presented to simulate stochastic diffusion in a highly efficient manner. The gain in speed is obtained by considering collections of molecules, instead of tracking the movement of individual molecules. The probability of a molecule leaving a spatially discrete compartment is used to create a lookup table that stores the probability of k m molecules leaving the compartment as a function of the total number of molecules in the compartment. During the simulation, the number of molecules leaving the compartment is determined using a uniform random number as an index into the lookup table. Simulations illustrate the accuracy of this algorithm by comparing it with the theoretical solution for deterministic diffusion. Additional simulations show how spines on a dendritic branch compartmentalize diffusible molecules. The efficiency of the algorithm is sufficient to allow simulation of second messenger pathways in a multitude of spines on an entire neuron. [Copyright &y& Elsevier] Copyright of Journal of Neuroscience Methods is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22131184&site=ehost-live
118,Asynchronous τ-leaping.,Kim Blackwell,Journal of Chemical Physics,219606,,2016,144,12,125104-1,18,114171524,10.1063/1.4944575,American Institute of Physics,Article,CELLULAR signal transduction; CELL physiology; STOCHASTIC processes; REACTION-diffusion equations; SIMULATION methods & models,,"Stochastic simulation of cell signaling pathways and genetic regulatory networks has contributed to the understanding of cell function; however, investigation of larger, more complicated systems requires computationally efficient algorithms. τ-leaping methods, which improve efficiency when some molecules have high copy numbers, either use a fixed leap size, which does not adapt to changing state, or recalculate leap size at a heavy computational cost.We present a hybrid simulation method for reaction-diffusion systems which combines exact stochastic simulation and τ-leaping in a dynamic way. Putative times of events are stored in a priority queue, which reduces the cost of each step of the simulation. For every reaction and diffusion channel at each step of the simulation the more efficient of an exact stochastic event or a τ-leap is chosen. This new approach removes the inherent trade-off between speed and accuracy in stiff systems which was present in all τ-leaping methods by allowing each reaction channel to proceed at its own pace. Both directions of reversible reactions and diffusion are combined in a single event, allowing bigger leaps to be taken. This improves efficiency for systems near equilibrium where forward and backward events are approximately equally frequent. Comparison with existing algorithms and behaviour for five test cases of varying complexity shows that the new method is almost as accurate as exact stochastic simulation, scales well for large systems, and for various problems can be significantly faster than τ-leaping. [ABSTRACT FROM AUTHOR] Copyright of Journal of Chemical Physics is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114171524&site=ehost-live
119,Calcium dynamics predict direction of synaptic plasticity in striatal spiny projection neurons.,Kim Blackwell,European Journal of Neuroscience,0953816X,,Apr-17,45,8,1044,13,122539089,10.1111/ejn.13287,Wiley-Blackwell,Article,SENSORIMOTOR cortex; NEUROPLASTICITY; INTRACELLULAR calcium; DOPAMINE; CALCIUM in the body,basal ganglia; computational model; long‐term potentiation/long‐term depression; long-term potentiation/long-term depression; spike timing‐dependent plasticity; spike timing-dependent plasticity; striatum,"The striatum is a major site of learning and memory formation for sensorimotor and cognitive association. One of the mechanisms used by the brain for memory storage is synaptic plasticity - the long-lasting, activity-dependent change in synaptic strength. All forms of synaptic plasticity require an elevation in intracellular calcium, and a common hypothesis is that the amplitude and duration of calcium transients can determine the direction of synaptic plasticity. The utility of this hypothesis in the striatum is unclear in part because dopamine is required for striatal plasticity and in part because of the diversity in stimulation protocols. To test whether calcium can predict plasticity direction, we developed a calcium-based plasticity rule using a spiny projection neuron model with sophisticated calcium dynamics including calcium diffusion, buffering and pump extrusion. We utilized three spike timing-dependent plasticity ( STDP) induction protocols, in which postsynaptic potentials are paired with precisely timed action potentials and the timing of such pairing determines whether potentiation or depression will occur. Results show that despite the variation in calcium dynamics, a single, calcium-based plasticity rule, which explicitly considers duration of calcium elevations, can explain the direction of synaptic weight change for all three STDP protocols. Additional simulations show that the plasticity rule correctly predicts the NMDA receptor dependence of long-term potentiation and the L-type channel dependence of long-term depression. By utilizing realistic calcium dynamics, the model reveals mechanisms controlling synaptic plasticity direction, and shows that the dynamics of calcium, not just calcium amplitude, are crucial for synaptic plasticity. [ABSTRACT FROM AUTHOR] Copyright of European Journal of Neuroscience is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122539089&site=ehost-live
120,Combining hypothesis- and data-driven neuroscience modeling in FAIR workflows.,Kim Blackwell,eLife,2050084X,,7/6/22,,,1,31,158077558,10.7554/eLife.69013,"eLife Sciences Publications, Ltd.",Article,,,"Modeling in neuroscience occurs at the intersection of different points of view and approaches. Typically, hypothesis-driven modeling brings a question into focus so that a model is constructed to investigate a specific hypothesis about how the system works or why certain phenomena are observed. Data-driven modeling, on the other hand, follows a more unbiased approach, with model construction informed by the computationally intensive use of data. At the same time, researchers employ models at different biological scales and at different levels of abstraction. Combining these models while validating them against experimental data increases understanding of the multiscale brain. However, a lack of interoperability, transparency, and reusability of both models and the workflows used to construct them creates barriers for the integration of models representing different biological scales and built using different modeling philosophies. We argue that the same imperatives that drive resources and policy for data – such as the FAIR (Findable, Accessible, Interoperable, Reusable) principles – also support the integration of different modeling approaches. The FAIR principles require that data be shared in formats that are Findable, Accessible, Interoperable, and Reusable. Applying these principles to models and modeling workflows, as well as the data used to constrain and validate them, would allow researchers to find, reuse, question, validate, and extend published models, regardless of whether they are implemented phenomenologically or mechanistically, as a few equations or as a multiscale, hierarchical system. To illustrate these ideas, we use a classical synaptic plasticity model, the Bienenstock–Cooper–Munro rule, as an example due to its long history, different levels of abstraction, and implementation at many scales. [ABSTRACT FROM AUTHOR] Copyright of eLife is the property of eLife Sciences Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158077558&site=ehost-live
121,Control of βAR- and N-methyl-D-aspartate (NMDA) Receptor-Dependent cAMP Dynamics in Hippocampal Neurons.,Kim Blackwell,PLoS Computational Biology,1553734X,,2/22/16,12,2,1,32,113218256,10.1371/journal.pcbi.1004735,Public Library of Science,Article,NORADRENALINE; BETA adrenoceptors; INTRACELLULAR calcium; LONG-term potentiation; METHYL aspartate receptors; PROTEIN kinases,Adenylyl cyclase; Animal cells; Biochemistry; Biology and life sciences; Cell biology; Cell membranes; Cellular structures and organelles; Cellular types; Drugs; Enzymes; Enzymology; Fluorescence resonance energy transfer; Fluorophotometry; Isoproterenol; Lyases; Medicine and health sciences; Neurites; Neuronal dendrites; Neurons; Pharmacology; Phosphorylation; Post-translational modification; Proteins; Research and analysis methods; Research Article; Simulation and modeling; Spectrophotometry; Spectrum analysis techniques,"Norepinephrine, a neuromodulator that activates β-adrenergic receptors (βARs), facilitates learning and memory as well as the induction of synaptic plasticity in the hippocampus. Several forms of long-term potentiation (LTP) at the Schaffer collateral CA1 synapse require stimulation of both βARs and N-methyl-D-aspartate receptors (NMDARs). To understand the mechanisms mediating the interactions between βAR and NMDAR signaling pathways, we combined FRET imaging of cAMP in hippocampal neuron cultures with spatial mechanistic modeling of signaling pathways in the CA1 pyramidal neuron. Previous work implied that cAMP is synergistically produced in the presence of the βAR agonist isoproterenol and intracellular calcium. In contrast, we show that when application of isoproterenol precedes application of NMDA by several minutes, as is typical of βAR-facilitated LTP experiments, the average amplitude of the cAMP response to NMDA is attenuated compared with the response to NMDA alone. Models simulations suggest that, although the negative feedback loop formed by cAMP, cAMP-dependent protein kinase (PKA), and type 4 phosphodiesterase may be involved in attenuating the cAMP response to NMDA, it is insufficient to explain the range of experimental observations. Instead, attenuation of the cAMP response requires mechanisms upstream of adenylyl cyclase. Our model demonstrates that Gs-to-Gi switching due to PKA phosphorylation of βARs as well as Gi inhibition of type 1 adenylyl cyclase may underlie the experimental observations. This suggests that signaling by β-adrenergic receptors depends on temporal pattern of stimulation, and that switching may represent a novel mechanism for recruiting kinases involved in synaptic plasticity and memory. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=113218256&site=ehost-live
122,Dendritic diameter influences the rate and magnitude of hippocampal cAMP and PKA transients during β-adrenergic receptor activation.,Kim Blackwell,Neurobiology of Learning & Memory,10747427,,Feb-17,138,,10,11,121491862,10.1016/j.nlm.2016.08.006,Academic Press Inc.,Article,ADRENERGIC receptors; HIPPOCAMPUS (Brain); PROTEIN kinases; CARBONIC anhydrase; ISOPROTERENOL,β-adrenergic receptors; cAMP; Computational modeling; FRET; PKA; Signaling dynamics,"In the hippocampus, cyclic-adenosine monophosphate (cAMP) and cAMP-dependent protein kinase (PKA) form a critical signaling cascade required for long-lasting synaptic plasticity, learning and memory. Plasticity and memory are known to occur following pathway-specific changes in synaptic strength that are thought to result from spatially and temporally coordinated intracellular signaling events. To better understand how cAMP and PKA dynamically operate within the structural complexity of hippocampal neurons, we used live two-photon imaging and genetically-encoded fluorescent biosensors to monitor cAMP levels or PKA activity in CA1 neurons of acute hippocampal slices. Stimulation of β-adrenergic receptors (isoproterenol) or combined activation of adenylyl cyclase (forskolin) and inhibition of phosphodiesterase (IBMX) produced cAMP transients with greater amplitude and rapid on-rates in intermediate and distal dendrites compared to somata and proximal dendrites. In contrast, isoproterenol produced greater PKA activity in somata and proximal dendrites compared to intermediate and distal dendrites, and the on-rate of PKA activity did not differ between compartments. Computational models show that our observed compartmental difference in cAMP can be reproduced by a uniform distribution of PDE4 and a variable density of adenylyl cyclase that scales with compartment size to compensate for changes in surface to volume ratios. However, reproducing our observed compartmental difference in PKA activity required enrichment of protein phosphatase in small compartments; neither reduced PKA subunits nor increased PKA substrates were sufficient. Together, our imaging and computational results show that compartment diameter interacts with rate-limiting components like adenylyl cyclase, phosphodiesterase and protein phosphatase to shape the spatial and temporal components of cAMP and PKA signaling in CA1 neurons and suggests that small neuronal compartments are most sensitive to cAMP signals whereas large neuronal compartments accommodate a greater dynamic range in PKA activity. [ABSTRACT FROM AUTHOR] Copyright of Neurobiology of Learning & Memory is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121491862&site=ehost-live
123,From membrane receptors to protein synthesis and actin cytoskeleton: Mechanisms underlying long lasting forms of synaptic plasticity.,Kim Blackwell,Seminars in Cell & Developmental Biology,10849521,,Nov-19,95,,120,10,140095945,10.1016/j.semcdb.2019.01.006,Academic Press Inc.,Article,NEUROPLASTICITY; PROTEIN synthesis; MEMBRANE proteins; PROTEIN receptors; CYTOSKELETAL proteins; DENDRITIC spines; CYTOSKELETON,Actin; Computational model; LTD; LTP; Synaptic tagging; Translation,"Synaptic plasticity, the activity dependent change in synaptic strength, forms the molecular foundation of learning and memory. Synaptic plasticity includes structural changes, with spines changing their size to accomodate insertion and removal of postynaptic receptors, which are correlated with functional changes. Of particular relevance for memory storage are the long lasting forms of synaptic plasticity which are protein synthesis dependent. Due to the importance of spine structural plasticity and protein synthesis, this review focuses on the signaling pathways that connect synaptic stimulation with regulation of protein synthesis and remodeling of the actin cytoskeleton. We also review computational models that implement novel aspects of molecular signaling in synaptic plasticity, such as the role of neuromodulators and spatial microdomains, as well as highlight the need for computational models that connect activation of memory kinases with spine actin dynamics. [ABSTRACT FROM AUTHOR] Copyright of Seminars in Cell & Developmental Biology is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140095945&site=ehost-live
124,Gap Junctions between Striatal Fast-Spiking Interneurons Regulate Spiking Activity and Synchronization as a Function of Cortical Activity.,Kim Blackwell,Journal of Neuroscience,2706474,,4/22/09,29,16,5276,11,38333059,10.1523/JNEUROSCI.6031-08.2009,Society for Neuroscience,Article,INTERNEURONS; GAP junctions (Cell biology); BASAL ganglia; SYNAPSES; EFFERENT pathways; NEURAL circuitry; NEUROSCIENCES,,"Striatal fast-spiking (FS) interneurons are interconnected by gap junctions into sparsely connected networks. As demonstrated for cortical FS interneurons, these gap junctions in the striatum may cause synchronized spiking, which would increase the influence that FS neurons have on spiking by the striatal medium spiny (MS) neurons. Dysfunction of the basal ganglia is characterized by changes in synchrony or periodicity, thus gap junctions between FS interneurons may modulate synchrony and thereby influence behavior such as reward learning and motor control. To explore the roles of gap junctions on activity and spike synchronization in a striatal FS population, we built a network model of FS interneurons. Each FS connects to 30-40% of its neighbors, as found experimentally, and each FS interneuron in the network is activated by simulated corticostriatal synaptic inputs. Our simulations show that the proportion of synchronous spikes in FS networks with gap junctions increases with increased conductance of the electrical synapse; however, the synchronization effects are moderate for experimentally estimated conductances. Instead, the main tendency is that the presence of gap junctions reduces the total number of spikes generated in response to synaptic inputs in the network. The reduction in spike firing is due to shunting through the gap junctions; which is minimized or absent when the neurons receive coincident inputs. Together these findings suggest that a population of electrically coupled FS interneurons may function collectively as input detectors that are especially sensitive to synchronized synaptic inputs received from the cortex. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroscience is the property of Society for Neuroscience and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=38333059&site=ehost-live
125,Improved spatial direct method with gradient-based diffusion to retain full diffusive fluctuations.,Kim Blackwell,Journal of Chemical Physics,219606,,10/21/12,137,15,154111,14,82712881,10.1063/1.4758459,American Institute of Physics,Article,DIFFUSION; FLUCTUATIONS (Physics); STOCHASTIC analysis; SIMULATION methods & models; ALGORITHMS; NUMERICAL analysis,,"The spatial direct method with gradient-based diffusion is an accelerated stochastic reaction-diffusion simulation algorithm that treats diffusive transfers between neighboring subvolumes based on concentration gradients. This recent method achieved a marked improvement in simulation speed and reduction in the number of time-steps required to complete a simulation run, compared with the exact algorithm, by sampling only the net diffusion events, instead of sampling all diffusion events. Although the spatial direct method with gradient-based diffusion gives accurate means of simulation ensembles, its gradient-based diffusion strategy results in reduced fluctuations in populations of diffusive species. In this paper, we present a new improved algorithm that is able to anticipate all possible microscopic fluctuations due to diffusive transfers in the system and incorporate this information to retain the same degree of fluctuations in populations of diffusing species as the exact algorithm. The new algorithm also provides a capability to set the desired level of fluctuation per diffusing species, which facilitates adjusting the balance between the degree of exactness in simulation results and the simulation speed. We present numerical results that illustrate the recovery of fluctuations together with the accuracy and efficiency of the new algorithm. [ABSTRACT FROM AUTHOR] Copyright of Journal of Chemical Physics is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=82712881&site=ehost-live
126,Intrinsic and network mechanisms involved in balanced firing and striatal synchrony during dopamine depletion.,Kim Blackwell,BMC Neuroscience,14712202,,2013,14,Suppl 1,1,1,89723441,10.1186/1471-2202-14-S1-P27,BioMed Central,Abstract,DOPAMINE; NEUROSCIENCES,,"An abstract of the article ""Intrinsic and network mechanisms involved in balanced firing and striatal synchrony during dopamine depletion"" by Sriraman Damodaran and colleague is presented.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89723441&site=ehost-live
127,Modelling the molecular mechanisms of synaptic plasticity using systems biology approaches.,Kim Blackwell,Nature Reviews Neuroscience,1471003X,,Apr-10,11,4,239,13,48642492,10.1038/nrn2807,Springer Nature,journal article,"NEUROPLASTICITY; COMPUTER simulation; HIPPOCAMPUS (Brain); NEUROSCIENCES; ION channels; DIAGNOSTIC imaging; CALCIUM metabolism; ANIMALS; BIOLOGICAL models; CELLULAR signal transduction; NERVOUS system; RESEARCH funding; BIOINFORMATICS; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",,"Synaptic plasticity is thought to underlie learning and memory, but the complexity of the interactions between the ion channels, enzymes and genes that are involved in synaptic plasticity impedes a deep understanding of this phenomenon. Computer modelling has been used to investigate the information processing that is performed by the signalling pathways involved in synaptic plasticity in principal neurons of the hippocampus, striatum and cerebellum. In the past few years, new software developments that combine computational neuroscience techniques with systems biology techniques have allowed large-scale, kinetic models of the molecular mechanisms underlying long-term potentiation and long-term depression. We highlight important advancements produced by these quantitative modelling efforts and introduce promising approaches that use advancements in live-cell imaging. [ABSTRACT FROM AUTHOR] Copyright of Nature Reviews Neuroscience is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=48642492&site=ehost-live
128,Molecular mechanisms underlying striatal synaptic plasticity: relevance to chronic alcohol consumption and seeking.,Kim Blackwell,European Journal of Neuroscience,0953816X,,Mar-19,49,5,768,16,135444753,10.1111/ejn.13919,Wiley-Blackwell,Article,NEUROPLASTICITY; ALCOHOL drinking; LONG-term synaptic depression; ADENYLATE cyclase; DENDRITIC spines; DOPAMINE receptors; Drinking Places (Alcoholic Beverages),basal ganglia; computational model; long‐term depression; long‐term potentiation; signaling pathways; striatum,"The striatum, the input structure of the basal ganglia, is a major site of learning and memory for goal‐directed actions and habit formation. Spiny projection neurons of the striatum integrate cortical, thalamic, and nigral inputs to learn associations, with cortico‐striatal synaptic plasticity as a learning mechanism. Signaling molecules implicated in synaptic plasticity are altered in alcohol withdrawal, which may contribute to overly strong learning and increased alcohol seeking and consumption. To understand how interactions among signaling molecules produce synaptic plasticity, we implemented a mechanistic model of signaling pathways activated by dopamine D1 receptors, acetylcholine receptors, and glutamate. We use our novel, computationally efficient simulator, NeuroRD, to simulate stochastic interactions both within and between dendritic spines. Dopamine release during theta burst and 20‐Hz stimulation was extrapolated from fast‐scan cyclic voltammetry data collected in mouse striatal slices. Our results show that the combined activity of several key plasticity molecules correctly predicts the occurrence of either LTP, LTD, or no plasticity for numerous experimental protocols. To investigate spatial interactions, we stimulate two spines, either adjacent or separated on a 20‐μm dendritic segment. Our results show that molecules underlying LTP exhibit spatial specificity, whereas 2‐arachidonoylglycerol exhibits a spatially diffuse elevation. We also implement changes in NMDA receptors, adenylyl cyclase, and G protein signaling that have been measured following chronic alcohol treatment. Simulations under these conditions suggest that the molecular changes can predict changes in synaptic plasticity, thereby accounting for some aspects of alcohol use disorder. We created a comprehensive model of signaling pathways underlying synaptic plasticity. The model shows that a combination of molecules in the spine and dendrite predicts the development of LTP or LTD. [ABSTRACT FROM AUTHOR] Copyright of European Journal of Neuroscience is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135444753&site=ehost-live
129,Molecular mechanisms underlying striatal synaptic plasticity: relevance to chronic alcohol consumption and seeking.,Kim Blackwell,European Journal of Neuroscience,0953816X,,Mar-19,49,6,768,16,135516691,10.1111/ejn.13919,Wiley-Blackwell,Article,NEUROPLASTICITY; ALCOHOL drinking; LONG-term synaptic depression; ADENYLATE cyclase; DENDRITIC spines; DOPAMINE receptors; Drinking Places (Alcoholic Beverages),basal ganglia; computational model; long‐term depression; long‐term potentiation; signaling pathways; striatum,"The striatum, the input structure of the basal ganglia, is a major site of learning and memory for goal‐directed actions and habit formation. Spiny projection neurons of the striatum integrate cortical, thalamic, and nigral inputs to learn associations, with cortico‐striatal synaptic plasticity as a learning mechanism. Signaling molecules implicated in synaptic plasticity are altered in alcohol withdrawal, which may contribute to overly strong learning and increased alcohol seeking and consumption. To understand how interactions among signaling molecules produce synaptic plasticity, we implemented a mechanistic model of signaling pathways activated by dopamine D1 receptors, acetylcholine receptors, and glutamate. We use our novel, computationally efficient simulator, NeuroRD, to simulate stochastic interactions both within and between dendritic spines. Dopamine release during theta burst and 20‐Hz stimulation was extrapolated from fast‐scan cyclic voltammetry data collected in mouse striatal slices. Our results show that the combined activity of several key plasticity molecules correctly predicts the occurrence of either LTP, LTD, or no plasticity for numerous experimental protocols. To investigate spatial interactions, we stimulate two spines, either adjacent or separated on a 20‐μm dendritic segment. Our results show that molecules underlying LTP exhibit spatial specificity, whereas 2‐arachidonoylglycerol exhibits a spatially diffuse elevation. We also implement changes in NMDA receptors, adenylyl cyclase, and G protein signaling that have been measured following chronic alcohol treatment. Simulations under these conditions suggest that the molecular changes can predict changes in synaptic plasticity, thereby accounting for some aspects of alcohol use disorder. We created a comprehensive model of signaling pathways underlying synaptic plasticity. The model shows that a combination of molecules in the spine and dendrite predicts the development of LTP or LTD. [ABSTRACT FROM AUTHOR] Copyright of European Journal of Neuroscience is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135516691&site=ehost-live
130,Sensitivity to interstimulus interval due to calcium interactions in the Purkinje cell spines,Kim Blackwell,Neurocomputing,9252312,,Jun-02,44-46,,13,6,7826431,10.1016/S0925-2312(02)00361-2,Elsevier B.V.,Article,CLASSICAL conditioning; PURKINJE cells,<f>Ca2+</f> buffers; Classical conditioning; IP3 induced <f>Ca2+</f> release; LTD; Purkinje cells,"Pairing specific LTD (PSD) is produced by paired parallel fiber (PF) and climbing fiber (CF) stimulation and requires <f>Ca2+</f> elevation. CF or PF activation cause <f>Ca2+</f> increase through voltage dependent channels and IP3 induced <f>Ca2+</f> release, respectively. We developed a model of <f>Ca2+</f> dynamics in Purkinje cell spines to investigate why paired PF–CF activation is necessary for PSD. Simulations show a supralinear increase of the <f>Ca2+</f> signal if the CF input occurs in a restricted time interval following the PF input. <f>Ca2+</f> buffers significantly contribute to this phenomenon. This mechanism may be involved in the requirement of temporal specificity in classical conditioning. [Copyright &y& Elsevier] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=7826431&site=ehost-live
131,Signaling pathways underlying striatal synaptic plasticity and reward learning.,Kim Blackwell,BMC Neuroscience,14712202,,2008 Supplement 1,9,,L3,1,60732461,10.1186/1471-2202-9-S1-L3,BioMed Central,Article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60732461&site=ehost-live
132,Subcellular Location of PKA Controls Striatal Plasticity: Stochastic Simulations in Spiny Dendrites.,Kim Blackwell,PLoS Computational Biology,1553734X,,Feb-12,8,2,1,19,73462103,10.1371/journal.pcbi.1002383,Public Library of Science,Article,CYCLIC-AMP-dependent protein kinase; NEUROPLASTICITY; STOCHASTIC analysis; DENDRITES; DOPAMINE; SPATIAL analysis (Statistics); ADENYLATE cyclase,,"Dopamine release in the striatum has been implicated in various forms of reward dependent learning. Dopamine leads to production of cAMP and activation of protein kinase A (PKA), which are involved in striatal synaptic plasticity and learning. PKA and its protein targets are not diffusely located throughout the neuron, but are confined to various subcellular compartments by anchoring molecules such as A-Kinase Anchoring Proteins (AKAPs). Experiments have shown that blocking the interaction of PKA with AKAPs disrupts its subcellular location and prevents LTP in the hippocampus and striatum; however, these experiments have not revealed whether the critical function of anchoring is to locate PKA near the cAMP that activates it or near its targets, such as AMPA receptors located in the post-synaptic density. We have developed a large scale stochastic reaction-diffusion model of signaling pathways in a medium spiny projection neuron dendrite with spines, based on published biochemical measurements, to investigate this question and to evaluate whether dopamine signaling exhibits spatial specificity post-synaptically. The model was stimulated with dopamine pulses mimicking those recorded in response to reward. Simulations show that PKA colocalization with adenylate cyclase, either in the spine head or in the dendrite, leads to greater phosphorylation of DARPP-32 Thr34 and AMPA receptor GluA1 Ser845 than when PKA is anchored away from adenylate cyclase. Simulations further demonstrate that though cAMP exhibits a strong spatial gradient, diffusible DARPP-32 facilitates the spread of PKA activity, suggesting that additional inactivation mechanisms are required to produce spatial specificity of PKA activity. INSET: Author Summary. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=73462103&site=ehost-live
133,Temporal pattern and synergy influence activity of ERK signaling pathways during L-LTP induction.,Kim Blackwell,eLife,2050084X,,8/13/21,,,1,28,152068546,10.7554/eLife.64644,"eLife Sciences Publications, Ltd.",Article,LONG-term potentiation; EXTRACELLULAR signal-regulated kinases,,"Long-lasting long-term potentiation (L-LTP) is a cellular mechanism of learning and memory storage. Studies have demonstrated a requirement for extracellular signal-regulated kinase (ERK) activation in L-LTP produced by a diversity of temporal stimulation patterns. Multiple signaling pathways converge to activate ERK, with different pathways being required for different stimulation patterns. To answer whether and how different temporal patterns select different signaling pathways for ERK activation, we developed a computational model of five signaling pathways (including two novel pathways) leading to ERK activation during L-LTP induction. We show that calcium and cAMP work synergistically to activate ERK and that stimuli given with large intertrial intervals activate more ERK than shorter intervals. Furthermore, these pathways contribute to different dynamics of ERK activation. These results suggest that signaling pathways with different temporal sensitivities facilitate ERK activation to diversity of temporal patterns. [ABSTRACT FROM AUTHOR] Copyright of eLife is the property of eLife Sciences Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152068546&site=ehost-live
134,The road to ERK activation: Do neurons take alternate routes?,Kim Blackwell,Cellular Signalling,8986568,,Apr-20,68,,N.PAG,1,142476415,10.1016/j.cellsig.2020.109541,Elsevier B.V.,Article,G protein coupled receptors; NEURAL pathways; SCAFFOLD proteins; NEURONS; GROWTH factors; COMPUTATIONAL neuroscience,,"The ERK cascade is a central signaling pathway that regulates a wide variety of cellular processes including proliferation, differentiation, learning and memory, development, and synaptic plasticity. A wide range of inputs travel from the membrane through different signaling pathway routes to reach activation of one set of output kinases, ERK1&2. The classical ERK activation pathway beings with growth factor activation of receptor tyrosine kinases. Numerous G-protein coupled receptors and ionotropic receptors also lead to ERK through increases in the second messengers calcium and cAMP. Though both types of pathways are present in diverse cell types, a key difference is that most stimuli to neurons, e.g. synaptic inputs, are transient, on the order of milliseconds to seconds, whereas many stimuli acting on non-neural tissue, e.g. growth factors, are longer duration. The ability to consolidate these inputs to regulate the activation of ERK in response to diverse signals raises the question of which factors influence the difference in ERK activation pathways. This review presents both experimental studies and computational models aimed at understanding the control of ERK activation and whether there are fundamental differences between neurons and other cells. Our main conclusion is that differences between cell types are quite subtle, often related to differences in expression pattern and quantity of some molecules such as Raf isoforms. In addition, the spatial location of ERK is critical, with regulation by scaffolding proteins producing differences due to colocalization of upstream molecules that may differ between neurons and other cells. • Some functions of ERK1 and ERK2 are redundant, whereas other ERK2 functions are unique. • ERK scaffolds both enhance ERK activation and segregates pools of ERK. • BDNF is required for long term potentiation in response to weak stimulation protocols. • G protein coupled receptors activate ERK through diverse, cell type dependent pathways. [ABSTRACT FROM AUTHOR] Copyright of Cellular Signalling is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142476415&site=ehost-live
135,β-adrenergic signaling broadly contributes to LTP induction.,Kim Blackwell,PLoS Computational Biology,1553734X,,7/24/17,13,7,1,32,124275127,10.1371/journal.pcbi.1005657,Public Library of Science,Article,LONG-term potentiation; KINASES; PROTEIN kinases; MOLECULES; DENDRITES; NEUROPLASTICITY,Amines; Animal cells; Biochemistry; Biogenic amines; Biology and life sciences; Calcium signaling; Catecholamines; Cell biology; Cell signaling; Cellular neuroscience; Cellular types; Chemical compounds; Chemistry; Developmental neuroscience; Enzymes; Enzymology; Functional electrical stimulation; Hormones; Medicine and health sciences; Neurochemistry; Neuronal dendrites; Neurons; Neuroscience; Neurotransmitters; Norepinephrine; Organic chemistry; Organic compounds; Phosphatases; Phosphorylation; Physical sciences; Post-translational modification; Proteins; Research and analysis methods; Research Article; Signal transduction; Simulation and modeling; Surgical and invasive medical procedures; Synaptic plasticity,"Long-lasting forms of long-term potentiation (LTP) represent one of the major cellular mechanisms underlying learning and memory. One of the fundamental questions in the field of LTP is why different molecules are critical for long-lasting forms of LTP induced by diverse experimental protocols. Further complexity stems from spatial aspects of signaling networks, such that some molecules function in the dendrite and some are critical in the spine. We investigated whether the diverse experimental evidence can be unified by creating a spatial, mechanistic model of multiple signaling pathways in hippocampal CA1 neurons. Our results show that the combination of activity of several key kinases can predict the occurrence of long-lasting forms of LTP for multiple experimental protocols. Specifically Ca2+/calmodulin activated kinase II, protein kinase A and exchange protein activated by cAMP (Epac) together predict the occurrence of LTP in response to strong stimulation (multiple trains of 100 Hz) or weak stimulation augmented by isoproterenol. Furthermore, our analysis suggests that activation of the β-adrenergic receptor either via canonical (Gs-coupled) or non-canonical (Gi-coupled) pathways underpins most forms of long-lasting LTP. Simulations make the experimentally testable predictions that a complete antagonist of the β-adrenergic receptor will likely block long-lasting LTP in response to strong stimulation. Collectively these results suggest that converging molecular mechanisms allow CA1 neurons to flexibly utilize signaling mechanisms best tuned to temporal pattern of synaptic input to achieve long-lasting LTP and memory storage. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124275127&site=ehost-live
136,Colocalization of Protein Kinase A with Adenylyl Cyclase Enhances Protein Kinase A Activity during Induction of Long-Lasting Long-Term-Potentiation.,Kim Blackwell,PLoS Computational Biology,1553734X,,Jun-11,7,6,1,18,63154328,10.1371/journal.pcbi.1002084,Public Library of Science,Article,NEURONS; HIPPOCAMPUS (Brain); ADENYLATE cyclase; PROTEIN kinases; MOLECULES; CHEMICAL inhibitors,,"The ability of neurons to differentially respond to specific temporal and spatial input patterns underlies information storage in neural circuits. One means of achieving spatial specificity is to restrict signaling molecules to particular subcellular compartments using anchoring molecules such as A-Kinase Anchoring Proteins (AKAPs). Disruption of protein kinase A (PKA) anchoring to AKAPs impairs a PKA-dependent form of long term potentiation (LTP) in the hippocampus. To investigate the role of localized PKA signaling in LTP, we developed a stochastic reaction-diffusion model of the signaling pathways leading to PKA activation in CA1 pyramidal neurons. Simulations investigated whether the role of anchoring is to locate kinases near molecules that activate them, or near their target molecules. The results show that anchoring PKA with adenylyl cyclase (which produces cAMP that activates PKA) produces significantly greater PKA activity, and phosphorylation of both inhibitor-1 and AMPA receptor GluR1 subunit on S845, than when PKA is anchored apart from adenylyl cyclase. The spatial microdomain of cAMP was smaller than that of PKA suggesting that anchoring PKA near its source of cAMP is critical because inactivation by phosphodiesterase limits diffusion of cAMP. The prediction that the role of anchoring is to colocalize PKA near adenylyl cyclase was confirmed by experimentally rescuing the deficit in LTP produced by disruption of PKA anchoring using phosphodiesterase inhibitors. Additional experiments confirm the model prediction that disruption of anchoring impairs S845 phosphorylation produced by forskolin-induced synaptic potentiation. Collectively, these results show that locating PKA near adenylyl cyclase is a critical function of anchoring. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=63154328&site=ehost-live
137,Desynchronization of Fast-Spiking Interneurons Reduces α-Band Oscillations and Imbalance in Firing in the Dopamine-Depleted Striatum.,Kim Blackwell,Journal of Neuroscience,2706474,,1/21/15,35,3,1149,11,100598214,10.1523/JNEUROSCI.3490-14.2015,Society for Neuroscience,Article,INTERNEURONS; DOPAMINE; BASAL ganglia; PARKINSON'S disease; GAP junctions (Cell biology); PATHOLOGICAL physiology,beta-band oscillation; fast-spiking interneurons; gap junctions; Parkinson's disease; striatal imbalance; striatum,"Oscillations in the β-band (8 -30 Hz) that emerge in the output nuclei of the basal ganglia during Parkinson's disease, along with an imbalanced activation of the direct and indirect pathways, have been linked to the hypokinetic motor output associated with the disease. Although dopamine depletion causes a change in cellular and network properties in the striatum, it is unclear whether abnormal activity measured in the globus pallidus and substantia nigra pars reticulata is caused by abnormal striatal activity. Here we use a computational network model of medium spiny neurons (MSNs)--fast-spiking interneurons (FSIs), based on data from several mammalian species, and find that robust β-band oscillations and imbalanced firing emerge from implementation of changes to cellular and circuit properties caused by dopamine depletion. These changes include a reduction in connections between MSNs, a doubling of FSI inhibition to D2 MSNs, an increase in D2 MSN dendritic excitability, and a reduction in D2 MSN somatic excitability. The model reveals that the reduced decorrelation between MSNs attributable to weakened lateral inhibition enables the strong influence of synchronous FSIs on MSN firing and oscillations. Weakened lateral inhibition also produces an increased sensitivity of MSN output to cortical correlation, a condition relevant to the parkinsonian striatum. The oscillations of FSIs, in turn, are strongly modulated by fast electrical transmission between FSIs through gap junctions. These results suggest that pharmaceuticals that desynchronize FSI activity may provide a novel treatment for the enhanced β-band oscillations, imbalanced firing, and motor dysfunction in Parkinson's disease. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroscience is the property of Society for Neuroscience and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100598214&site=ehost-live
138,Dopamine Oppositely Modulates State Transitions in Striosome and Matrix Direct Pathway Striatal Spiny Neurons.,Kim Blackwell,Neuron,8966273,,Dec-20,108,6,1091,1,147909437,10.1016/j.neuron.2020.09.028,Cell Press,Article,DOPAMINE; CALCIUM channels; NEURONS; MATRICES; DOPAMINE receptors,2-photon imaging; dendrite; L-type calcium channel; model; plateau; state-transition; striatum; synaptic integration; up-state; voltage-gated calcium channel,"Corticostriatal synaptic integration is partitioned among striosome (patch) and matrix compartments of the dorsal striatum, allowing compartmentalized control of discrete aspects of behavior. Despite the significance of such organization, it's unclear how compartment-specific striatal output is dynamically achieved, particularly considering new evidence that overlap of afferents is substantial. We show that dopamine oppositely shapes responses to convergent excitatory inputs in mouse striosome and matrix striatal spiny projection neurons (SPNs). Activation of postsynaptic D1 dopamine receptors promoted the generation of long-lasting synaptically evoked ""up-states"" in matrix SPNs but opposed it in striosomes, which were more excitable under basal conditions. Differences in dopaminergic modulation were mediated, in part, by dendritic voltage-gated calcium channels (VGCCs): pharmacological manipulation of L-type VGCCs reversed compartment-specific responses to D1 receptor activation. These results support a novel mechanism for the selection of striatal circuit components, where fluctuating levels of dopamine shift the balance of compartment-specific striatal output. • Both striosome and matrix dSPNs support dendritically evoked somatic ""up-states"" • Dopamine oppositely modulates up-state length in striosome versus matrix dSPNs via D1Rs • Compartment-specific responses to D1R activation involve L-type Ca2+ channels • Changes in striatal dopamine may shift the balance of striosome versus matrix output Prager et al. show that dopamine promotes the maintenance of dendritically evoked ""up-states"" in mouse direct pathway matrix SPNs but opposes it in striosomes. This requires postsynaptic D1 receptors and involves differential engagement of L-type Ca2+ channels. These findings reveal a mechanism where fluctuations in dopamine may constrain compartment-specific striatal output. [ABSTRACT FROM AUTHOR] Copyright of Neuron is the property of Cell Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147909437&site=ehost-live
139,GABAergic control of backpropagating action potentials in striatal medium spiny neurons.,Kim Blackwell,BMC Neuroscience,14712202,,2008 Supplement 1,9,,1,1,38599810,10.1186/1471-2202-9-S1-P105,BioMed Central,Article,GABA; BACK propagation; ACTION potentials; VISUAL cortex; NEURONS; CALCIUM channels,,"Introduction Experiments have demonstrated the ability of action potentials to actively backpropagate in striatal medium spiny (MS) neurons, affecting the calcium levels in the dendrites. Increased calcium levels trigger changes in plasticity, which is important for learning and other functions. Studies in the hippocampus have shown that GABAergic input can modulate the backpropagation of action potentials from the soma to the distal dendrites. The MS neurons receive both proximal feedforward GABAergic inhibition from fast spiking interneurons (FS), and distal feedback inhibition from other neighbouring MS neurons. In the present study the effect of these GABAergic inputs on the dendritic calcium dynamics is investigated. Model A previously published MS model was reimplemented in GENESIS. The dendritic axial resistance and sodium conductances have been modified to better fit experimental results. For example, in the modified model, backpropagation of dendritic action potentials requires sodium channel activation, and fails if those channels are blocked. The MS neuron was activated by simulated AMPA/NMDA and GABAergic synaptic inputs, or by somatic current injections. Computational investigation In this study we compare the effect of FS and MS synaptic inhibition on the backpropagation of action potentials in the MS model. Preliminary results suggest that GABAerigic inputs in distal dendrites can decrease the backpropagating action potential, and thus can reduce dendritic calcium levels, even though the cell is still spiking in the soma. These findings might suggest that feedback inhibition can control how prone the neighbouring MS neurons will be to plastic changes. [ABSTRACT FROM AUTHOR] Copyright of BMC Neuroscience is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=38599810&site=ehost-live
140,Long-term plasticity of corticostriatal synapses is modulated by pathway-specific co-release of opioids through κ-opioid receptors.,Kim Blackwell,Journal of Physiology,223751,,Aug-17,595,16,5637,16,124624264,10.1113/JP274190,Wiley-Blackwell,Article,OPIOID peptides; NEUROPLASTICITY; OPIOID receptors; OPTOGENETICS; NEUROPEPTIDE genetics; DOPAMINE,dopamine; dynorphin; electrophysiology; striatum; synaptic plasticity; voltammetry,"Key points Both endogenous opioids and opiate drugs of abuse modulate learning of habitual and goal-directed actions, and can also modify long-term plasticity of corticostriatal synapses., Striatal projection neurons of the direct pathway co-release the opioid neuropeptide dynorphin which can inhibit dopamine release via κ-opioid receptors., Theta-burst stimulation of corticostriatal fibres produces long-term potentiation (LTP) in striatal projection neurons when measured using whole-cell patch recording., Optogenetic activation of direct pathway striatal projection neurons inhibits LTP while reducing dopamine release., Because the endogenous release of opioids is activity dependent, this modulation of synaptic plasticity represents a negative feedback mechanism that may limit runaway enhancement of striatal neuron activity in response to drugs of abuse., Abstract Synaptic plasticity in the striatum adjusts behaviour adaptively during skill learning, or maladaptively in the case of addiction. Just as dopamine plays a critical role in synaptic plasticity underlying normal skill learning and addiction, endogenous and exogenous opiates also modulate learning and addiction-related striatal plasticity. Though the role of opioid receptors in long-term depression in striatum has been characterized, their effect on long-term potentiation (LTP) remains unknown. In particular, direct pathway (dopamine D1 receptor-containing; D1R-) spiny projection neurons (SPNs) co-release the opioid neuropeptide dynorphin, which acts at presynaptic κ-opioid receptors (KORs) on dopaminergic afferents and can negatively regulate dopamine release. Therefore, we evaluated the interaction of co-released dynorphin and KOR on striatal LTP. We optogenetically facilitate the release of endogenous dynorphin from D1R-SPNs in brain slice while using whole-cell patch recording to measure changes in the synaptic response of SPNs following theta-burst stimulation (TBS) of cortical afferents. Our results demonstrate that TBS evokes corticostriatal LTP, and that optogenetic activation of D1R-SPNs during induction impairs LTP. Additional experiments demonstrate that optogenetic activation of D1R-SPNs reduces stimulation-evoked dopamine release and that bath application of a KOR antagonist provides full rescue of both LTP induction and dopamine release during optogenetic activation of D1R-SPNs. These results suggest that an increase in the opioid neuropeptide dynorphin is responsible for reduced TBS LTP and illustrate a physiological phenomenon whereby heightened D1R-SPN activity can regulate corticostriatal plasticity. Our findings have important implications for learning in addictive states marked by elevated direct pathway activation. [ABSTRACT FROM AUTHOR] Copyright of Journal of Physiology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124624264&site=ehost-live
141,Sensitivity to theta-burst timing permits LTP in dorsal striatal adult brain slice.,Kim Blackwell,Journal of Neurophysiology,223077,,Nov-13,110,9,2027,10,108280070,10.1152/jn.00115.2013,American Physiological Society,Article,LONG-term potentiation; EXCITATION (Physiology); BRAIN physiology; NEUROPLASTICITY; LABORATORY mice; All Other Animal Production,learning; LTP; plasticity; striatum; theta,"Long-term potentiation (LTP) of excitatory afferents to the dorsal striatum likely occurs with learning to encode new skills and habits, yet corticostriatal LTP is challenging to evoke reliably in brain slice under physiological conditions. Here we test the hypothesis that stimulating striatal afferents with theta-burst timing, similar to recently reported in vivo temporal patterns corresponding to learning, evokes LTP. Recording from adult mouse brain slice extracellularly in 1 mM Mg2, we find LTP in dorsomedial and dorsolateral striatum is preferentially evoked by certain theta-burst patterns. In particular, we demonstrate that greater LTP is produced using moderate intraburst and high thetarange frequencies, and that pauses separating bursts of stimuli are critical for LTP induction. By altering temporal pattern alone, we illustrate the importance of burst-patterning for LTP induction and demonstrate that corticostriatal long-term depression is evoked in the same preparation. In accord with prior studies, LTP is greatest in dorsomedial striatum and relies on N-methyl-D-aspartate receptors. We also demonstrate a requirement for both Gq- and Gs/olf-coupled pathways, as well as several kinases associated with memory storage: PKC, PKA, and ERK. Our data build on previous reports of activitydirected plasticity by identifying effective values for distinct temporal parameters in variants of theta-burst LTP induction paradigms. We conclude that those variants which best match reports of striatal activity during learning behavior are most successful in evoking dorsal striatal LTP in adult brain slice without altering artificial cerebrospinal fluid. Future application of this approach will enable diverse investigations of plasticity serving striatal-based learning. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neurophysiology is the property of American Physiological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108280070&site=ehost-live
142,Signaling Pathways Involved in Striatal Synaptic Plasticity are Sensitive to Temporal Pattern and Exhibit Spatial Specificity.,Kim Blackwell,PLoS Computational Biology,1553734X,,Mar-13,9,3,1,16,86939640,10.1371/journal.pcbi.1002953,Public Library of Science,Article,NEUROPLASTICITY; CELLULAR signal transduction; MOTOR neurons; REINFORCEMENT learning; BASAL ganglia; PROTEIN kinase C; LONG-term synaptic depression,,"The basal ganglia is a brain region critically involved in reinforcement learning and motor control. Synaptic plasticity in the striatum of the basal ganglia is a cellular mechanism implicated in learning and neuronal information processing. Therefore, understanding how different spatio-temporal patterns of synaptic input select for different types of plasticity is key to understanding learning mechanisms. In striatal medium spiny projection neurons (MSPN), both long term potentiation (LTP) and long term depression (LTD) require an elevation in intracellular calcium concentration; however, it is unknown how the post-synaptic neuron discriminates between different patterns of calcium influx. Using computer modeling, we investigate the hypothesis that temporal pattern of stimulation can select for either endocannabinoid production (for LTD) or protein kinase C (PKC) activation (for LTP) in striatal MSPNs. We implement a stochastic model of the post-synaptic signaling pathways in a dendrite with one or more diffusionally coupled spines. The model is validated by comparison to experiments measuring endocannabinoid-dependent depolarization induced suppression of inhibition. Using the validated model, simulations demonstrate that theta burst stimulation, which produces LTP, increases the activation of PKC as compared to 20 Hz stimulation, which produces LTD. The model prediction that PKC activation is required for theta burst LTP is confirmed experimentally. Using the ratio of PKC to endocannabinoid production as an index of plasticity direction, model simulations demonstrate that LTP exhibits spine level spatial specificity, whereas LTD is more diffuse. These results suggest that spatio-temporal control of striatal information processing employs these Gq coupled pathways. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=86939640&site=ehost-live
143,Stop! border ahead: Automatic detection of subthalamic exit during deep brain stimulation surgery.,Kim Blackwell,Movement Disorders,8853185,,Jan-17,32,1,70,10,120947072,10.1002/mds.26806,"John Wiley & Sons, Inc.",journal article,BRAIN stem physiology; PARKINSON'S disease treatment; DIENCEPHALON; ELECTRODES; ELECTROPHYSIOLOGY; ARTIFICIAL implants; PARKINSON'S disease; PROBABILITY theory; RESEARCH funding; SIGNAL processing; DEEP brain stimulation; PHYSIOLOGY; Surgical Appliance and Supplies Manufacturing; BRAIN stem anatomy,deep brain stimulation; microelectrode recording; Parkinson's disease; substantia nigra; subthalamic nucleus,"<bold>Background: </bold>Microelectrode recordings along preplanned trajectories are often used for accurate definition of the subthalamic nucleus (STN) borders during deep brain stimulation (DBS) surgery for Parkinson's disease. Usually, the demarcation of the STN borders is performed manually by a neurophysiologist. The exact detection of the borders is difficult, especially detecting the transition between the STN and the substantia nigra pars reticulata. Consequently, demarcation may be inaccurate, leading to suboptimal location of the DBS lead and inadequate clinical outcomes.<bold>Methods: </bold>We present machine-learning classification procedures that use microelectrode recording power spectra and allow for real-time, high-accuracy discrimination between the STN and substantia nigra pars reticulata.<bold>Results: </bold>A support vector machine procedure was tested on microelectrode recordings from 58 trajectories that included both STN and substantia nigra pars reticulata that achieved a 97.6% consistency with human expert classification (evaluated by 10-fold cross-validation). We used the same data set as a training set to find the optimal parameters for a hidden Markov model using both microelectrode recording features and trajectory history to enable real-time classification of the ventral STN border (STN exit). Seventy-three additional trajectories were used to test the reliability of the learned statistical model in identifying the exit from the STN. The hidden Markov model procedure identified the STN exit with an error of 0.04 ± 0.18 mm and detection reliability (error < 1 mm) of 94%.<bold>Conclusions: </bold>The results indicate that robust, accurate, and automatic real-time electrophysiological detection of the ventral STN border is feasible. © 2016 International Parkinson and Movement Disorder Society. [ABSTRACT FROM AUTHOR] Copyright of Movement Disorders is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=120947072&site=ehost-live
144,Synchronized firing of fast-spiking interneurons is critical to maintain balanced firing between direct and indirect pathway neurons of the striatum.,Kim Blackwell,Journal of Neurophysiology,223077,,Feb-14,111,4,836,13,108280219,10.1152/jn.00382.2013,American Physiological Society,Article,GAP junctions (Cell biology); INTERNEURONS; EXCITATORY postsynaptic potential; BRAIN physiology; GLOBUS pallidus; MOTOR ability,fast-spiking interneuron; gap junctions; medium-spiny neuron; Parkinson's disease; striatum,"The inhibitory circuits of the striatum are known to be critical for motor function, yet their contributions to Parkinsonian motor deficits are not clear. Altered firing in the globus pallidus suggests that striatal medium spiny neurons (MSN) of the direct (D1 MSN) and indirect pathway (D2 MSN) are imbalanced during dopamine depletion. Both MSN classes receive inhibitory input from each other and from inhibitory interneurons within the striatum, specifically the fast-spiking interneurons (FSI). To investigate the role of inhibition in maintaining striatal balance, we developed a biologically-realistic striatal network model consisting of multicompartmental neuron models: 500 D1 MSNs, 500 D2 MSNs and 49 FSIs. The D1 and D2 MSN models are differentiated based on published experiments of individual channel modulations by dopamine, with D2 MSNs being more excitable than D1 MSNs. Despite this difference in response to current injection, in the network D1 and D2 MSNs fire at similar frequencies in response to excitatory synaptic input. Simulations further reveal that inhibition from FSIs connected by gap junctions is critical to produce balanced firing. Although gap junctions produce only a small increase in synchronization between FSIs, removing these connections resulted in significant firing differences between D1 and D2 MSNs, and balanced firing was restored by providing synchronized cortical input to the FSIs. Together these findings suggest that desynchronization of FSI firing is sufficient to alter balanced firing between D1 and D2 MSNs. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neurophysiology is the property of American Physiological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108280219&site=ehost-live
145,Temporal sensitivity of protein kinase A activation in a stochastic reaction-diffusion model of late phase long term potentiation.,Kim Blackwell,BMC Neuroscience,14712202,,2009 Supplement 1,10,,1,1,44114604,10.1186/1471-2202-10-S1-P202,BioMed Central,Article,CHRONOBIOLOGY; PROTEIN kinases; STOCHASTIC models; NEURAL physiology; HIPPOCAMPUS (Brain); CELLULAR signal transduction; NEUROPLASTICITY,,"Introduction The ability of neurons within the hippocampus to differentially respond to specific temporal and spatial patterns of stimulation underlies the storage of memory and information in neural circuits. Signal transduction pathways are critical for information storage and alterations in key signaling molecules, such as the cAMP-dependent protein kinase (PKA) signaling pathway, modify both hippocampus-dependent learning and a form of synaptic plasticity known as late-phase long-term potentiation (L-LTP). The induction of late phase LTP (L-LTP) in the CA1 region of the hippocampus requires several kinases, including CaMKII and PKA, which are activated by calcium-dependent signaling processes and other intracellular signaling pathways. Many of the biochemical reactions leading to activation of these critical kinases are localized to dendritic spines. The small size of theses spines implies that small numbers of molecules are involved; the presence of anchoring proteins and the morphology of neurons implies that molecules are inhomogeneously distributed. Therefore, to accurately model these cellular signaling events requires software for stochastic reaction-diffusion systems. Methods We developed a spatial, stochastic, computational model of CA1 signaling pathways to investigate the sensitivity of PKA to spatial and temporal patterns of stimulation. The model is implemented using NeuroRD, novel software for efficient computational modeling of stochastic reaction-diffusion systems. The model describes the interactions of calcium and cAMP signaling pathways and is based on published biochemical measurements of two key synaptic signaling molecules, PKA and CaMKII. The model is stimulated using four 100 Hz tetani separated by 3 sec (massed) or 5 min (spaced), identical to experimental LLTP induction protocols. Results The cAMP concentration is larger in response to massed, as compared to spaced stimulation, similar to the results observed for a deterministic model. Though cAMP directly activates PKA, the ability to differentiate the effect of temporal stimulation pattern on PKA activation depends on morphological factors such as the size of the spine head, and whether PKA is anchored in the spine. In very small spines without anchoring, only a few molecules of PKA are activated; thus the effect of stimulation, much less temporal pattern, is not apparent. In contrast, in large spines temporal stimulation pattern influences PKA activation, and spaced stimulation produces a larger cumulative activity than massed stimulation. This leads to enhanced phosphorylation of Inhibitor-1, and inhibition of protein phosphotase 1. Additional simulations further explore the effect of anchoring and protein co-localization on PKA activation in response to LTP stimulation. [ABSTRACT FROM AUTHOR] Copyright of BMC Neuroscience is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=44114604&site=ehost-live
146,Temporal Sensitivity of Protein Kinase A Activation in Late-Phase Long Term Potentiation.,Kim Blackwell,PLoS Computational Biology,1553734X,,Feb-10,6,2,1,14,48735200,10.1371/journal.pcbi.1000691,Public Library of Science,Article,PROTEIN kinases; NEUROPLASTICITY; HIPPOCAMPUS (Brain); ADRENERGIC receptors; SYMPATHETIC nervous system; DOPAMINE,,"Protein kinases play critical roles in learning and memory and in long term potentiation (LTP), a form of synaptic plasticity. The induction of late-phase LTP (L-LTP) in the CA1 region of the hippocampus requires several kinases, including CaMKII and PKA, which are activated by calcium-dependent signaling processes and other intracellular signaling pathways. The requirement for PKA is limited to L-LTP induced using spaced stimuli, but not massed stimuli. To investigate this temporal sensitivity of PKA, a computational biochemical model of L-LTP induction in CA1 pyramidal neurons was developed. The model describes the interactions of calcium and cAMP signaling pathways and is based on published biochemical measurements of two key synaptic signaling molecules, PKA and CaMKII. The model is stimulated using four 100 Hz tetani separated by 3 sec (massed) or 300 sec (spaced), identical to experimental L-LTP induction protocols. Simulations show that spaced stimulation activates more PKA than massed stimulation, and makes a key experimental prediction, that L-LTP is PKA-dependent for intervals larger than 60 sec. Experimental measurements of L-LTP demonstrate that intervals of 80 sec, but not 40 sec, produce PKA-dependent L-LTP, thereby confirming the model prediction. Examination of CaMKII reveals that its temporal sensitivity is opposite that of PKA, suggesting that PKA is required after spaced stimulation to compensate for a decrease in CaMKII. In addition to explaining the temporal sensitivity of PKA, these simulations suggest that the use of several kinases for memory storage allows each to respond optimally to different temporal patterns. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=48735200&site=ehost-live
147,The Effects of NMDA Subunit Composition on Calcium Influx and Spike Timing-Dependent Plasticity in Striatal Medium Spiny Neurons.,Kim Blackwell,PLoS Computational Biology,1553734X,,Apr-12,8,4,1,13,75195845,10.1371/journal.pcbi.1002493,Public Library of Science,Article,METHYL aspartate receptors; CALCIUM in the body; NEUROPLASTICITY; NEURONS; LONG-term potentiation; INFORMATION processing,,"Calcium through NMDA receptors (NMDARs) is necessary for the long-term potentiation (LTP) of synaptic strength; however, NMDARs differ in several properties that can influence the amount of calcium influx into the spine. These properties, such as sensitivity to magnesium block and conductance decay kinetics, change the receptor's response to spike timing dependent plasticity (STDP) protocols, and thereby shape synaptic integration and information processing. This study investigates the role of GluN2 subunit differences on spine calcium concentration during several STDP protocols in a model of a striatal medium spiny projection neuron (MSPN). The multi-compartment, multi-channel model exhibits firing frequency, spike width, and latency to first spike similar to current clamp data from mouse dorsal striatum MSPN. We find that NMDAR-mediated calcium is dependent on GluN2 subunit type, action potential timing, duration of somatic depolarization, and number of action potentials. Furthermore, the model demonstrates that in MSPNs, GluN2A and GluN2B control which STDP intervals allow for substantial calcium elevation in spines. The model predicts that blocking GluN2B subunits would modulate the range of intervals that cause long term potentiation. We confirmed this prediction experimentally, demonstrating that blocking GluN2B in the striatum, narrows the range of STDP intervals that cause long term potentiation. This ability of the GluN2 subunit to modulate the shape of the STDP curve could underlie the role that GluN2 subunits play in learning and development. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=75195845&site=ehost-live
148,The role of background synaptic noise in striatal fast spiking interneurons,Kim Blackwell,Neurocomputing,9252312,,Jun-05,65-66,,727,6,17796169,10.1016/j.neucom.2004.10.068,Elsevier B.V.,Article,NEURONS; NEURAL circuitry; NEURAL transmission; SYNAPSES,Fast spiking interneurons; Stochastic resonance; Striatum; Transient A current; Up-states,"Abstract: Striatal fast spiking (FS) interneurons provide inhibition to each other as well as to medium spiny projection (SP) neurons. They exhibit up-states synchronously with SP neurons, and receive GABAergic and AMPA synaptic input during both up- and down-states. The synaptic input during down-states can be considered noise and might affect detection of up-states. We investigate what role this background noise might play for up-state firing in a 127 compartment FS model neuron. The model has Na, KDr and KA conductances, and is activated through AMPA and GABA synapses. The model''s response to current injection and synaptic inputs resembled experimental data. We show that intermediate levels of noise neither facilitates nor degrades the ability of the FS neuron model to detect up-states. [Copyright &y& Elsevier] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17796169&site=ehost-live
149,The Role of Type 4 Phosphodiesterases in Generating Microdomains of cAMP: Large Scale Stochastic Simulations.,Kim Blackwell,PLoS ONE,19326203,,2010,5,7,1,14,52884600,10.1371/journal.pone.0011725,Public Library of Science,Article,PHOSPHODIESTERASES; CYCLIC adenylic acid; PROTEIN kinases; NEUROPLASTICITY; STOCHASTIC models; BIOLOGICAL transport; CYTOSOL; PHOSPHORYLATION; AFFERENT pathways,,"Cyclic AMP (cAMP) and its main effector Protein Kinase A (PKA) are critical for several aspects of neuronal function including synaptic plasticity. Specificity of synaptic plasticity requires that cAMP activates PKA in a highly localized manner despite the speed with which cAMP diffuses. Two mechanisms have been proposed to produce localized elevations in cAMP, known as microdomains: impeded diffusion, and high phosphodiesterase (PDE) activity. This paper investigates the mechanism of localized cAMP signaling using a computational model of the biochemical network in the HEK293 cell, which is a subset of pathways involved in PKA-dependent synaptic plasticity. This biochemical network includes cAMP production, PKA activation, and cAMP degradation by PDE activity. The model is implemented in NeuroRD: novel, computationally efficient, stochastic reaction-diffusion software, and is constrained by intracellular cAMP dynamics that were determined experimentally by real-time imaging using an Epac-based FRET sensor (H30). The model reproduces the high concentration cAMP microdomain in the submembrane region, distinct from the lower concentration of cAMP in the cytosol. Simulations further demonstrate that generation of the cAMP microdomain requires a pool of PDE4D anchored in the cytosol and also requires PKA-mediated phosphorylation of PDE4D which increases its activity. The microdomain does not require impeded diffusion of cAMP, confirming that barriers are not required for microdomains. The simulations reported here further demonstrate the utility of the new stochastic reaction-diffusion algorithm for exploring signaling pathways in spatially complex structures such as neurons. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=52884600&site=ehost-live
150,Transient Calcium and Dopamine Increase PKA Activity and DARPP-32 Phosphorylation.,Kim Blackwell,PLoS Computational Biology,1553734X,,Sep-06,2,9,e119,16,23454889,10.1371/journal.pcbi.0020119,Public Library of Science,Article,CALCIUM; DOPAMINE; PHOSPHORYLATION; SYNAPSES; NEURONS,,"Reinforcement learning theorizes that strengthening of synaptic connections in medium spiny neurons of the striatum occurs when glutamatergic input (from cortex) and dopaminergic input (from substantia nigra) are received simultaneously. Subsequent to learning, medium spiny neurons with strengthened synapses are more likely to fire in response to cortical input alone. This synaptic plasticity is produced by phosphorylation of AMPA receptors, caused by phosphorylation of various signalling molecules. A key signalling molecule is the phosphoprotein DARPP-32, highly expressed in striatal medium spiny neurons. DARPP-32 is regulated by several neurotransmitters through a complex network of intracellular signalling pathways involving cAMP (increased through dopamine stimulation) and calcium (increased through glutamate stimulation). Since DARPP-32 controls several kinases and phosphatases involved in striatal synaptic plasticity, understanding the interactions between cAMP and calcium, in particular the effect of transient stimuli on DARPP-32 phosphorylation, has major implications for understanding reinforcement learning. We developed a computer model of the biochemical reaction pathways involved in the phosphorylation of DARPP-32 on Thr34 and Thr75. Ordinary differential equations describing the biochemical reactions were implemented in a single compartment model using the software XPPAUT. Reaction rate constants were obtained from the biochemical literature. The first set of simulations using sustained elevations of dopamine and calcium produced phosphorylation levels of DARPP-32 similar to that measured experimentally, thereby validating the model. The second set of simulations, using the validated model, showed that transient dopamine elevations increased the phosphorylation of Thr34 as expected, but transient calcium elevations also increased the phosphorylation of Thr34, contrary to what is believed. When transient calcium and dopamine stimuli were paired, PKA activation and Thr34 phosphorylation increased compared with dopamine alone. This result, which is robust to variation in model parameters, supports reinforcement learning theories in which activity-dependent long-term synaptic plasticity requires paired glutamate and dopamine inputs. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23454889&site=ehost-live
151,Crowdsourcing cyber experts to determine relevant topics during cyber curriculum development efforts.,Mihai Boicu,Innovations in Education & Teaching International,14703297,,Apr-22,,,1,11,156289792,10.1080/14703297.2022.2063924,Taylor & Francis Ltd,Article,,curriculum development; cyber security education; engineering education; Expert crowdsourcing; needs assessment; requirements determination,"The cyber security environment, its threats, and its defence strategies are constantly changing. Educational programmes and their curriculum are known to be slow changing and at times out-of-date, resulting in content that may not be as relevant to their students and the industry. This research paper will 1 – present an overview of the curriculum development (CDev) process when using committees and their hindrance, 2 – describe the concept of crowdsourcing and its benefits when using domain experts, 3 – propose a Curriculum Development using Crowdsourcing Framework (CDC-F) to integrate expert crowdsourcing into parts of the CDev process (specifically the identification of industry-relevant topics and sub-topics for further curriculum content development), and 4 – present the process and results of ang experiment utilising the CDC-F. [ABSTRACT FROM AUTHOR] Copyright of Innovations in Education & Teaching International is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156289792&site=ehost-live
152,THE DISCIPLE–RKF LEARNING AND REASONING AGENT.,Gheorghe Tecuci,Computational Intelligence,8247935,,Nov-05,21,4,462,18,18417787,10.1111/j.1467-8640.2005.00282.x,Wiley-Blackwell,Article,MACHINE learning; ARTIFICIAL intelligence; PROBLEM solving; LEARNING strategies; REASONING,military center of gravity analysis; mixed-initiative reasoning; multistrategy apprenticeship learning; ontology; plausible version spaces; rule learning; task reduction,"Over the years we have developed the Disciple theory, methodology, and family of tools for building knowledge-based agents. This approach consists of developing an agent shell that can be taught directly by a subject matter expert in a way that resembles how the expert would teach a human apprentice when solving problems in cooperation. This paper presents the most recent version of the Disciple approach and its implementation in the Disciple–RKF (rapid knowledge formation) system. Disciple–RKF is based on mixed-initiative problem solving, where the expert solves the more creative parts of the problem and the agent solves the more routine ones, integrated teaching and learning, where the agent helps the expert to teach it, by asking relevant questions, and the expert helps the agent to learn, by providing examples, hints, and explanations, and multistrategy learning, where the agent integrates multiple learning strategies, such as learning from examples, learning from explanations, and learning by analogy, to learn from the expert how to solve problems. Disciple–RKF has been applied to build learning and reasoning agents for military center of gravity analysis, which are used in several courses at the US Army War College. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18417787&site=ehost-live
152,THE DISCIPLE–RKF LEARNING AND REASONING AGENT.,Mihai Boicu,Computational Intelligence,8247935,,Nov-05,21,4,462,18,18417787,10.1111/j.1467-8640.2005.00282.x,Wiley-Blackwell,Article,MACHINE learning; ARTIFICIAL intelligence; PROBLEM solving; LEARNING strategies; REASONING,military center of gravity analysis; mixed-initiative reasoning; multistrategy apprenticeship learning; ontology; plausible version spaces; rule learning; task reduction,"Over the years we have developed the Disciple theory, methodology, and family of tools for building knowledge-based agents. This approach consists of developing an agent shell that can be taught directly by a subject matter expert in a way that resembles how the expert would teach a human apprentice when solving problems in cooperation. This paper presents the most recent version of the Disciple approach and its implementation in the Disciple–RKF (rapid knowledge formation) system. Disciple–RKF is based on mixed-initiative problem solving, where the expert solves the more creative parts of the problem and the agent solves the more routine ones, integrated teaching and learning, where the agent helps the expert to teach it, by asking relevant questions, and the expert helps the agent to learn, by providing examples, hints, and explanations, and multistrategy learning, where the agent integrates multiple learning strategies, such as learning from examples, learning from explanations, and learning by analogy, to learn from the expert how to solve problems. Disciple–RKF has been applied to build learning and reasoning agents for military center of gravity analysis, which are used in several courses at the US Army War College. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18417787&site=ehost-live
153,Quasi-static responses and associated failure mechanisms of cold-formed steel roof trusses.,Doaa Bondok,Engineering Structures,1410296,,Mar-21,231,,N.PAG,1,148502985,10.1016/j.engstruct.2020.111741,Elsevier B.V.,Article,COLD-formed steel; TRUSSES; BLAST effect; FINITE element method; ROOFS; ALUMINUM foam; PEARLITIC steel; Roofing Contractors; Truss Manufacturing; Framing Contractors,"Cold-formed steel trusses; Finite element simulation; Large deformations, quasi-static tests; Roof trusses","• The static resistance and the associated failure mechanisms of cold-formed steel roof trusses were determined experimentally. • Advanced three-dimensional numerical models predicted the resistance and the ultimate failure mode. • The truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. • For moderate damage levels, strengthening compression web members improve the absorbed energy. The use of cold-formed steel trusses in roof framing has significantly increased recently. Cold-formed steel roof trusses are ideal and efficient systems for a variety of applications due to their design flexibility and ease of construction. Past research explored the behavior of these truss systems up to the ultimate capacity point; however, the inelastic behavior to the failure was not fully captured. Information about the response beyond the buckling point and the energy absorption capacities are missing and need to be investigated. In this paper, small-scale cold-formed steel roof truss specimens were tested to failure under quasi-static loading. The static resistance of these systems and the associated failure mechanisms were identified. Such information is key input when analyzing these roof systems under blast loads using the Single Degree of Freedom simplified technique. Experimental results and absorbed energy comparisons show that the truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. Three-dimensional finite element models were developed and verified against the experimental results. The advanced models predicted the static resistance with a high level of accuracy. Experimental and finite element analyses have shown that the energy absorbed is improved significantly when the web members susceptible to buckling are strengthened. [ABSTRACT FROM AUTHOR] Copyright of Engineering Structures is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148502985&site=ehost-live
153,Quasi-static responses and associated failure mechanisms of cold-formed steel roof trusses.,Girum Urgessa,Engineering Structures,1410296,,Mar-21,231,,N.PAG,1,148502985,10.1016/j.engstruct.2020.111741,Elsevier B.V.,Article,COLD-formed steel; TRUSSES; BLAST effect; FINITE element method; ROOFS; ALUMINUM foam; PEARLITIC steel; Roofing Contractors; Truss Manufacturing; Framing Contractors,"Cold-formed steel trusses; Finite element simulation; Large deformations, quasi-static tests; Roof trusses","• The static resistance and the associated failure mechanisms of cold-formed steel roof trusses were determined experimentally. • Advanced three-dimensional numerical models predicted the resistance and the ultimate failure mode. • The truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. • For moderate damage levels, strengthening compression web members improve the absorbed energy. The use of cold-formed steel trusses in roof framing has significantly increased recently. Cold-formed steel roof trusses are ideal and efficient systems for a variety of applications due to their design flexibility and ease of construction. Past research explored the behavior of these truss systems up to the ultimate capacity point; however, the inelastic behavior to the failure was not fully captured. Information about the response beyond the buckling point and the energy absorption capacities are missing and need to be investigated. In this paper, small-scale cold-formed steel roof truss specimens were tested to failure under quasi-static loading. The static resistance of these systems and the associated failure mechanisms were identified. Such information is key input when analyzing these roof systems under blast loads using the Single Degree of Freedom simplified technique. Experimental results and absorbed energy comparisons show that the truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. Three-dimensional finite element models were developed and verified against the experimental results. The advanced models predicted the static resistance with a high level of accuracy. Experimental and finite element analyses have shown that the energy absorbed is improved significantly when the web members susceptible to buckling are strengthened. [ABSTRACT FROM AUTHOR] Copyright of Engineering Structures is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148502985&site=ehost-live
154,Analysis and optimization based on reusable knowledge base of process performance models.,Alexander Brodsky,International Journal of Advanced Manufacturing Technology,2683768,,Jan-17,88,4-Jan,337,21,120738418,10.1007/s00170-016-8761-7,Springer Nature,Article,ARCHITECTURAL design; SOFTWARE frameworks; KNOWLEDGE base; COMPOSITE materials; DECISION support systems; USER interfaces,Data analytics; Domain specific user interface; Optimization; Process performance models; Reusable knowledge base; Smart manufacturing,"In this paper, we propose an architectural design and software framework for fast development of descriptive, diagnostic, predictive, and prescriptive analytics solutions for dynamic production processes. The proposed architecture and framework will support the storage of modular, extensible, and reusable knowledge base (KB) of process performance models. The approach requires developing automated methods that can translate the high-level models in the reusable KB into low-level specialized models required by a variety of underlying analysis tools, including data manipulation, optimization, statistical learning, estimation, and simulation. We also propose an organization and key structure for the reusable KB, composed of atomic and composite process performance models and domain-specific dashboards. Furthermore, we illustrate the use of the proposed architecture and framework by prototyping a decision support system for process engineers. The decision support system allows users to hierarchically compose and optimize dynamic production processes via a graphical user interface. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Advanced Manufacturing Technology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=120738418&site=ehost-live
154,Analysis and optimization based on reusable knowledge base of process performance models.,Daniel Menascé,International Journal of Advanced Manufacturing Technology,2683768,,Jan-17,88,4-Jan,337,21,120738418,10.1007/s00170-016-8761-7,Springer Nature,Article,ARCHITECTURAL design; SOFTWARE frameworks; KNOWLEDGE base; COMPOSITE materials; DECISION support systems; USER interfaces,Data analytics; Domain specific user interface; Optimization; Process performance models; Reusable knowledge base; Smart manufacturing,"In this paper, we propose an architectural design and software framework for fast development of descriptive, diagnostic, predictive, and prescriptive analytics solutions for dynamic production processes. The proposed architecture and framework will support the storage of modular, extensible, and reusable knowledge base (KB) of process performance models. The approach requires developing automated methods that can translate the high-level models in the reusable KB into low-level specialized models required by a variety of underlying analysis tools, including data manipulation, optimization, statistical learning, estimation, and simulation. We also propose an organization and key structure for the reusable KB, composed of atomic and composite process performance models and domain-specific dashboards. Furthermore, we illustrate the use of the proposed architecture and framework by prototyping a decision support system for process engineers. The decision support system allows users to hierarchically compose and optimize dynamic production processes via a graphical user interface. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Advanced Manufacturing Technology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=120738418&site=ehost-live
155,Desalination supply chain decision analysis and optimization.,Alexander Brodsky,Desalination,119164,,Aug-14,347,,144,14,96786583,10.1016/j.desal.2014.05.037,Elsevier B.V.,Article,SALINE water conversion; SUPPLY chains; DECISION making; RENEWABLE energy sources; STRATEGIC planning; SAUDI Arabia,Decision analysis; Desalination; Optimization modeling; Strategic planning; Supply chain,"Abstract: The desalination industry has been growing progressively in the last few decades. A large number of new plants are contracted every year. Strategic decisions related to plant locations and capacity, the selection of the desalination technology, and many other technical decisions related to the plant design and operation are very critical to these strategic investments. Viewing the desalination industry network as a supply chain provides a holistic view allowing decision makers to perform optimization of water desalination operations end to end. The methodology we propose provides the decision makers with (1) a set of investment alternatives comprising combinations of the different desalination locations, capacities, technologies, and energy sources, and (2) a decision graph showing the performance of each decision alternative in terms of quantitative and qualitative performance metrics chosen by the decision maker. The case study of Saudi Arabia, the world leader in desalination, shows how the methodology can present strategic planners with an optimal configuration of the desalination supply chain. [Copyright &y& Elsevier] Copyright of Desalination is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96786583&site=ehost-live
156,Optimal splitting for rare-event simulation.,Chun-Hung Chen,IIE Transactions,0740817X,,May-12,44,5,352,16,72338848,10.1080/0740817X.2011.596507,Taylor & Francis Ltd,Article,SIMULATION methods & models; STOCHASTIC analysis; PROBABILITY theory; COMPARATIVE studies; NUMERICAL analysis; MATHEMATICAL optimization,comparison of designs; Rare-event simulation; splitting,"Simulation is a popular tool for analyzing large, complex, stochastic engineering systems. When estimating rare-event probabilities, efficiency is a big concern, since a huge number of simulation replications may be needed in order to obtain a reasonable estimate of the rare-event probability. The idea of splitting has emerged as a promising variance reduction technique. The basic idea is to create separate copies (splits) of the simulation whenever it gets close to the rare event. Some splitting methods use an equal number of splits at all levels. This can compromise the efficiency and can even increase the estimation variance. This article formulates the problem of determining the number of splits as an optimization problem that minimizes the variance of an estimator subject to a constraint on the total computing budget. An optimal solution for a certain class of problems is derived that is then extended to the problem of choosing the better of two designs, where each design is evaluated via rare-event simulation. Theoretical results for the improvements that are achievable using the methods are provided. Numerical experiments indicate that the proposed approaches are efficient and robust. [ABSTRACT FROM AUTHOR] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=72338848&site=ehost-live
156,Optimal splitting for rare-event simulation.,Alexander Brodsky,IIE Transactions,0740817X,,May-12,44,5,352,16,72338848,10.1080/0740817X.2011.596507,Taylor & Francis Ltd,Article,SIMULATION methods & models; STOCHASTIC analysis; PROBABILITY theory; COMPARATIVE studies; NUMERICAL analysis; MATHEMATICAL optimization,comparison of designs; Rare-event simulation; splitting,"Simulation is a popular tool for analyzing large, complex, stochastic engineering systems. When estimating rare-event probabilities, efficiency is a big concern, since a huge number of simulation replications may be needed in order to obtain a reasonable estimate of the rare-event probability. The idea of splitting has emerged as a promising variance reduction technique. The basic idea is to create separate copies (splits) of the simulation whenever it gets close to the rare event. Some splitting methods use an equal number of splits at all levels. This can compromise the efficiency and can even increase the estimation variance. This article formulates the problem of determining the number of splits as an optimization problem that minimizes the variance of an estimator subject to a constraint on the total computing budget. An optimal solution for a certain class of problems is derived that is then extended to the problem of choosing the better of two designs, where each design is evaluated via rare-event simulation. Theoretical results for the improvements that are achievable using the methods are provided. Numerical experiments indicate that the proposed approaches are efficient and robust. [ABSTRACT FROM AUTHOR] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=72338848&site=ehost-live
157,Unauthorized inferences in semistructured databases,Alexander Brodsky,Information Sciences,200255,,Nov-06,176,22,3269,31,22134565,10.1016/j.ins.2006.01.004,Elsevier B.V.,Article,ALGORITHMS; DATABASES; HORN clauses; LOGIC programming,Completeness; Entailment; Inference problem; Privacy; Semistructured data; Soundness,"Abstract: In this paper we study the problem of providing controlled access to confidential data stored in semistructured databases. More specifically, we focus on privacy violations via data inferences that occur when domain knowledge is combined with non-private data. We propose a formal model, called Privacy Information Flow Model, to represent the information flow and the privacy requirements. These privacy requirements are enforced by the Privacy Mediator. Privacy Mediator guarantees that users are not be able to logically entail information that violates the privacy requirements. We present an inference algorithm that is sound and complete. The inference algorithm is developed for a tree-like, semistructured data model, selection–projection queries, and domain knowledge, represented as Horn-clause constraints. [Copyright &y& Elsevier] Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22134565&site=ehost-live
158,A note on coding and standardization of categorical variables in (sparse) group lasso regression.,Juan Cebral,Journal of Statistical Planning & Inference,3783758,,May-20,206,,1,11,140090765,10.1016/j.jspi.2019.08.003,Elsevier B.V.,Article,STANDARDIZATION; COMPUTATIONAL complexity; REGRESSION analysis,Group Lasso; Regularization; Standardization; Variable Selection,"Categorical regressor variables are usually handled by introducing a set of indicator variables, and imposing a linear constraint to ensure identifiability in the presence of an intercept, or equivalently, using one of various coding schemes. As proposed in Yuan and Lin (2006), the group lasso is a natural and computationally convenient approach to perform variable selection in settings with categorical covariates. As pointed out by Simon and Tibshirani (2012), ""standardization"" by means of block-wise orthonormalization of column submatrices each corresponding to one group of variables can substantially boost performance. In this note, we study the aspect of standardization for the special case of categorical predictors in detail. The main result is that orthonormalization is not required; column-wise scaling of the design matrix followed by re-scaling and centering of the coefficients is shown to have exactly the same effect. Similar reductions can be achieved in the case of interactions. The extension to the so-called sparse group lasso, which additionally promotes within-group sparsity, is considered as well. The importance of proper standardization is illustrated via simulations and a case study. • Fitting regularized regression models with categorical predictors depends on the encoding scheme. • The proposed standardization schemes yield significant reductions in terms of computational complexity. • The extension to interaction terms is discussed as well. • The effect of proper standardization is demonstrated on real and synthetic data sets. [ABSTRACT FROM AUTHOR] Copyright of Journal of Statistical Planning & Inference is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140090765&site=ehost-live
158,A note on coding and standardization of categorical variables in (sparse) group lasso regression.,Martin Slawski,Journal of Statistical Planning & Inference,3783758,,May-20,206,,1,11,140090765,10.1016/j.jspi.2019.08.003,Elsevier B.V.,Article,STANDARDIZATION; COMPUTATIONAL complexity; REGRESSION analysis,Group Lasso; Regularization; Standardization; Variable Selection,"Categorical regressor variables are usually handled by introducing a set of indicator variables, and imposing a linear constraint to ensure identifiability in the presence of an intercept, or equivalently, using one of various coding schemes. As proposed in Yuan and Lin (2006), the group lasso is a natural and computationally convenient approach to perform variable selection in settings with categorical covariates. As pointed out by Simon and Tibshirani (2012), ""standardization"" by means of block-wise orthonormalization of column submatrices each corresponding to one group of variables can substantially boost performance. In this note, we study the aspect of standardization for the special case of categorical predictors in detail. The main result is that orthonormalization is not required; column-wise scaling of the design matrix followed by re-scaling and centering of the coefficients is shown to have exactly the same effect. Similar reductions can be achieved in the case of interactions. The extension to the so-called sparse group lasso, which additionally promotes within-group sparsity, is considered as well. The importance of proper standardization is illustrated via simulations and a case study. • Fitting regularized regression models with categorical predictors depends on the encoding scheme. • The proposed standardization schemes yield significant reductions in terms of computational complexity. • The extension to interaction terms is discussed as well. • The effect of proper standardization is demonstrated on real and synthetic data sets. [ABSTRACT FROM AUTHOR] Copyright of Journal of Statistical Planning & Inference is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140090765&site=ehost-live
159,Adaptive embedded and immersed unstructured grid techniques,Juan Cebral,Computer Methods in Applied Mechanics & Engineering,457825,,Apr-08,197,25-28,2173,25,31676886,10.1016/j.cma.2007.09.010,Elsevier B.V.,Article,METHODOLOGY; DISCOURSE analysis; ETHNOLOGY methodology; ETHNOMETHODOLOGY,CFD; Embedded surface method; Finite elements; Immersed body technique,"Abstract: Embedded mesh, immersed body or fictitious domain techniques have been used for many years as a way to discretize geometrically complex domains with structured grids. The use of such techniques within adaptive, unstructured grid solvers is relatively recent. The combination of body-fitted functionality for some portion of the domain, together with embedded mesh or immersed body functionality for another portion of the domain offers great advantages, which are increasingly being exploited. The present paper reviews the methodologies pursued so far, addresses implementational issues and shows the possibilities such techniques offer. [Copyright &y& Elsevier] Copyright of Computer Methods in Applied Mechanics & Engineering is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=31676886&site=ehost-live
160,Analysis of flow changes in side branches jailed by flow diverters in rabbit models.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Oct-14,30,10,988,12,98676060,10.1002/cnm.2640,Wiley-Blackwell,Article,INTRACRANIAL aneurysms; BLOOD flow; LABORATORY rabbits; COMPUTATIONAL fluid dynamics; DOPPLER ultrasonography; ANGIOGRAPHY; SCANNING electron microscopy; MATHEMATICAL models,cerebral aneurysms; computational fluid dynamics; flow diverters; perforators; rabbit models,"SUMMARY Understanding the flow alteration in side branches during flow diversion treatment of cerebral aneurysms is important to prevent ischemic complications and improve device designs. Flow diverters were placed in the aorta of four rabbits crossing the origin of side arteries. Subject-specific computational models were constructed from 3D angiographies and Doppler ultrasounds (DUSs). Flow simulations were run before and after virtually deploying the flow diverters, assuming distal resistances remained unchanged after treatment. All jailed arteries remained patent angiographically 8 weeks after treatment. The computational models estimated decreases compared to pretreatment in the mean flow rates between 2% and 20% and in peak flow rates between 5% and 36%. The major changes were observed during systole. Flow patterns did not exhibit recirculation zones before treatment. Implantation of the flow diverters altered the flow structure only locally near the device wires. No major recirculation regions were created or destroyed. Flow diverters seem safe with respect to perforator or side branch occlusion. Relatively small changes in flow rates through jailed arteries are expected, even for moderate to large degrees of coverage of their origins. These results seem consistent with previous clinical experiences where no or very few complications related to perforator occlusion have been reported.Copyright © 2014 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98676060&site=ehost-live
161,Analysis of hemodynamic changes from aneurysm inception to large sizes.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Jan-21,37,1,1,15,148137484,10.1002/cnm.3415,Wiley-Blackwell,Article,INTRACRANIAL aneurysms; ANEURYSMS; COMPUTATIONAL fluid dynamics; STRESS concentration; FLOW velocity,aneurysm growth; cerebral aneurysm; evolution; hemodynamics,"While previous studies have identified many risk factors for the progression and rupture of cerebral aneurysms, the changes in aneurysm flow characteristics during its evolution are not fully understood. This work analyzes the changes in the aneurysm hemodynamic environment from its initial development to later stages when the aneurysm has substantially enlarged. A total of 88 aneurysms at four locations were studied with image based computational fluid dynamics (CFD). Two synthetic sequences representing the aneurysm geometry at three earlier stages were generated by shrinking the aneurysm sac while keeping the neck fixed or shrinking the neck simultaneously. The flow conditions were then quantitatively compared between these two modes of evolution. As aneurysms enlarged, the inflow rate increased in growing neck sequences, but decreased in fixed neck sequences. The inflow jet became more concentrated in both sequences. The mean aneurysm flow velocity and wall shear stress decreased in both sequences, but they decreased faster in enlarging aneurysms if the neck was fixed. Additionally, the intra‐aneurysmal flows became more complex and more unstable, wall shear stress distribution became more oscillatory, and the area under low wall shear stress increased for both sequences. The evolution of flow characteristics of aneurysms with fixed and growing necks are different. The observed trends suggest that fixed neck aneurysms may evolve towards a flow environment characteristic of stable aneurysms faster than aneurysms with growing necks, which could also evolve towards a more disfavorable environment. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148137484&site=ehost-live
162,Analysis of hemodynamics and wall mechanics at sites of cerebral aneurysm rupture.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Jul-15,7,7,530,7,103650325,10.1136/neurintsurg-2014-011247,BMJ Publishing Group,Article,ANGIOGRAPHY; BIOMECHANICS; COMPUTER simulation; HEMODYNAMICS; HUMAN anatomical models; INTRACRANIAL aneurysms; RESEARCH funding; RUPTURED aneurysms; IN vitro studies; DISEASE complications,,"Background It is thought that aneurysms evolve as the result of progressive degradation of the wall in response to abnormal hemodynamics characterized by either high or low wall shear stress (WSS). Objective To investigate the effects of these two different hemodynamic pathways in a series of cerebral aneurysms with known rupture sites. Methods Nine aneurysms in which the rupture site could be identified in three-dimensional images were analyzed. The WSS distribution was obtained from computational fluid dynamics (CFD) simulations. Internal wall stresses were computed using structural wall models under hemodynamic loads determined by the CFD models. Wall properties (thickness and stiffness) were modulated with the WSS distribution (increased or decreased in regions of high or low WSS) to test possible wall degradation pathways. Rupture probability indices (RPI) were calculated to compare different wall models. Results Most rupture sites aligned with the intrasaccular flow stream and downstream of the primary impaction zone. The model that best explained the rupture site (produced higher RPI) in eight of the nine aneurysms (89%) had thinner and stiffer walls in regions of abnormally high WSS. The remaining case (11%) was best explained by a model with thinner and stiffer walls in regions of abnormally low WSS. Conclusions Aneurysm rupture seems to be caused by localized degradation and weakening of the wall in response to abnormal hemodynamics. Image-based computational models assuming wall thinning and stiffening in regions of abnormally high WSS were able to explain most of the observed rupture sites. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103650325&site=ehost-live
163,Combining data from multiple sources to study mechanisms of aneurysm disease: Tools and techniques.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Nov-18,34,11,N.PAG,1,132936109,10.1002/cnm.3133,Wiley-Blackwell,Article,ANEURYSMS; COMPUTED tomography; HEMODYNAMICS; BIOMECHANICS; NEUROSURGERY; Irradiation Apparatus Manufacturing,cerebral aneurysms; hemodynamics; mapping; micro‐CT; wall properties,"Introduction: Connecting local hemodynamics, biomechanics, and tissue properties in cerebral aneurysms is important for understanding the processes of wall degeneration and subsequent aneurysm progression and rupture. This challenging problem requires integration of data from multiple sources. Methods: This paper describes the tools and techniques developed to integrate data from multiple sources, including clinical information, 3D imaging, intraoperative videos, ex vivo micro–computed tomography (CT), and multiphoton microscopy. Central to this approach is a 3D tissue model constructed from micro‐CT images of aneurysm samples resected during neurosurgery. This model is aligned to vascular models constructed from 3D clinical images and is used to map and compare flow, biomechanics, and tissue data. Results: The approach is illustrated with data of three human intracranial aneurysms. These case studies demonstrated the ability of this approach to study relationships between different factors affecting the aneurysm wall and produced provocative observations that will be further studied with larger series. For instance, ""atherosclerotic"" and ""hyperplastic"" looking parts of the aneurysm corresponded to thicker walls and occurred in regions of recirculating flow and low wall shear stress (WSS); thin regions were associated with inflow jets, flow impingement, and high WSS; blebs had walls of varying structures, including calcified, thin, or hyperplastic walls. Conclusions: The current approach enables the study of interactions of multiple factors thought to be responsible for the progressive degradation and weakening of the aneurysm wall during its evolution. This paper describes the tools and techniques for integrating data from multiple sources, including clinical information, 3D imaging, intraoperative videos, ex vivo micro‐CT, and multiphoton microscopy. Central is a micro‐CT–based 3D tissue model of aneurysm samples resected during neurosurgery. This model is aligned to vascular models from 3D clinical images and used to map and compare flow, biomechanics, and tissue data. This enables studying interactions of multiple factors thought to be responsible for the progressive wall degradation and weakening. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=132936109&site=ehost-live
164,Efficient Pipeline for Image-Based Patient-Specific Analysis of Cerebral Aneurysm Hemodynamics: Technique and Sensitivity.,Juan Cebral,IEEE Transactions on Medical Imaging,2780062,,Apr-05,24,4,457,11,16706878,10.1109/TMI.2005.844159,IEEE,Article,HEMODYNAMICS; MEDICAL imaging systems; BLOOD circulation; HYDRODYNAMICS; ANEURYSMS; VASCULAR diseases,Cerebral aneurysm; Computational Fluid dynamics; rotational angiography; sensitivity,"Hemodynamic factors are thought to be implicated in the progression and rupture of intracranial aneurysms. Current efforts aim to study the possible associations of hemodynamic chars acteristics such as complexity and stability of intra-aneurysmal flow patterns, size and location of the region of flow impingement with the clinical history of aneurysmal rupture. However, there are no reliable methods for measuring blood flow patterns in vivo. In this paper, an efficient methodology for patient-specific modeling and characterization of the hemodynamics in cerebral aneurysms from medical images is described. A sensitivity analysis of the hemodynamic characteristics with respect to variations of several variables over the expected physiologic range of conditions is also presented. This sensitivity analysis shows that although changes in the velocity fields can be observed, the characterization of the intra-aneurysmal flow patterns is not altered when the mean input flow, the flow division, the viscosity model, or mesh resolution are changed. It was also found that the variable that has the greater impact on the computed flow fields is the geometry of the vascular structures. We conclude that with the proposed modeling pipeline clinical studies involving large numbers cerebral aneurysms are feasible. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Medical Imaging is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=16706878&site=ehost-live
165,Efficient Simulation of Blood Flow Past Complex Endovascular Devices Using an Adaptive Embedding Technique.,Juan Cebral,IEEE Transactions on Medical Imaging,2780062,,Apr-05,24,4,468,9,16706879,10.1109/TMI.2005.844172,IEEE,Article,BLOOD flow; FLUID dynamics; BLOOD circulation; BODY fluid flow; HEMODYNAMICS; AMORPHOUS substances,Adavptive embedded unstructured grids; cerebral aneurysms; coiling; computational fluid dynamics; endovascular devices; stenting,"The simulation of blood flow past endovascular devices such as coils and stents is a challenging problem due to the complex geometry of the devices. Traditional unstructured grid computational fluid dynamics relies on the generation of finite element grids that conform to the boundary of the computational domain. However, the generation of such grids for patient-specific modeling of cerebral aneurysm treatment with coils or stents is extremely difficult and time consuming. This paper describes the application of an adaptive grid embedding technique previously developed for complex fluid structure interaction problems to the simulation of endovascular devices. A hybrid approach is used: the vessel walls are treated with body conforming grids and the endovascular devices with an adaptive mesh embedding technique. This methodology fits naturally in the framework of image-based computational fluid dynamics and opens the door for exploration of different therapeutic options and personalization of endovascular procedures. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Medical Imaging is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=16706879&site=ehost-live
166,Estimation of bolus dispersion effects in perfusion MRI using image-based computational fluid dynamics,Juan Cebral,NeuroImage,10538119,,Jun-03,19,2,341,13,10012408,10.1016/S1053-8119(03)00090-9,Academic Press Inc.,Article,MAGNETIC resonance imaging; CEREBRAL circulation; Diagnostic Imaging Centers,Cerebral blood flow; Computational fluid dynamics; Perfusion; Quantification,"Bolus tracking magnetic resonance imaging (MRI) is a powerful technique for measuring perfusion, and is playing an increasing role in the investigation of acute stroke. However, limitations have been reported when assessing patients with steno-occlusive disease. The presence of a steno-occlusive disease in the artery may cause bolus dispersion, which has been shown to introduce significant errors in cerebral blood flow (CBF) quantification. Bolus dispersion is commonly described by a vascular transport function, but the function that properly characterizes the dispersion is unknown. A novel method to quantify bolus dispersion errors on perfusion measurements is presented. A realistic patient-specific model is constructed from anatomical and physiologic MR data, and the arterial blood flow pattern and the transport of the bolus of contrast agent are computed using finite element analysis. The methodology presented was used also to evaluate the accuracy of three simple vascular models. The methodology was tested on MR data from two normal subjects and two subjects with mild carotid artery stenosis. The estimated CBF errors were of the order of 15% to 20%. However, the presence of stenosis did not necessarily introduce larger dispersion (not only the geometrical model but also the particular physiologic conditions influence the degree of bolus dispersion). The method described will contribute to a better understanding of errors introduced by dispersion effects, to the assessment and validation of vascular models, and to the development of new methods for the correction of dispersion errors in CBF quantification. [Copyright &y& Elsevier] Copyright of NeuroImage is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=10012408&site=ehost-live
167,Hemodynamics of Cerebral Aneurysms.,Juan Cebral,Annual Review of Fluid Mechanics,664189,,2009,41,,91,17,36464258,10.1146/annurev.fluid.40.111406.102126,Annual Reviews Inc.,Article,HEMODYNAMICS; INTRACRANIAL aneurysms; ENDOTHELIUM; BLOOD flow; FLUID dynamics,computational fluid dynamics; growth; intracranial aneurysms; rupture; wall shear stress,"The initiation and progression of cerebral aneurysms are degenerative processes of the arterial wall driven by a complex interaction of biological and hemodynamic factors. Endothelial cells on the artery wall respond physiologically to blood-flow patterns. In normal conditions, these responses are associated with nonpathological tissue remodeling and adaptation. The combination of abnormal blood patterns and genetics predisposition could lead to the pathological formation of aneurysms. Here, we review recent progress on the basic mechanisms of aneurysm formation and evolution, with a focus on the role of hemodynamic patterns. [ABSTRACT FROM AUTHOR] Copyright of Annual Review of Fluid Mechanics is the property of Annual Reviews Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=36464258&site=ehost-live
168,Improving the speed and accuracy of projection-type incompressible flow solvers,Juan Cebral,Computer Methods in Applied Mechanics & Engineering,457825,,Apr-06,195,23/24,3087,23,20254209,10.1016/j.cma.2004.07.058,Elsevier B.V.,Article,GALERKIN methods; POISSON'S equation; COMPRESSIBILITY; PRESSURE,CFD; FEM; Incompressible flow solvers; Linelet preconditioning; LU-SGS; Multistage Runge–Kutta; Projection schemes,"Abstract: Superseding so-called first-generation incompressible flow solvers of the projection type (based on Taylor–Galerkin advection, second-order pressure damping and element-based data structures), the current, second-generation solvers (based on high-order upwind advection, fourth-order pressure damping and edge-based data structures) have now been in use for half a decade and have proven remarkably robust and efficient for many large-scale problems. In order to achieve higher accuracy and speed, these solvers have recently been enhanced in a variety of ways: (a) substepping for advection, (b) implicit treatment of advective terms via SGS and GMRES-LU-SGS iterative solvers, (c) fully implicit, time-accurate advancement of pressure and velocities, and (d) linelet preconditioning for the pressure-Poisson equation. The combined effect of these third-generation improvements leads to speedups of the order of O(1:5−1:10), with similar or even better temporal accuracy, as demonstrated on a variety of academic and industrial problems. [Copyright &y& Elsevier] Copyright of Computer Methods in Applied Mechanics & Engineering is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20254209&site=ehost-live
169,Incorporating variability of patient inflow conditions into statistical models for aneurysm rupture assessment.,Martin Slawski,Acta Neurochirurgica,16268,,Mar-20,162,3,553,14,141985830,10.1007/s00701-020-04234-8,Springer Nature,Article,STATISTICAL models; COMPUTATIONAL fluid dynamics; RECEIVER operating characteristic curves; INTRACRANIAL aneurysms; CEREBRAL circulation,Cerebral aneurysm; Computational fluid dynamics; Hemodynamics; Prediction; Risk factors; Rupture,"Background: Hemodynamic patterns have been associated with cerebral aneurysm instability. For patient-specific computational fluid dynamics (CFD) simulations, the inflow rates of a patient are typically not known. The aim of this study was to analyze the influence of inter- and intra-patient variations of cerebral blood flow on the computed hemodynamics through CFD simulations and to incorporate these variations into statistical models for aneurysm rupture prediction. Methods: Image data of 1820 aneurysms were used for patient-specific steady CFD simulations with nine different inflow rates per case, capturing inter- and intra-patient flow variations. Based on the computed flow fields, 17 hemodynamic parameters were calculated and compared for the different flow conditions. Next, statistical models for aneurysm rupture were trained in 1571 of the aneurysms including hemodynamic parameters capturing the flow variations either by defining hemodynamic ""response variables"" (model A) or repeatedly randomly selecting flow conditions by patients (model B) as well as morphological and patient-specific variables. Both models were evaluated in the remaining 249 cases. Results: All hemodynamic parameters were significantly different for the varying flow conditions (p < 0.001). Both the flow-independent ""response"" model A and the flow-dependent model B performed well with areas under the receiver operating characteristic curve of 0.8182 and 0.8174 ± 0.0045, respectively. Conclusions: The influence of inter- and intra-patient flow variations on computed hemodynamics can be taken into account in multivariate aneurysm rupture prediction models achieving a good predictive performance. Such models can be applied to CFD data independent of the specific inflow boundary conditions. [ABSTRACT FROM AUTHOR] Copyright of Acta Neurochirurgica is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141985830&site=ehost-live
169,Incorporating variability of patient inflow conditions into statistical models for aneurysm rupture assessment.,Juan Cebral,Acta Neurochirurgica,16268,,Mar-20,162,3,553,14,141985830,10.1007/s00701-020-04234-8,Springer Nature,Article,STATISTICAL models; COMPUTATIONAL fluid dynamics; RECEIVER operating characteristic curves; INTRACRANIAL aneurysms; CEREBRAL circulation,Cerebral aneurysm; Computational fluid dynamics; Hemodynamics; Prediction; Risk factors; Rupture,"Background: Hemodynamic patterns have been associated with cerebral aneurysm instability. For patient-specific computational fluid dynamics (CFD) simulations, the inflow rates of a patient are typically not known. The aim of this study was to analyze the influence of inter- and intra-patient variations of cerebral blood flow on the computed hemodynamics through CFD simulations and to incorporate these variations into statistical models for aneurysm rupture prediction. Methods: Image data of 1820 aneurysms were used for patient-specific steady CFD simulations with nine different inflow rates per case, capturing inter- and intra-patient flow variations. Based on the computed flow fields, 17 hemodynamic parameters were calculated and compared for the different flow conditions. Next, statistical models for aneurysm rupture were trained in 1571 of the aneurysms including hemodynamic parameters capturing the flow variations either by defining hemodynamic ""response variables"" (model A) or repeatedly randomly selecting flow conditions by patients (model B) as well as morphological and patient-specific variables. Both models were evaluated in the remaining 249 cases. Results: All hemodynamic parameters were significantly different for the varying flow conditions (p < 0.001). Both the flow-independent ""response"" model A and the flow-dependent model B performed well with areas under the receiver operating characteristic curve of 0.8182 and 0.8174 ± 0.0045, respectively. Conclusions: The influence of inter- and intra-patient flow variations on computed hemodynamics can be taken into account in multivariate aneurysm rupture prediction models achieving a good predictive performance. Such models can be applied to CFD data independent of the specific inflow boundary conditions. [ABSTRACT FROM AUTHOR] Copyright of Acta Neurochirurgica is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141985830&site=ehost-live
170,Mechanisms Involved in the Formation of BiocompatibleLipid Polymeric Hollow Patchy Particles.,Juan Cebral,Langmuir,7437463,,Jun-15,31,24,6639,10,103545644,10.1021/acs.langmuir.5b01551,American Chemical Society,Article,BIOMEDICAL materials; LIPID synthesis; ANISOTROPIC crystals; CROSS-sectional method; SHEARING force; SURFACE morphology,,"Patchypolymeric particles have anisotropic surface domains thatcan be remarkably useful in diverse medical and industrial fieldsbecause of their ability to simultaneously present two different surfacechemistries on the same construct. In this article, we report themechanisms involved in the formation of novel lipid–polymerichollow patchy particles during their synthesis. By cross-sectioningthe patchy particles, we found that a phase segregation phenomenonoccurs between the core, shell, and patch. Importantly, we found thatthe shear stress that the polymer blend undergoes during the particlesynthesis is the most important parameter for the formation of thesepatchy particles. In addition, we found that the interplay of solvent–solvent,polymer–solvent, and polymer–polymer–solventinteractions generates particles with different surface morphologies.Understanding the mechanisms involved in the formation of patchy particlesallows us to have a better control on their physicochemical properties.Therefore, these fundamental studies are critical to achieve batchcontrol and scalability, which are essential aspects that must beaddressed in any type of particle synthesis to be safely used in medicine. [ABSTRACT FROM AUTHOR] Copyright of Langmuir is the property of American Chemical Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103545644&site=ehost-live
171,Parabolic recovery of boundary gradients.,Juan Cebral,Communications in Numerical Methods in Engineering,10698299,,Dec-08,24,12,1611,5,35485663,10.1002/cnm.1054,Wiley-Blackwell,Article,STRAINS & stresses (Mechanics); STRUCTURAL analysis (Engineering); PARABOLIC differential equations; HEAT flux; HEAT transfer,CFD; higher order gradients; stress recovery,"A parabolic recovery procedure suited for shear stress and heat flux recovery on surfaces from linear element data is proposed. The information required consists of the usual unknowns at points, as well as gradients recovered at the points that are one layer away from the wall. The procedure has been in use for some time and has consistently delivered superior results as compared with the usual wall shear stress and heat flux obtained from linear finite element method shape functions. Copyright © 2007 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of Communications in Numerical Methods in Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35485663&site=ehost-live
172,Progress in computational fluid dynamics for bioengineering modelling.,Juan Cebral,International Journal of Computational Fluid Dynamics,10618562,,Sep-09,23,8,567,2,45020530,10.1080/10618560903411656,Taylor & Francis Ltd,Article,COMPUTATIONAL fluid dynamics; BIOMEDICAL engineering; FLUID dynamics; SENSITIVITY analysis; News Syndicates; EDITORIALS; TREATMENT of vascular diseases,,"The author reflects on the use of image-based computational fluid dynamics (CFD) in bioengineering and clinical problems. He notes the efforts in evaluating computational models such as model verification and sensitivity analysis. He notes that CFD method is used in understanding diseases, risk assessment, treatment planning, device design and optimization. Moreover, he discusses the used of the method in patient evaluation and treatment planning for vascular diseases.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=45020530&site=ehost-live
173,Quantification of the Rupture Potential of Patient-Specific Intracranial Aneurysms under Contact Constraints.,Juan Cebral,Bioengineering (Basel),23065354,,Nov-21,8,11,1,9,154092164,10.3390/bioengineering8110149,MDPI,Article,INTRACRANIAL aneurysms; PHYSICIANS; NERVE tissue; STRUCTURAL mechanics; OPTIC nerve; INTERNAL carotid artery; CEREBRAL arteries; Offices of physicians; Offices of Physicians (except Mental Health Specialists),contact constraints; effective wall stress; hyper-elastic membrane; intracranial aneurysms; rupture potential,"Intracranial aneurysms (IAs) are localized enlargements of cerebral blood vessels that cause substantial rates of mortality and morbidity in humans. The rupture possibility of these aneurysms is a critical medical challenge for physicians during treatment planning. This treatment planning while assessing the rupture potential of aneurysms becomes more complicated when they are constrained by an adjacent structure such as optic nerve tissues or bones, which is not widely studied yet. In this work, we considered and studied a constitutive model to investigate the bio-mechanical response of image-based patient-specific IA data using cardiovascular structural mechanics equations. We performed biomechanical modeling and simulations of four different patient-specific aneurysms' data (three middle cerebral arteries and one internal carotid artery) to assess the rupture potential of those aneurysms under a plane contact constraint. Our results suggest that aneurysms with plane contact constraints produce less or almost similar maximum wall effective stress compared to aneurysms with no contact constraints. In our research findings, we observed that a plane contact constraint on top of an internal carotid artery might work as a protective wall due to the 16.6% reduction in maximum wall effective stress than that for the case where there is no contact on top of the aneurysm. [ABSTRACT FROM AUTHOR] Copyright of Bioengineering (Basel) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154092164&site=ehost-live
174,Strategy for analysis of flow diverting devices based on multi-modality image-based modeling.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Oct-14,30,10,951,18,98676053,10.1002/cnm.2638,Wiley-Blackwell,Article,INTRACRANIAL aneurysms; HUMAN anatomical models; LABORATORY rabbits; BLOOD flow measurement; COMPUTATIONAL fluid dynamics; THROMBOSIS; ANGIOGRAPHY,cerebral aneurysm; CFD; flow diversion; hemodynamics; rabbit model,"SUMMARY Quantification and characterization of the hemodynamic environment created after flow diversion treatment of cerebral aneurysms is important to understand the effects of flow diverters and their interactions with the biology of the aneurysm wall and the thrombosis process that takes place subsequently. This paper describes the construction of multi-modality image-based subject-specific CFD models of experimentally created aneurysms in rabbits and subsequently treated with flow diverters. Briefly, anatomical models were constructed from 3D rotational angiography images, flow conditions were derived from Doppler ultrasound measurements, stent models were created and virtually deployed, and the results were compared with in vivo digital subtraction angiography and Doppler ultrasound images. The models were capable of reproducing in vivoobservations, including velocity waveforms measured in the parent artery, peak velocity values measured in the aneurysm, and flow structures observed with digital subtraction angiography before and after deployment of flow diverters. The results indicate that regions of aneurysm occlusion after flow diversion coincide with slow and smooth flow patterns, whereas regions still permeable at the time of animal sacrifice were observed in parts of the aneurysm exposed to larger flow activity, that is, higher velocities, more swirling, and more complex flow structures. Copyright © 2014 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98676053&site=ehost-live
175,Tracheal and Central Bronchial Aerodynamics Using Virtual Bronchoscopy and Computational Fluid Dynamics.,Juan Cebral,IEEE Transactions on Medical Imaging,2780062,,Aug-04,23,8,1021,13,14115656,10.1109/TMI.2004.828680,IEEE,Article,FLUID dynamics; TRACHEA; BRONCHOSCOPY; VISUALIZATION; DYNAMICS; ENDOSCOPY,,"Virtual bronchoscopy reconstructions of the airway noninvasively provide useful morphologic information of structural abnormalities such as stenoses and masses. In this paper, we show how virtual bronchoscopy can be used to perform aerodynamic calculations in anatomically realistic models. Pressure and flow patterns in a human airway were computed noninvasively. These showed decreased pressure and increased shear stress in the region of a stenosis. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Medical Imaging is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14115656&site=ehost-live
176,Unsteady wall shear stress analysis from image-based computational fluid dynamic aneurysm models under Newtonian and Casson rheological models.,Juan Cebral,Medical & Biological Engineering & Computing,1400118,,Oct-14,52,10,827,13,98286355,10.1007/s11517-014-1189-z,Springer Nature,Article,UNSTEADY flow; SHEARING force; COMPUTATIONAL fluid dynamics; INTRACRANIAL aneurysms; NEWTONIAN fluids; RHEOLOGY (Biology),Angiography; Casson flow; Cerebral aneurysms; Computational fluid dynamics; Wall shear stress,"The aim of this work was to determine whether or not Newtonian rheology assumption in image-based patient-specific computational fluid dynamics (CFD) cerebrovascular models harboring cerebral aneurysms may affect the hemodynamics characteristics, which have been previously associated with aneurysm progression and rupture. Ten patients with cerebral aneurysms with lobulations were considered. CFD models were reconstructed from 3DRA and 4DCTA images by means of region growing, deformable models, and an advancing front technique. Patient-specific FEM blood flow simulations were performed under Newtonian and Casson rheological models. Wall shear stress (WSS) maps were created and distributions were compared at the end diastole. Regions of lower WSS (lobulation) and higher WSS (neck) were identified. WSS changes in time were analyzed. Maximum, minimum and time-averaged values were calculated and statistically compared. WSS characterization remained unchanged. At high WSS regions, Casson rheology systematically produced higher WSS minimum, maximum and time-averaged values. However, those differences were not statistically significant. At low WSS regions, when averaging over all cases, the Casson model produced higher stresses, although in some cases the Newtonian model did. However, those differences were not significant either. There is no evidence that Newtonian model overestimates WSS. Differences are not statistically significant. [ABSTRACT FROM AUTHOR] Copyright of Medical & Biological Engineering & Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98286355&site=ehost-live
177,Association between hemodynamic conditions and occlusion times after flow diversion in cerebral aneurysms.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Apr-15,7,4,286,5,101747673,10.1136/neurintsurg-2013-011080,BMJ Publishing Group,Article,INTRACRANIAL aneurysm surgery; VASCULAR surgery; HEMODYNAMICS; CASE studies; RESEARCH funding; STATISTICS; DATA analysis; DATA analysis software; DESCRIPTIVE statistics,,"Background Evaluation of flow diversion treatment of intracranial aneurysms is difficult owing to lack of knowledge of the target hemodynamic environment. Objective To identify hemodynamic conditions created after flow diversion that induce fast aneurysm occlusion. Methods Two groups of aneurysms treated with flow diverters alone were selected: (a) aneurysms completely occluded at 3 months (fast occlusion), and (b) aneurysms patent or incompletely occluded at 6 months (slow occlusion). A total of 23 aneurysms were included in the study. Patient-specific computational fluid dynamics models were constructed and used to characterize the hemodynamic environment immediately before and after treatment. Average post-treatment hemodynamic conditions between the fast and slow occlusion groups were statistically compared. Results Aneurysms in the fast occlusion group had significantly lower post-treatment mean velocity (fast=1.13cm/s, slow=3.11cm/s, p=0.02), inflow rate (fast=0.47mL/s, slow=1.89mL/s, p=0.004) and shear rate (fast=20.52 1/s, slow=32.37 1/s, p=0.02) than aneurysms in the slow occlusion group. Receiver operating characteristics analysis showed that mean post-treatment velocity, inflow rate, and shear rate below a certain threshold could discriminate between aneurysms of the fast and slow occlusion groups with good accuracy (84%, 77%, and 76%, respectively). Conclusions The occlusion time of cerebral aneurysms treated with flow diverters can be predicted by the hemodynamic conditions created immediately after device implantation. Specifically, low post-implantation flow velocity, inflow rate, and shear rate are associated with fast occlusion times. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101747673&site=ehost-live
178,"Associations of hemodynamics, morphology, and patient characteristics with aneurysm rupture stratified by aneurysm location.",Juan Cebral,Neuroradiology,283940,,Mar-19,61,3,275,10,135115185,10.1007/s00234-018-2135-9,Springer Nature,Article,AGE distribution; CEREBRAL arteries; HEMODYNAMICS; INTRACRANIAL aneurysms; RUPTURED aneurysms; SEX distribution; SHEAR (Mechanics); LOGISTIC regression analysis; CAROTID artery dissections; DISEASE risk factors,Aneurysm; CFD; Hemodynamics; Morphology; Risk factors,"Purpose: The mechanisms of cerebral aneurysm rupture are not fully understood. We analyzed the associations of hemodynamics, morphology, and patient age and gender with aneurysm rupture stratifying by location.Methods: Using image-based models, 20 hemodynamic and 17 morphological parameters were compared in 1931 ruptured and unruptured aneurysms with univariate logistic regression. Rupture rates were compared between males and females as well as younger and older patients and bifurcation versus sidewall aneurysms for different aneurysm locations. Subsequently, associations between hemodynamics and morphology and patient as well as aneurysm characteristics were analyzed for aneurysms at five locations.Results: Compared to unruptured aneurysms, ruptured aneurysms were characterized by a more irregular shape and were exposed to a more adverse hemodynamic environment described by faster flow, higher wall shear stress, more oscillatory shear, and more unstable and complex flows. These associations with rupture status were consistent for different aneurysm locations. Rupture rates were significantly higher in males at the internal carotid artery (ICA) bifurcation, ophthalmic ICA, and the middle cerebral artery (MCA) bifurcation. At the anterior communicating artery (ACOM) and MCA bifurcation, they were significantly higher for younger patients. Bifurcation aneurysms had significantly larger rupture rates at the MCA and posterior communicating artery (PCOM). In these groups with higher rupture rates, aneurysms were characterized by adverse hemodynamics and more complex shapes.Conclusion: Hemodynamic and morphological differences between ruptured and unruptured aneurysms are consistent across locations. Adverse morphology and hemodynamics are related to rupture as well as younger age, male gender, and bifurcation aneurysms. [ABSTRACT FROM AUTHOR] Copyright of Neuroradiology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135115185&site=ehost-live
179,Computational hemodynamics framework for the analysis of cerebral aneurysms.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Jun-11,27,6,822,18,60826896,10.1002/cnm.1424,Wiley-Blackwell,Article,INTRACRANIAL aneurysms; HEMODYNAMICS; RUPTURED aneurysms; COMPUTATIONAL fluid dynamics; ANGIOGRAPHY,cerebral aneurysms; hemodynamics; patient-specific modeling; rupture risk,"Assessing the risk of rupture of intracranial aneurysms is important for clinicians because the natural rupture risk can be exceeded by the small but significant risk carried by current treatments. To this end numerous investigators have used image-based computational fluid dynamics models to extract patient-specific hemodynamics information, but there is no consensus on which variables or hemodynamic characteristics are the most important. This paper describes a computational framework to study and characterize the hemodynamic environment of cerebral aneurysms in order to relate it to clinical events, such as growth or rupture. In particular, a number of hemodynamic quantities are proposed to describe the most salient features of these hemodynamic environments. Application to a patient population indicates that ruptured aneurysms tend to have concentrated inflows, concentrated wall shear stress distributions, high maximal wall shear stress, and smaller viscous dissipation ratios than unruptured aneurysms. Furthermore, these statistical associations are largely unaffected by the choice of physiologic flow conditions. This confirms the notion that hemodynamic information derived from image-based computational models can be used to assess aneurysm rupture risk, to test hypotheses about the mechanisms responsible for aneurysm formation, progression, and rupture, and to answer specific clinical questions. Copyright © 2010 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60826896&site=ehost-live
180,Connecting curves in higher dimensions.,Juan Cebral,Journal of Physics: A Mathematical & Theoretical,17518113,,5/30/14,47,21,1,1,96340255,10.1088/1751-8113/47/21/215101,IOP Publishing,Article,CURVES; CURVILINEAR motion; PARAMETRIC equations; CURVES on surfaces; DYNAMICAL systems; APPLIED mathematics,,"Connecting curves have been shown to organize the rotational structure of strange attractors in three-dimensional dynamical systems. We extend the description of connecting curves and their properties to higher dimensions within a special class of differential dynamical systems. The general properties of connecting curves are derived and selection rules stated. Examples are presented to illustrate these properties for dynamical systems of dimension n = 3, 4, 5. [ABSTRACT FROM AUTHOR] Copyright of Journal of Physics: A Mathematical & Theoretical is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96340255&site=ehost-live
181,Development of a statistical model for discrimination of rupture status in posterior communicating artery aneurysms.,Juan Cebral,Acta Neurochirurgica,16268,,Aug-18,160,8,1643,10,130694515,10.1007/s00701-018-3595-8,Springer Nature,Article,INTRACRANIAL aneurysm ruptures; INTRACRANIAL aneurysms; COMPUTATIONAL fluid dynamics; INTERNAL carotid artery; HEMODYNAMICS; SHEARING force; DISEASE risk factors,Cerebral aneurysm; Hemodynamics; Morphology; Posterior communicating artery; Prediction; Rupture,"Background: Intracranial aneurysms at the posterior communicating artery (PCOM) are known to have high rupture rates compared to other locations. We developed and internally validated a statistical model discriminating between ruptured and unruptured PCOM aneurysms based on hemodynamic and geometric parameters, angio-architectures, and patient age with the objective of its future use for aneurysm risk assessment.Methods: A total of 289 PCOM aneurysms in 272 patients modeled with image-based computational fluid dynamics (CFD) were used to construct statistical models using logistic group lasso regression. These models were evaluated with respect to discrimination power and goodness of fit using tenfold nested cross-validation and a split-sample approach to mimic external validation.Results: The final model retained maximum and minimum wall shear stress (WSS), mean parent artery WSS, maximum and minimum oscillatory shear index, shear concentration index, and aneurysm peak flow velocity, along with aneurysm height and width, bulge location, non-sphericity index, mean Gaussian curvature, angio-architecture type, and patient age. The corresponding area under the curve (AUC) was 0.8359. When omitting data from each of the three largest contributing hospitals in turn, and applying the corresponding model on the left-out data, the AUCs were 0.7507, 0.7081, and 0.5842, respectively.Conclusions: Statistical models based on a combination of patient age, angio-architecture, hemodynamics, and geometric characteristics can discriminate between ruptured and unruptured PCOM aneurysms with an AUC of 84%. It is important to include data from different hospitals to create models of aneurysm rupture that are valid across hospital populations. [ABSTRACT FROM AUTHOR] Copyright of Acta Neurochirurgica is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=130694515&site=ehost-live
181,Development of a statistical model for discrimination of rupture status in posterior communicating artery aneurysms.,Martin Slawski,Acta Neurochirurgica,16268,,Aug-18,160,8,1643,10,130694515,10.1007/s00701-018-3595-8,Springer Nature,Article,INTRACRANIAL aneurysm ruptures; INTRACRANIAL aneurysms; COMPUTATIONAL fluid dynamics; INTERNAL carotid artery; HEMODYNAMICS; SHEARING force; DISEASE risk factors,Cerebral aneurysm; Hemodynamics; Morphology; Posterior communicating artery; Prediction; Rupture,"Background: Intracranial aneurysms at the posterior communicating artery (PCOM) are known to have high rupture rates compared to other locations. We developed and internally validated a statistical model discriminating between ruptured and unruptured PCOM aneurysms based on hemodynamic and geometric parameters, angio-architectures, and patient age with the objective of its future use for aneurysm risk assessment.Methods: A total of 289 PCOM aneurysms in 272 patients modeled with image-based computational fluid dynamics (CFD) were used to construct statistical models using logistic group lasso regression. These models were evaluated with respect to discrimination power and goodness of fit using tenfold nested cross-validation and a split-sample approach to mimic external validation.Results: The final model retained maximum and minimum wall shear stress (WSS), mean parent artery WSS, maximum and minimum oscillatory shear index, shear concentration index, and aneurysm peak flow velocity, along with aneurysm height and width, bulge location, non-sphericity index, mean Gaussian curvature, angio-architecture type, and patient age. The corresponding area under the curve (AUC) was 0.8359. When omitting data from each of the three largest contributing hospitals in turn, and applying the corresponding model on the left-out data, the AUCs were 0.7507, 0.7081, and 0.5842, respectively.Conclusions: Statistical models based on a combination of patient age, angio-architecture, hemodynamics, and geometric characteristics can discriminate between ruptured and unruptured PCOM aneurysms with an AUC of 84%. It is important to include data from different hospitals to create models of aneurysm rupture that are valid across hospital populations. [ABSTRACT FROM AUTHOR] Copyright of Acta Neurochirurgica is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=130694515&site=ehost-live
182,Effects of changing physiologic conditions on the in vivo quantification of hemodynamic variables in cerebral aneurysms treated with flow diverting devices.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Jan-14,30,1,135,8,93524954,10.1002/cnm.2594,Wiley-Blackwell,Article,COMPUTATIONAL fluid dynamics; HEMODYNAMICS; BLOOD circulation measurement; MEDICAL radiography; ANGIOGRAPHY; INTRACRANIAL aneurysms; Diagnostic Imaging Centers,cerebral aneurysm; computational fluid dynamics; flow diversion; flow quantification; hemodynamics; X‐ray angiography; X-ray angiography,"SUMMARY Quantifying the hemodynamic environment within aneurysms and its change after deployment of flow diverting devices is important to assess the device efficacy and understand their long-term effects. The purpose of this study was to estimate deviations in the quantification of the relative change of hemodynamic variables during flow diversion treatment of cerebral aneurysms due to changing physiologic flow conditions. Computational fluid dynamics calculations were carried out on three patient-specific geometries. Three flow diverters were virtually implanted in each geometry and simulations were performed under five pulsatile flow conditions. Hemodynamic variables including aneurysm inflow rate, mean velocity, shear rate, and wall shear stress were quantified before and after stenting. Deviations in the relative change of these variables due to varying flow conditions were calculated. The results indicate that a change in the mean flow of the parent artery of approximately 30-50% can induce large deviations in the relative change of hemodynamic variables in the range of 30-80%. Thus, quantification of hemodynamic changes during flow diversion must be carried out carefully. Variations in the inflow conditions during the procedure may induce large deviations in the quantification of these changes. Copyright © 2013 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=93524954&site=ehost-live
183,External validation of cerebral aneurysm rupture probability model with data from two patient cohorts.,Juan Cebral,Acta Neurochirurgica,16268,,Dec-18,160,12,2425,10,133160533,10.1007/s00701-018-3712-8,Springer Nature,Article,INTRACRANIAL aneurysms; INTRACRANIAL arterial diseases; RECEIVER operating characteristic curves; COMPUTATIONAL fluid dynamics; STROKE; RUPTURED aneurysms; GOODNESS-of-fit tests,Cerebral aneurysm; Hemodynamics; Prediction; Risk factors; Rupture; Shape,"Background: For a treatment decision of unruptured cerebral aneurysms, physicians and patients need to weigh the risk of treatment against the risk of hemorrhagic stroke caused by aneurysm rupture. The aim of this study was to externally evaluate a recently developed statistical aneurysm rupture probability model, which could potentially support such treatment decisions.Methods: Segmented image data and patient information obtained from two patient cohorts including 203 patients with 249 aneurysms were used for patient-specific computational fluid dynamics simulations and subsequent evaluation of the statistical model in terms of accuracy, discrimination, and goodness of fit. The model’s performance was further compared to a similarity-based approach for rupture assessment by identifying aneurysms in the training cohort that were similar in terms of hemodynamics and shape compared to a given aneurysm from the external cohorts.Results: When applied to the external data, the model achieved a good discrimination and goodness of fit (area under the receiver operating characteristic curve AUC = 0.82), which was only slightly reduced compared to the optimism-corrected AUC in the training population (AUC = 0.84). The accuracy metrics indicated a small decrease in accuracy compared to the training data (misclassification error of 0.24 vs. 0.21). The model’s prediction accuracy was improved when combined with the similarity approach (misclassification error of 0.14).Conclusions: The model’s performance measures indicated a good generalizability for data acquired at different clinical institutions. Combining the model-based and similarity-based approach could further improve the assessment and interpretation of new cases, demonstrating its potential use for clinical risk assessment. [ABSTRACT FROM AUTHOR] Copyright of Acta Neurochirurgica is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133160533&site=ehost-live
184,Fast numerical solutions of patient-specific blood flows in 3D arterial systems.,Juan Cebral,Communications in Numerical Methods in Engineering,10698299,,Jan-10,26,1,73,13,47267895,10.1002/cnm.1235,Wiley-Blackwell,Article,ARTERIES; THREE-dimensional imaging; DIAGNOSTIC imaging; ALGORITHMS; MEDICAL imaging systems; BIOMECHANICS; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,cerebral aneurysms; cerebral arteries; computational fluid dynamics; deflated conjugate gradients; hemodynamics,"The study of hemodynamics in arterial models constructed from patient-specific medical images requires the solution of the incompressible flow equations in geometries characterized by complex branching tubular structures. The main challenge with this kind of geometries is that the convergence rate of the pressure Poisson solver is dominated by the graph depth of the computational grid. This paper presents a deflated preconditioned conjugate gradients (DPCG) algorithm for accelerating the pressure Poisson solver. A subspace deflation technique is used to approximate the lowest eigenvalues along the tubular domains. This methodology was tested with an idealized cylindrical model and three patient-specific models of cerebral arteries and aneurysms constructed from medical images. For these cases, the number of iterations decreased by up to a factor of 16, while the total CPU time was reduced by up to 4 times when compared with the standard PCG solver. Copyright © 2009 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of Communications in Numerical Methods in Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=47267895&site=ehost-live
185,Gene expression comparison of flow diversion and coiling in an experimental aneurysm model.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Dec-15,7,12,926,5,111309662,10.1136/neurintsurg-2014-011452,BMJ Publishing Group,Article,"INTRACRANIAL aneurysm surgery; AGAR; ANIMAL experimentation; CEREBRAL circulation; CHEMOKINES; COLLAGEN; ELECTROPHORESIS; FIBRONECTINS; FRACTURE fixation; GENE expression; MUSCLE proteins; NEUROSURGERY; OXIDOREDUCTASES; RABBITS; SURGICAL instruments; TUMOR necrosis factors; MATRIX metalloproteinases; Medicinal and Botanical Manufacturing; Pharmaceutical and medicine manufacturing; Poultry Processing; Fur-Bearing Animal and Rabbit Production; Medical, Dental, and Hospital Equipment and Supplies Merchant Wholesalers; Surgical Appliance and Supplies Manufacturing; Surgical and Medical Instrument Manufacturing; Other Electronic and Precision Equipment Repair and Maintenance",,"Background and purpose Mechanisms of both healing and complications, including spontaneous aneurysm rupture, remain unclear following flow diverter treatment. The aim of this study was to compare gene expression of various key molecules involved in the healing of aneurysms, between aneurysms treated with microcoils and flow diverters. Methods Saccular aneurysms were created in rabbits. Aneurysms were treated with coils (n=6) or flow diverters (n=6). Aneurysms were harvested at 4 weeks following treatment and used for gene expression and zymography experiments. Genes with a fold change of 1.2 or more were considered upregulated whereas those with a fold change of 0.8 or less were considered downregulated. Results All coil embolized aneurysms were completely occluded at follow-up. Two aneurysms were occluded and the remaining four samples were incompletely occluded in the flow diverter treated group. The following genes were expressed at lower levels in the flow diverter group compared with the coiled aneurysm group: proteinases (matrix metalloproteinases 2 and 9), cellular markers (endothelial nitric oxide synthase and smooth muscle actin), and structural proteins (collagens and fibronectin). Genes related to inflammation (tumor necrosis factor α and monocyte chemoattractant protein 1) were upregulated in flow diverter treated aneurysms compared with coil embolized aneurysms. Notably, the enzymatic activity of active matrix metalloproteinase 9 was high in aneurysms treated with flow diverters. Conclusions Our findings may provide improved understanding of rupture risk and healing following aneurysm treatment and inform development of therapies aimed at lowering rupture risk and accelerating healing. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=111309662&site=ehost-live
186,Hemodynamic analysis of fast and slow aneurysm occlusions by flow diversion in rabbits.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Dec-15,7,12,931,5,111309660,10.1136/neurintsurg-2014-011412,BMJ Publishing Group,Article,INTRACRANIAL aneurysm surgery; ANIMAL experimentation; BLOOD flow measurement; VASCULAR surgery; CEREBRAL angiography; CEREBRAL circulation; HEMODYNAMICS; INTRACRANIAL aneurysms; PROTEOLYTIC enzymes; RABBITS; Poultry Processing; Fur-Bearing Animal and Rabbit Production,,"Purpose To assess hemodynamic differences between aneurysms that occlude rapidly and those occluding in delayed fashion after flow diversion in rabbits. Methods Thirty-six elastase-induced aneurysms in rabbits were treated with flow diverting devices. Aneurysm occlusion was assessed angiographically immediately before they were sacrificed at 1 (n=6), 2 (n=4), 4 (n=8) or 8 weeks (n=18) after treatment. The aneurysms were classified into a fast occlusion group if they were completely or near completely occluded at 4 weeks or earlier and a slow occlusion group if they remained incompletely occluded at 8 weeks. The immediate post-treatment flow conditions in aneurysms of each group were quantified using subject-specific computational fluid dynamics and statistically compared. Results Nine aneurysms were classified into the fast occlusion group and six into the slow occlusion group. Aneurysms in the fast occlusion group were on average significantly smaller (fast=0.9 cm, slow=1.393 cm, p=0.024) and had smaller ostia (fast=0.144 cm2, slow=0.365 cm2, p=0.015) than aneurysms in the slow occlusion group. They also had a lower mean posttreatment inflow rate (fast=0.047 mL/s, slow=0.155 mL/s, p=0.0239), kinetic energy (fast=0.519 erg, slow=1.283 erg, p=0.0468), and velocity (fast=0.221 cm/s, slow=0.506 cm/s, p=0.0582). However, the differences in the latter two variables were only marginally significant. Conclusions Hemodynamic conditions after flow diversion treatment of cerebral aneurysms in rabbits are associated with the subsequent aneurysm occlusion time. Specifically, smaller inflow rate, kinetic energy, and velocity seem to promote faster occlusions, especially in smaller and small-necked aneurysms. These results are consistent with previous studies based on clinical series. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=111309660&site=ehost-live
187,Hemodynamic analysis of intracranial aneurysms with moving parent arteries: Basilar tip aneurysms.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Oct-10,26,10,1219,9,53978655,10.1002/cnm.1385,Wiley-Blackwell,Article,INTRACRANIAL aneurysm ruptures; HEMODYNAMICS; COMPUTATIONAL fluid dynamics; VERTEBROBASILAR aneurysms; ARTERIOGRAPHY,angiography; arterial wall motion; basilar artery; cerebral aneurysms; hemodynamics,"The effects of parent artery motion on the hemodynamics of basilar tip saccular aneurysms and its potential effect on aneurysm rupture were studied. The aneurysm and parent artery motions in two patients were determined from cine loops of dynamic angiographies. The oscillatory motion amplitude was quantified by registering the frames. Patient-specific computational fluid dynamics (CFD) models of both aneurysms were constructed from 3D rotational angiography images. Two CFD calculations were performed for each patient, corresponding to static and moving models. The motion estimated from the dynamic images was used to move the surface grid points in the moving model. Visualizations from the simulations were compared for wall shear stress (WSS), velocity profiles, and streamlines. In both patients, a rigid oscillation of the aneurysm and basilar artery in the anterio-posterior direction was observed and measured. The distribution of WSS was nearly identical between the models of each patient, as well as major intra-aneurysmal flow structures, inflow jets, and regions of impingement. The motion observed in pulsating intracranial vasculature does not have a major impact on intra-aneurysmal hemodynamic variables. Parent artery motion is unlikely to be a risk factor for increased risk of aneurysmal rupture. Copyright © 2010 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53978655&site=ehost-live
188,Hemodynamics in growing and stable cerebral aneurysms.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Apr-16,8,4,407,6,114159465,10.1136/neurintsurg-2014-011339,BMJ Publishing Group,Article,BIOLOGICAL models; CEREBRAL circulation; HEMODYNAMICS; INTRACRANIAL aneurysms; SHEAR (Mechanics); PSYCHOLOGICAL stress; LOGISTIC regression analysis; THREE-dimensional imaging,,"Objective The detailed mechanisms of cerebral aneurysm evolution are poorly understood but are important for objective aneurysm evaluation and improved patient management. The purpose of this study was to identify hemodynamic conditions that may predispose aneurysms to growth. Methods A total of 33 intracranial unruptured aneurysms longitudinally followed with three-dimensional imaging were studied. Patient-specific computational fluid dynamics models were constructed and used to quantitatively characterize the hemodynamic environments of these aneurysms. Hemodynamic characteristics of growing (n=16) and stable (n=17) aneurysms were compared. Logistic regression statistical models were constructed to test the predictability of aneurysm growth by hemodynamic features. Results Growing aneurysms had significantly smaller shear rate ratios (p=0.01), higher concentration of wall shear stress (p=0.03), smaller vorticity ratios (p=0.01), and smaller viscous dissipation ratios (p=0.01) than stable aneurysms. They also tended to have larger areas under low wall shear stress (p=0.06) and larger aspect ratios (p=0.18), but these trends were not significant. Mean wall shear stress was not significantly different between growing and stable aneurysms. Logistic regression models based on hemodynamic variables were able to discriminate between growing and stable aneurysms with a high degree of accuracy (94-100%). Conclusions Growing aneurysms tend to have complex intrasaccular flow patterns that induce non-uniform wall shear stress distributions with areas of concentrated high wall shear stress and large areas of low wall shear stress. Statistical models based on hemodynamic features seem capable of discriminating between growing and stable aneurysms. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114159465&site=ehost-live
189,Hemodynamics in two tandem aneurysms treated with flow diverters.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Apr-14,30,4,517,8,95322584,10.1002/cnm.2614,Wiley-Blackwell,Article,HEMODYNAMICS; ANEURYSMS; TANDEM mass spectrometers; FUSION reactor divertors; ANGIOGRAPHY,cerebral aneurysms; flow diverters; hemodynamics,"SUMMARY The purpose of this study was to investigate whether the occlusion time of cerebral aneurysms treated with flow diverters depends on the hemodynamic conditions created immediately after treatment. A case study of a pair of tandem intracranial aneurysms that were treated with flow-diverting devices and occluded at different times was carried out. A patient-specific computational fluid dynamics model was constructed from 3D rotational angiography images. Blood flow simulations were carried out under pulsatile physiologic conditions, and hemodynamic variables before and after deployment of the flow-diverting devices were quantified and compared. The flow-diverting devices reduced aneurysm inflow rates, intra-aneurysmal flow velocities, shear rates, and wall shear stresses. The flow patterns after flow modulation by the flow diverters were smoother and with less swirling. The reductions in hemodynamic quantities depended on the aneurysm and parent artery and were larger in the aneurysm that occluded faster. The results of this case study suggest that the larger the reduction in the hemodynamic variables considered, the shorter the time it takes for the aneurysm to thrombose. This result can help us better define the goal of these interventions. Copyright © 2013 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95322584&site=ehost-live
190,Image‐based modeling of blood flow in cerebral aneurysms treated with intrasaccular flow diverting devices.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Jun-19,35,6,N.PAG,1,136997892,10.1002/cnm.3202,Wiley-Blackwell,Article,INTRACRANIAL aneurysms; CEREBRAL circulation; HEMODYNAMICS,cerebral aneurysms; computational hemodynamics; flow diversion; intrasaccular flow diverter,"Modeling the flow dynamics in cerebral aneurysms after the implantation of intrasaccular devices is important for understanding the relationship between flow conditions created immediately posttreatment and the subsequent outcomes. This information, ideally available a priori based on computational modeling prior to implantation, is valuable to identify which aneurysms will occlude immediately and which aneurysms will likely remain patent and would benefit from a different procedure or device. In this report, a methodology for modeling the hemodynamics in intracranial aneurysms treated with intrasaccular flow diverting devices is described. This approach combines an image‐guided, virtual device deployment within patient‐specific vascular models with an immersed boundary method on adaptive unstructured grids. A partial mesh refinement strategy that reduces the number of mesh elements near the aneurysm dome where the flow conditions are largely stagnant was compared with the full refinement strategy that refines the mesh everywhere around the device wires. The results indicate that using the partial mesh refinement approach is adequate for analyzing the posttreatment hemodynamics, at a reduced computational cost. The results obtained on a series of four cerebral aneurysms treated with different intrasaccular devices were in good qualitative agreement with angiographic observations. Promising results were obtained relating posttreatment flow conditions and outcomes of treatments with intrasaccular devices, which need to be confirmed on larger series. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136997892&site=ehost-live
191,Patient-specific flow analysis of brain aneurysms at a single location: comparison of hemodynamic characteristics in small aneurysms.,Juan Cebral,Medical & Biological Engineering & Computing,1400118,,Nov-08,46,11,1113,8,35261178,10.1007/s11517-008-0400-5,Springer Nature,Article,ANEURYSMS; BRAIN diseases; HEMODYNAMICS; BLOOD circulation; VASCULAR diseases,Brain aneurysm; Flow analysis; Hemodynamics,"The purpose of this study is to examine and compare the hemodynamic characteristics of small aneurysms at the same anatomical location. Six internal carotid artery-ophthalmic artery aneurysms smaller than 10 mm were selected. Image-based computational fluid dynamics (CFD) techniques were used to simulate aneurysm hemodynamics. Flow velocity and wall shear stress (WSS) were also quantitatively compared, both in absolute value and relative value using the parent artery as a baseline. We found that flow properties were similar in ruptured and unruptured small aneurysms. However, the WSS was lower at the aneurysm site in unruptured aneurysms and higher in ruptured aneurysms ( P < 0.05). Hemodynamic analyses at a single location with similar size enabled us to directly compare the hemodynamics and clinical presentation of brain aneurysms. The results suggest that the WSS in an aneurysm sac can be an important hemodynamic parameter related to the mechanism of brain aneurysm growth and rupture. [ABSTRACT FROM AUTHOR] Copyright of Medical & Biological Engineering & Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35261178&site=ehost-live
192,Patient-specific hemodynamic analysis of small internal carotid artery-ophthalmic artery aneurysms,Juan Cebral,Surgical Neurology,903019,,Nov-09,72,5,444,7,45638735,10.1016/j.surneu.2008.12.013,Elsevier B.V.,Article,INTRACRANIAL aneurysm ruptures; BRAIN disease treatment; VASCULAR diseases; CAROTID artery diseases; HEMODYNAMICS; STANDARD deviations; COMPUTATIONAL fluid dynamics,Cerebral aneurysm; computational fluid dynamics ( CFD ); Flow analysis; Hemodynamics; internal carotid artery-ophthalmic artery ( ICA-Oph ); standard deviation ( SD ); Wall shear stress; wall shear stress ( WSS ),"Abstract: Background: Prophylactic treatment of unruptured small brain aneurysms is still controversial due to the low risk of rupture. Distinguishing which small aneurysms are at risk for rupture has become important for treatment. Previous studies have indicated a variety of hemodynamic properties that may influence aneurysm rupture. This study uses hemodynamic principles to evaluate these in the context of ruptured and unruptured small aneurysms in a single location. Methods: Eight small internal carotid artery-ophthalmic artery (ICA-Oph) aneurysms (<10 mm) were selected from the University of California, Los Angeles, database. We analyzed rupture-related hemodynamic characteristics including flow patterns, wall shear stress (WSS), and flow impingement using previously developed patient-specific computational fluid dynamics software. Results: Most ruptured aneurysms had complicated flow patterns in the aneurysm domes, but all of the unruptured cases showed a simple vortex. A reduction in flow velocity between the parent artery and the aneurysm sac was found in all the cases. Inside the aneurysms, the highest flow velocities were found either at the apex or neck. We also observed a trend of higher and more inhomogeneous WSS distribution within ruptured aneurysms (10.66 ± 5.99 Pa) in comparison with the unruptured ones (6.31 ± 6.47 Pa) (P < .01). Conclusion: A comparison of hemodynamic properties between ruptured and unruptured small ICA-Oph aneurysms found that some hemodynamic properties vary between small aneurysms although they are similar in size and share the same anatomical location. In particular, WSS may be a useful hemodynamic factor for studying small aneurysm rupture. [Copyright &y& Elsevier] Copyright of Surgical Neurology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=45638735&site=ehost-live
193,Plaque Morphology in High Versus Low Shear Stress Regions of Human Carotid Stenosis.,Juan Cebral,FASEB Journal,8926638,,Apr-07,21,5,A16,0,25631633,,"John Wiley & Sons, Inc.",Article,MORPHOLOGY; PHYSIOLOGICAL stress; ATHEROSCLEROTIC plaque; HEMORRHAGE; CAROTID artery stenosis,,"Background: A role for high shear stress in plaque destabilization has been postulated. We investigate this potential effect by correlating carotid plaque morphology directly with in vivo wall shear stress measurements. Materials and Methods: Carotid endarterectomy patients were enrolled in an IRB approved study using magnetic resonance angiography (MRA) to measure in vivo wall shear stress. After endarterectomy, the specimen was oriented, cross sectioned for histology at 2 mm intervals, and photographed. The shear stress correlating with each histologic segment of vessel was calculated from presurgical MRA using Navier-Stokes finite element analysis. The plaque regions were evaluated quantitatively for area of necrosis, calcification, hemorrhage and inflammation; fibrous cap and intimal thickness, plaque erosion, rupture or thrombosis. Results: Complete analysis with shear stress correlation has been performed on twenty fields of plaque. No erosion, rupture or thrombosis was noted. Low shear stress areas had a thinner plaque (1.42 vs 230) and thinner fibrous cap (11 vs 35). Intraplaque hemorrhage was significantly more prevalent in high shear stress (p=0.001, as was calcification (p=0.04), but inflammation and necrosis were not. Conclusion: We can correlate quantitative and qualitative morphologic features and eventually proteomic and genomic alterations within areas of low and high shear stress in atherosclerotic plaques. Our quantitative data supports the concept that high shear stress leads to destabilization of the plaque indicated predominantly by more intraplaque hemorrhage. [ABSTRACT FROM AUTHOR] Copyright of FASEB Journal is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=25631633&site=ehost-live
194,Relationship between aneurysm occlusion and flow diverting device oversizing in a rabbit model.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Jan-16,8,1,94,5,112059232,10.1136/neurintsurg-2014-011487,BMJ Publishing Group,Article,ANEURYSM surgery; ANEURYSMS; ANGIOGRAPHY; ANIMAL experimentation; RABBITS; SURGICAL stents; TREATMENT effectiveness; Fur-Bearing Animal and Rabbit Production; Poultry Processing,,"Background and purpose Implanted, actual flow diverter pore density is thought to be strongly influenced by proper matching between the device size and parent artery diameter. The objective of this study was to characterize the correlation between device sizing, metal coverage, and the resultant occlusion of aneurysms following flow diverter treatment in a rabbit model. Methods Rabbit saccular aneurysms were treated with flow diverters (iso-sized to proximal parent artery, 0.5 mm oversized, or 1.0 mm oversized, respectively, n=6 for each group). Eight weeks after implantation, the angiographic degree of aneurysm occlusion was graded (complete, near-complete, or incomplete). The ostium of the explanted aneurysm covered with the flow diverter struts was photographed. Based on gross anatomic findings, the metal coverage and pore density at the ostium of the aneurysm were calculated and correlated with the degree of aneurysm occlusion. Results Angiographic results showed there were no statistically significant differences in aneurysm geometry and occlusion among groups. The mean parent artery diameter to flow diverter diameter ratio was higher in the 1.0 mm oversized group than in the other groups. Neither the percentage metal coverage nor the pore density showed statistically significant differences among groups. Aneurysm occlusion was inversely correlated with the ostium diameter, irrespective of the size of the device implanted. Conclusions Device sizing alone does not predict resultant pore density or metal coverage following flow diverter implantation in the rabbit aneurysm model. Aneurysm occlusion was not impacted by either metal coverage or pore density, but was inversely correlated with the diameter of the ostium. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=112059232&site=ehost-live
195,The Computational Fluid Dynamics Rupture Challenge 2013-Phase II: Variability of Hemodynamic Simulations in Two Intracranial Aneurysms.,Juan Cebral,Journal of Biomechanical Engineering,1480731,,Dec-15,137,12,121008-1,13,112962876,10.1115/1.4031794,American Society of Mechanical Engineers,Article,RUPTURED aneurysms; COMPUTATIONAL fluid dynamics; INTRACRANIAL aneurysms; BLOOD flow; HEMODYNAMICS,challenge; computational fluid dynamics; intracranial aneurysm; variability,"With the increased availability of computational resources, the past decade has seen a rise in the use of computational fluid dynamics (CFD) for medical applications. There has been an increase in the application of CFD to attempt to predict the rupture of intracranial aneurysms, however, while many hemodynamic parameters can be obtained from these computations, to date, no consistent methodology for the prediction of the rupture has been identified. One particular challenge to CFD is that many factors contribute to its accuracy; the mesh resolution and spatial!temporal discretization can alone contribute to a variation in accuracy. This failure to identify the importance of these factors and identify a methodology for the prediction of ruptures has limited the acceptance of CFD among physicians for rupture prediction. The International CFD Rupture Challenge 2013 seeks to comment on the sensitivity of these various CFD assumptions to predict the rupture by undertaking a comparison of the rupture and blood-flow predictions from a wide range of independent participants utilizing a range of CFD approaches. Twenty-six groups from 15 countries took part in the challenge. Participants were provided with surface models of two intracranial aneurysms and asked to carry out the corresponding hemodynamics simulations, free to choose their own mesh, solver, and temporal discretization. They were requested to submit velocity and pressure predictions along the centerline and on specified planes. The first phase of the challenge, described in a separate paper, was aimed at predicting which of the two aneutysms had previously ruptured and where the rupture site was located. The second phase, described in this paper, aims to assess the variability of the solutions and the sensitivity to the modeling assumptions. Panticipants were free to choose boundaty conditions in the first phase, whereas they were prescribed in the second phase but all other CFD modeling parameters were not prescribed. In order to compare the computational results of one representative group with experimental results, steady-flow measurements using particle image velocimetry (P1V) were carried out in a silicone model of one of the provided aneutysms. Approximately 80% of the participating groups generated similar results. Both velocity and pressure computations were in good agreement with each other for cycle-averaged and peaksystolic predictions. Most apparent ""outliers"" (results that stand out of the collective) were observed to have underestimated velocity levels compared to the majority of solutions, but nevertheless identified comparable flow structures. In only two cases, the results deviate by over 35% from the mean solution of all the participants. Results of steady CFD simulations of the representative group and PIV experiments were in good agreement. The study demonstrated that white a range of numerical schemes, mesh resolution, and solvers was used, similar flow predictions were observed in the majority of cases. To further validate the computational results, it is suggested that time-dependent measurements should be conducted in the future. However, it is recognized that this study does not include the biological aspects of the aneuiysm, which needs to be considered to be able to more precisely identify the specific rupture risk of an intracranial aneuiysm. [ABSTRACT FROM AUTHOR] Copyright of Journal of Biomechanical Engineering is the property of American Society of Mechanical Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=112962876&site=ehost-live
196,The effect of aneurysm geometry on the intra-aneurysmal flow condition.,Juan Cebral,Neuroradiology,283940,,Dec-10,52,12,1135,7,55315026,10.1007/s00234-010-0687-4,Springer Nature,Article,BLOOD flow measurement; BRAIN; CEREBRAL angiography; COMPUTER simulation; STATISTICAL correlation; HEMODYNAMICS; INTRACRANIAL aneurysms; QUANTITATIVE research; EVALUATION; PATHOLOGY; RADIOGRAPHY; DONOR blood supply,Aneurysm shape; Aneurysm size; Computational fluid dynamics simulation; Hemodynamics; Intracranial aneurysm,"Introduction: Various anatomical parameters affect on intra-aneurysmal hemodynamics. Nevertheless, how the shapes of real patient aneurysms affect on their intra-aneurysmal hemodynamics remains unanswered. Methods: Quantitative computational fluid dynamics simulation was conducted using eight patients' angiograms of internal carotid artery-ophthalmic artery aneurysms. The mean size of the intracranial aneurysms was 11.5 mm (range 5.8 to 19.9 mm). Intra-aneurysmal blood flow velocity and wall shear stress (WSS) were collected from three measurement planes in each aneurysm dome. The correlation coefficients ( r) were obtained between hemodynamic values (flow velocity and WSS) and the following anatomical parameters: averaged dimension of aneurysm dome, the largest aneurysm dome dimension, aspect ratio, and dome-neck ratio. Results: Negative linear correlations were observed between the averaged dimension of aneurysm dome and intra-aneurysmal flow velocity ( r = −0.735) and also WSS ( r = −0.736). The largest dome diameter showed a negative correlation with intra-aneurysmal flow velocity ( r = −0.731) and WSS ( r = −0.496). The aspect ratio demonstrated a weak negative correlation with the intra-aneurysmal flow velocity ( r = −0.381) and WSS ( r = −0.501). A clear negative correlation was seen between the intra-aneurysmal flow velocity and the dome-neck ratio ( r = −0.708). A weak negative correlation is observed between the intra-aneurysmal WSS and the dome-neck ratio ( r = −0.392). Conclusion: The aneurysm dome size showed a negative linear correlation with intra-aneurysmal flow velocity and WSS. Wide-necked aneurysm geometry was associated with faster intra-aneurysmal flow velocity. [ABSTRACT FROM AUTHOR] Copyright of Neuroradiology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=55315026&site=ehost-live
197,Computational modelling of blood flow in side arterial branches after stenting of cerebral aneurysms.,Juan Cebral,International Journal of Computational Fluid Dynamics,10618562,,Dec-08,22,10,669,8,35348388,10.1080/10618560802495255,Taylor & Francis Ltd,Article,SURGICAL stents; INTRACRANIAL aneurysms; HEMODYNAMIC monitoring; BLOOD flow; CEREBRAL ischemia; ARTERIAL occlusions; PHYSIOLOGY; THERAPEUTICS; All Other Miscellaneous Ambulatory Health Care Services; Medical equipment and supplies manufacturing; Surgical and Medical Instrument Manufacturing; All other ambulatory health care services,cerebral aneurysms; haemodynamics; image-based modelling; perforators; stenting,"The major concern with the use of stents as flow diverters for the treatment of intracranial aneurysms is the potential occlusion of a perforating artery or other side branches which can cause ischemic strokes. This article presents image-based patient-specific models of stented cerebral aneurysms in which a small side artery has been jailed by the stent mesh. The results indicate that, because of the large resistances of the distal vascular beds which dominate the flow divisions among the different arterial branches, the flow reduction in jailed side branches is quite small even when a large percentage of the inlet area of these branches has been blocked. This suggests that unless the side branch is completely occluded, it will likely maintain its normal blood flow. Although this conclusion eases the concern of stenting cerebral aneurysms, a complete occlusion can still be caused depending on the conformability characteristics of the stents. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Computational Fluid Dynamics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35348388&site=ehost-live
198,Deflated preconditioned conjugate gradient solvers for the Pressure–Poisson equation,Juan Cebral,Journal of Computational Physics,219991,,Dec-08,227,24,10196,13,34954917,10.1016/j.jcp.2008.08.025,Academic Press Inc.,Article,CONJUGATE gradient methods; APPROXIMATION theory; NUMERICAL solutions to equations; FLUID dynamics,CFD; Conjugate gradients; Finite elements; Incompressible flow solvers; Pressure–Poisson equation,"Abstract: A deflated preconditioned conjugate gradient technique has been developed for the solution of the Pressure–Poisson equation within an incompressible flow solver. The deflation is done using a region-based decomposition of the unknowns, making it extremely simple to implement. The procedure has shown a considerable reduction in the number of iterations. For grids with large graph-depth the savings exceed an order of magnitude. Furthermore, the technique has shown a remarkable insensitivity to the number of groups/regions chosen, and to the way the groups are formed. [Copyright &y& Elsevier] Copyright of Journal of Computational Physics is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34954917&site=ehost-live
199,Simulation of intracranial aneurysm stenting: Techniques and challenges,Juan Cebral,Computer Methods in Applied Mechanics & Engineering,457825,,Sep-09,198,45/46,3567,16,44259967,10.1016/j.cma.2009.01.017,Elsevier B.V.,Article,SIMULATION methods & models; INTRACRANIAL aneurysms; BLOOD flow; COMPUTATIONAL fluid dynamics; HEMODYNAMICS; MEDICAL imaging systems,Adaptive embedded unstructured grids; Cerebral aneurysms; Computational fluid dynamics; Hemodynamics; Stenting,"Abstract: Recently, there has been considerable interest in the use of stents as endovascular flow diverters for the treatment of intracranial aneurysms. Simulating this novel method of treatment is essential for understanding the intra-aneurysmal hemodynamics in order to design better stents and to personalize and optimize the endovascular stenting procedures. This paper describes a methodology based on unstructured embedded grids for patient-specific modeling of stented cerebral aneurysms, demonstrates how the methodology can be used to address specific clinical questions, and discusses remaining technical issues. In particular, simulations are presented on a number of patient-specific models constructed from medical images and using different stent designs and treatment alternatives. Preliminary sensitivity analyses with respect to stent positioning and truncation of the stent model are presented. The results show that these simulations provide useful and valuable information that can be used during the planning phase of endovascular stenting interventions for the treatment of intracranial aneurysms. [Copyright &y& Elsevier] Copyright of Computer Methods in Applied Mechanics & Engineering is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=44259967&site=ehost-live
200,A Sequential Budget Allocation Framework for Simulation Optimization.,Chun-Hung Chen,IEEE Transactions on Automation Science & Engineering,15455955,,Apr-17,14,2,1185,10,122420359,10.1109/TASE.2015.2501386,IEEE,Article,MANUFACTURING industries; RESOURCE management; SIMULATION methods & models; ITERATIVE methods (Mathematics); PROBLEM solving,Computational modeling; Exploration and exploitation; Indexes; Numerical models; Optimization; Resource management; sequential procedure; simulation budget allocation; simulation optimization; Space exploration,"Many problems in automation and manufacturing are most suitable to be modeled as simulation optimization problems. Solving these problems typically involves two efforts: one is to explore the solution space, and the other is to exploit the performance values of the sampled solutions. When the amount of computing budget is limited, we need to know how to balance these two efforts in order to obtain the best result. In this study, we derive two measures to quantify the marginal contribution of exploring the search space and exploiting the performance values. A sequential budget allocation framework is designed by keeping the two measures approximately the same at each iteration. Numerical experiments on both continuous and discrete simulation optimization problems demonstrate that our new approach can significantly enhance the computing efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122420359&site=ehost-live
201,An efficient simulation budget allocation method incorporating regression for partitioned domains.,Chun-Hung Chen,Automatica,51098,,May-14,50,5,1391,10,95813698,10.1016/j.automatica.2014.03.011,Elsevier B.V.,Article,SIMULATION methods & models; BUDGET; ALLOCATION (Accounting); PARTITIONS (Mathematics); REGRESSION analysis; MATHEMATICAL domains; DECISION making; Public Finance Activities,Budget allocation; Regression; Simulation,"Abstract: Simulation can be a very powerful tool to help decision making in many applications but exploring multiple courses of actions can be time consuming. Numerous ranking and selection (R&S) procedures have been developed to enhance the simulation efficiency of finding the best design. To further improve efficiency, one approach is to incorporate information from across the domain into a regression equation. However, the use of a regression metamodel also inherits some typical assumptions from most regression approaches, such as the assumption of an underlying quadratic function and the simulation noise is homogeneous across the domain of interest. To extend the limitation while retaining the efficiency benefit, we propose to partition the domain of interest such that in each partition the mean of the underlying function is approximately quadratic. Our new method provides approximately optimal rules for between and within partitions that determine the number of samples allocated to each design location. The goal is to maximize the probability of correctly selecting the best design. Numerical experiments demonstrate that our new approach can dramatically enhance efficiency over existing efficient R&S methods. [Copyright &y& Elsevier] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95813698&site=ehost-live
201,An efficient simulation budget allocation method incorporating regression for partitioned domains.,Jie Xu,Automatica,51098,,May-14,50,5,1391,10,95813698,10.1016/j.automatica.2014.03.011,Elsevier B.V.,Article,SIMULATION methods & models; BUDGET; ALLOCATION (Accounting); PARTITIONS (Mathematics); REGRESSION analysis; MATHEMATICAL domains; DECISION making; Public Finance Activities,Budget allocation; Regression; Simulation,"Abstract: Simulation can be a very powerful tool to help decision making in many applications but exploring multiple courses of actions can be time consuming. Numerous ranking and selection (R&S) procedures have been developed to enhance the simulation efficiency of finding the best design. To further improve efficiency, one approach is to incorporate information from across the domain into a regression equation. However, the use of a regression metamodel also inherits some typical assumptions from most regression approaches, such as the assumption of an underlying quadratic function and the simulation noise is homogeneous across the domain of interest. To extend the limitation while retaining the efficiency benefit, we propose to partition the domain of interest such that in each partition the mean of the underlying function is approximately quadratic. Our new method provides approximately optimal rules for between and within partitions that determine the number of samples allocated to each design location. The goal is to maximize the probability of correctly selecting the best design. Numerical experiments demonstrate that our new approach can dramatically enhance efficiency over existing efficient R&S methods. [Copyright &y& Elsevier] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95813698&site=ehost-live
202,An Optimal Sample Allocation Strategy for Partition-Based Random Search.,Chun-Hung Chen,IEEE Transactions on Automation Science & Engineering,15455955,,Jan-14,11,1,177,10,93570384,10.1109/TASE.2013.2251881,IEEE,Article,OPTIMAL control theory; SEARCH algorithms; GLOBAL optimization; ITERATIVE methods (Mathematics); NUMERICAL analysis; RANDOM variables,Global optimization; optimal sample allocation; partition-based random search,"Partition-based random search (PRS) provides a class of effective algorithms for global optimization. In each iteration of a PRS algorithm, the solution space is partitioned into subsets which are randomly sampled and evaluated. One subset is then determined to be the promising subset for further partitioning. In this paper, we propose the problem of allocating samples to each subset so that the samples are utilized most efficiently. Two types of sample allocation problems are discussed, with objectives of maximizing the probability of correctly selecting the promising subset (P\CSPS\) given a sample budget and minimizing the required sample size to achieve a satisfied level of P\CSPS\, respectively. An extreme value-based prospectiveness criterion is introduced and an asymptotically optimal solution to the two types of sample allocation problems is developed. The resulting optimal sample allocation strategy (OSAS) is an effective procedure for the existing PRS algorithms by intelligently utilizing the limited computing resources. Numerical tests confirm that OSAS is capable of increasing the P\CSPS\ in each iteration and subsequently improving the performance of PRS algorithms. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=93570384&site=ehost-live
203,An optimal -statistics quantile estimator for a set of location–scale populations,Chun-Hung Chen,Statistics & Probability Letters,1677152,,Oct-12,82,10,1853,6,77961656,10.1016/j.spl.2012.05.015,Elsevier B.V.,Article,OPTIMAL designs (Statistics); REGRESSION quantiles; VECTOR calculus; CONSTRAINED optimization; MEAN square algorithms; ERROR analysis in mathematics; PARAMETERS (Statistics),-statistics; Location equivariance; Location–scale distributions; Quantile estimator,"Abstract: This paper presents an -statistics quantile estimator for estimating the th quantile of a population which belongs to a set of location–scale distributions. The design of the weight vector of the estimator is formulated as a constrained optimization problem. The objective of the optimization problem is to minimize the mean square error. The optimization problem is subject to a unitary constraint on the weight vector of the -statistics quantile estimation. We solve the optimization problem and obtain an optimal solution, which is the weight vector of the proposed estimator. [Copyright &y& Elsevier] Copyright of Statistics & Probability Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77961656&site=ehost-live
204,Balancing Search and Estimation in Random Search Based Stochastic Simulation Optimization.,Jie Xu,IEEE Transactions on Automatic Control,189286,,Nov-16,61,11,3593,6,119139493,10.1109/TAC.2016.2522094,IEEE,Article,STOCHASTIC analysis; MATHEMATICAL optimization; SIMULATION methods & models; AUTOMATIC control systems; SEARCH algorithms,Algorithm design and analysis; Computational modeling; Estimation; Optimal number of replications; optimal sampling set size; Optimization; Partitioning algorithms; simulation optimization; Stochastic processes,"Stochastic simulation optimization involves two fundamental steps: 1) searching the solution space to generate candidate solutions for comparison and 2) estimating the performance of each candidate solution via multiple simulations and selecting a solution as the best solution found. Comparisons of solutions via simulation estimation are subject to error due to the stochastic noise in simulation output. While estimation errors can be reduced by increasing the number of simulation replications, it would in turn limit the number of candidate solutions that can be generated for comparison in a fixed computation budget. Under a random search framework, we derive an analytical formula to (approximately) optimally determine the number of candidate solutions generated in the search step and simulation replications in the estimation step to maximize the quality of the solution selected as the best by the random search algorithm. We then propose a practical method based on this formula and test the method on several common benchmark problems. Experiment results show that our method is quite effective and leads to significant improvement in the quality of the best solution found. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119139493&site=ehost-live
204,Balancing Search and Estimation in Random Search Based Stochastic Simulation Optimization.,Chun-Hung Chen,IEEE Transactions on Automatic Control,189286,,Nov-16,61,11,3593,6,119139493,10.1109/TAC.2016.2522094,IEEE,Article,STOCHASTIC analysis; MATHEMATICAL optimization; SIMULATION methods & models; AUTOMATIC control systems; SEARCH algorithms,Algorithm design and analysis; Computational modeling; Estimation; Optimal number of replications; optimal sampling set size; Optimization; Partitioning algorithms; simulation optimization; Stochastic processes,"Stochastic simulation optimization involves two fundamental steps: 1) searching the solution space to generate candidate solutions for comparison and 2) estimating the performance of each candidate solution via multiple simulations and selecting a solution as the best solution found. Comparisons of solutions via simulation estimation are subject to error due to the stochastic noise in simulation output. While estimation errors can be reduced by increasing the number of simulation replications, it would in turn limit the number of candidate solutions that can be generated for comparison in a fixed computation budget. Under a random search framework, we derive an analytical formula to (approximately) optimally determine the number of candidate solutions generated in the search step and simulation replications in the estimation step to maximize the quality of the solution selected as the best by the random search algorithm. We then propose a practical method based on this formula and test the method on several common benchmark problems. Experiment results show that our method is quite effective and leads to significant improvement in the quality of the best solution found. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119139493&site=ehost-live
205,Dynamic Sampling Allocation Under Finite Simulation Budget for Feasibility Determination.,Chun-Hung Chen,INFORMS Journal on Computing,10919856,,Winter2022,34,1,557,12,155827904,10.1287/ijoc.2020.1057,INFORMS: Institute for Operations Research,Article,"MONTE Carlo method; STOCHASTIC systems; FINITE, The; RESOURCE allocation; CALL centers; Telephone call centres; Telemarketing Bureaus and Other Contact Centers",asymptotic optimality; dynamic sampling; feasibility determination; finite simulation budget,"Monte Carlo simulation is a commonly used tool for evaluating the performance of complex stochastic systems. In practice, simulation can be expensive, especially when comparing a large number of alternatives, thus motivating the need to intelligently allocate simulation replications. Given a finite set of alternatives whose means are estimated via simulation, we consider the problem of determining the subset of alternatives that have means smaller than a fixed threshold. A dynamic sampling procedure that possesses not only asymptotic optimality, but also desirable finite-sample properties is proposed. Theoretical results show that there is a significant difference between finite-sample optimality and asymptotic optimality. Numerical experiments substantiate the effectiveness of the new method. Summary of Contribution: Simulation is an important tool to estimate the performance of complex stochastic systems. We consider a feasibility determination problem of identifying all those among a finite set of alternatives with mean smaller than a given threshold, in which the means are unknown but can be estimated by sampling replications via stochastic simulation. This problem appears widely in many applications, including call center design and hospital resource allocation. Our work considers how to intelligently allocate simulation replications to different alternatives for efficiently finding the feasible alternatives. Previous work focuses on the asymptotic properties of the sampling allocation procedures, whereas our contribution lies in developing a finite-budget allocation rule that possesses both asymptotic optimality and desirable finite-budget properties. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155827904&site=ehost-live
206,Efficient Learning for Selecting Important Nodes in Random Network.,Chun-Hung Chen,IEEE Transactions on Automatic Control,189286,,Mar-21,66,3,1321,8,148970687,10.1109/TAC.2020.2989753,IEEE,Article,MARKOV processes; TAYLOR'S series; STOCHASTIC processes; PROBABILITY theory; LEARNING,Adaptive systems; Backstepping; Bayesian learning; dynamic sampling; Lyapunov methods; Markov chain; network; Nonlinear systems; ranking and selection (R&S); Stability analysis; Stochastic processes; Uncertainty,"In this article, we consider the problem of selecting important nodes in a random network, where the nodes connect to each other randomly with certain transition probabilities. The node importance is characterized by the stationary probabilities of the corresponding nodes in a Markov chain defined over the network, as in Google's PageRank. Unlike a deterministic network, the transition probabilities in a random network are unknown but can be estimated by sampling. Under a Bayesian learning framework, we apply the first-order Taylor expansion and normal approximation to provide a computationally efficient posterior approximation of the stationary probabilities. In order to maximize the probability of correct selection, we propose a dynamic sampling procedure, which uses not only posterior means and variances of certain interaction parameters between different nodes, but also the sensitivities of the stationary probabilities with respect to each interaction parameter. Numerical experiment results demonstrate the superiority of the proposed sampling procedure. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148970687&site=ehost-live
207,Efficient Sampling Allocation Procedures for Optimal Quantile Selection.,Chun-Hung Chen,INFORMS Journal on Computing,10919856,,Winter2021,33,1,230,16,148514740,10.1287/ijoc.2019.0946,INFORMS: Institute for Operations Research,Article,SAMPLE size (Statistics); INFINITY (Mathematics),Bayesian framework; dynamic sampling allocation; quantile optimization; ranking and selection,"We propose a dynamic sampling allocation and selection paradigm for finding the alternative with the optimal quantile in a Bayesian framework. Myopic allocation policies (MAPs), analogous to existing methods in classic ranking and selection for selecting the alternative with the optimal mean, and computationally efficient selection policies are derived for selecting the alternative with the optimal quantile. Under certain conditions, we prove that the proposed MAPs and selection procedures are consistent, which means that the best quantile would be eventually correctly selected as the sample size goes to infinity. Numerical experiments demonstrate that the proposed schemes can significantly improve the performance. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148514740&site=ehost-live
208,Efficient simulation budget allocation with regression.,Chun-Hung Chen,IIE Transactions,0740817X,,Mar-13,45,3,291,18,84423218,10.1080/0740817X.2012.712238,Taylor & Francis Ltd,Article,SIMULATION methods & models; REGRESSION analysis; DECISION making; RANKING (Statistics); APPROXIMATION theory; EXPERIMENTAL design,optimal computing budget allocation; optimization using regression; Simulation optimization; stochastic optimization; stochastic simulation,"Simulation can be a very powerful tool to help decision making in many applications; however, exploring multiple courses of actions can be time consuming. Numerous Ranking & Selection (R&S) procedures have been developed to enhance the simulation efficiency of finding the best design. This article explores the potential of further enhancing R&S efficiency by incorporating simulation information from across the domain into a regression metamodel. This article assumes that the underlying function to be optimized is one-dimensional as well as approximately quadratic or piecewise quadratic. Under some common conditions in most regression-based approaches, the proposed method provides approximations of the optimal rules that determine the design locations to conduct simulation runs and the number of samples allocated to each design location. Numerical experiments demonstrate that the proposed approach can dramatically enhance efficiency over existing efficient R&S methods and can obtain significant savings over regression-based methods. In addition to utilizing concepts from the Design Of Experiments (DOE) literature, it introduces the probability of correct selection optimality criterion that underpins our new R&S method to the DOE literature. [ABSTRACT FROM PUBLISHER] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=84423218&site=ehost-live
209,Efficient Simulation Resource Sharing and Allocation for Selecting the Best.,Chun-Hung Chen,IEEE Transactions on Automatic Control,189286,,Apr-13,58,4,1017,7,86391443,10.1109/TAC.2012.2215533,IEEE,Article,RESOURCE allocation; RANDOM numbers; RESOURCE management; STATISTICAL correlation; APPROXIMATION algorithms; NICKEL; MATHEMATICAL models; COMPUTER simulation; Metal service centres,Algorithm design and analysis; Approximation algorithms; Approximation methods; Computational modeling; Computing budget sharing; Correlation; multiple-comparison procedures; Nickel; optimal sampling schemes; Resource management; simulation budget allocation,"Common random numbers and the standard clock method are examples of effective variance reduction techniques that also share information and simulation resources when generating realizations of different simulated systems whose performances are being compared. This sharing of computing resources and the potentially widely different computational requirements for different simulation models are important considerations in allocating simulation replications among the candidate designs with the objective of maximizing the probability of selecting the best design, and we formulate the optimal computing budget allocation problem under this scenario. The resulting formulation leads to an optimization problem that can be viewed as a generalization of a correlated version considered in earlier work. An approximation to the problem is introduced to allow a tractable solution, for which a heuristic two-stage sequential allocation algorithm is proposed, and several numerical examples are used to illustrate the potential improvements that can be gained. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=86391443&site=ehost-live
210,Efficient Simulation Sampling Allocation Using Multifidelity Models.,Jie Xu,IEEE Transactions on Automatic Control,189286,,Aug-19,64,8,3156,14,137857844,10.1109/TAC.2018.2886165,IEEE,Article,GAUSSIAN mixture models; INFORMATION modeling,Analytical models; Bayes methods; Clustering analysis; Computational modeling; Gaussian mixture model; multifidelity models; Optimization; ranking and selection; Resource management; sequential sampling; simulation optimization,"Simulation is often used to estimate the performance of alternative system designs for selecting the best. For a complex system, high-fidelity simulation is usually time-consuming and expensive. In this paper, we provide a new framework that integrates information from the multifidelity models to increase efficiency for selecting the best. A Gaussian mixture model is introduced to capture performance clustering information in the multifidelity models. Posterior information obtained by a clustering analysis incorporates both cluster-wise information and idiosyncratic information for each design. We propose a new budget allocation method to efficiently allocate high-fidelity simulation replications, utilizing posterior information. Numerical experiments show that the proposed multifidelity framework achieves a significant boost in efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137857844&site=ehost-live
210,Efficient Simulation Sampling Allocation Using Multifidelity Models.,Chun-Hung Chen,IEEE Transactions on Automatic Control,189286,,Aug-19,64,8,3156,14,137857844,10.1109/TAC.2018.2886165,IEEE,Article,GAUSSIAN mixture models; INFORMATION modeling,Analytical models; Bayes methods; Clustering analysis; Computational modeling; Gaussian mixture model; multifidelity models; Optimization; ranking and selection; Resource management; sequential sampling; simulation optimization,"Simulation is often used to estimate the performance of alternative system designs for selecting the best. For a complex system, high-fidelity simulation is usually time-consuming and expensive. In this paper, we provide a new framework that integrates information from the multifidelity models to increase efficiency for selecting the best. A Gaussian mixture model is introduced to capture performance clustering information in the multifidelity models. Posterior information obtained by a clustering analysis incorporates both cluster-wise information and idiosyncratic information for each design. We propose a new budget allocation method to efficiently allocate high-fidelity simulation replications, utilizing posterior information. Numerical experiments show that the proposed multifidelity framework achieves a significant boost in efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137857844&site=ehost-live
211,Equipment Utilization Enhancement in Photolithography Area Through a Dynamic System Control Using Multi-Fidelity Simulation Optimization With Big Data Technique.,Edward Huang,IEEE Transactions on Semiconductor Manufacturing,8946507,,May-17,30,2,166,10,122903896,10.1109/TSM.2017.2693259,IEEE,Article,EQUIPMENT utilization; PHOTOLITHOGRAPHY equipment; BIG data; LITHOGRAPHY; MATHEMATICAL optimization; Other printing,Big Data; Control systems; Decision making; Dispatching; Industries; Industry 4.0; Lithography; multi-fidelity simulation optimization; Optimization; Photolithographic process,"Photolithographic (Photo) plays a key role in semiconductor manufacturing because of its importance to advanced process shrinking. Even with a small improvement in its operational efficiency, the cost competitiveness in production can be enhanced as a result of the huge amount of share capital cost. However, it is difficult to stabilize the throughput rhythm of Fabs, while keeping a high equipment utilization for Photo. In the light of Industry 4.0 and big data, a huge potential of maintaining a desired system performance by (near) real-time dynamic system control is highly anticipated. But it also poses challenges to intelligently handling mass data acquisition and allocating computing resources. This research aims to maximize the equipment utilization in Photo by an efficient multi-model simulation optimization approach with big data techniques in the era of Industry 4.0. dynamic Photo configurator and abnormality detector are the two critical units in our proposed system framework; the former can make a quick decision to optimize the system configuration while receiving the adjustment request from the latter. The results from an empirical study show the practical viability of proposed approach that the capacity loss in Photo has been effectively improved. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122903896&site=ehost-live
211,Equipment Utilization Enhancement in Photolithography Area Through a Dynamic System Control Using Multi-Fidelity Simulation Optimization With Big Data Technique.,Chun-Hung Chen,IEEE Transactions on Semiconductor Manufacturing,8946507,,May-17,30,2,166,10,122903896,10.1109/TSM.2017.2693259,IEEE,Article,EQUIPMENT utilization; PHOTOLITHOGRAPHY equipment; BIG data; LITHOGRAPHY; MATHEMATICAL optimization; Other printing,Big Data; Control systems; Decision making; Dispatching; Industries; Industry 4.0; Lithography; multi-fidelity simulation optimization; Optimization; Photolithographic process,"Photolithographic (Photo) plays a key role in semiconductor manufacturing because of its importance to advanced process shrinking. Even with a small improvement in its operational efficiency, the cost competitiveness in production can be enhanced as a result of the huge amount of share capital cost. However, it is difficult to stabilize the throughput rhythm of Fabs, while keeping a high equipment utilization for Photo. In the light of Industry 4.0 and big data, a huge potential of maintaining a desired system performance by (near) real-time dynamic system control is highly anticipated. But it also poses challenges to intelligently handling mass data acquisition and allocating computing resources. This research aims to maximize the equipment utilization in Photo by an efficient multi-model simulation optimization approach with big data techniques in the era of Industry 4.0. dynamic Photo configurator and abnormality detector are the two critical units in our proposed system framework; the former can make a quick decision to optimize the system configuration while receiving the adjustment request from the latter. The results from an empirical study show the practical viability of proposed approach that the capacity loss in Photo has been effectively improved. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122903896&site=ehost-live
212,On unbiased optimal -statistics quantile estimators,Chun-Hung Chen,Statistics & Probability Letters,1677152,,Nov-12,82,11,1891,7,79110145,10.1016/j.spl.2012.05.027,Elsevier B.V.,Article,OPTIMAL designs (Statistics); QUANTILES; ESTIMATION theory; LINEAR statistical models; POPULATION; ANALYSIS of variance,-statistics; Best linear unbiased estimator; Location-scale distributions; Unbiased quantile estimator,"Abstract: Recently, have presented two biased Optimal -statistics Quantile Estimators (OLQEs). In this work, we present two unbiased versions of the two biased OLQEs. Similar to the biased OLQEs, the proposed unbiased OLQEs are able to accommodate a set of scaled populations and a set of location-scale populations, respectively. Furthermore, we compare the proposed unbiased OLQEs with two state-of-the-art efficient unbiased estimators, called Best Linear Unbiased Estimators (BLUEs). Although OLQEs and BLUEs have different aims and models, we point out that the two proposed unbiased OLQEs are closely related to the two BLUEs, respectively. The differences between the unbiased OLQEs and the BLUEs are also provided. We conduct an experimental study to demonstrate that, for a set of location-scale populations and extreme quantiles, if the main concern is large biases, then a proposed unbiased location equivariance OLQE is more appealing. [Copyright &y& Elsevier] Copyright of Statistics & Probability Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=79110145&site=ehost-live
213,Optimal budget allocation for discrete-event simulation experiments.,Chun-Hung Chen,IIE Transactions,0740817X,,Jan-10,42,1,60,11,49144617,10.1080/07408170903116360,Taylor & Francis Ltd,Article,"SIMULATION methods & models; OPERATIONS research; NUMERICAL analysis; HEURISTIC algorithms; MATHEMATICAL analysis; Process, Physical Distribution, and Logistics Consulting Services",Discrete-event simulation; simulation optimization; simulation uncertainty,"Simulation plays a vital role in analyzing discrete-event systems, particularly in comparing alternative system designs with a view to optimizing system performance. Using simulation to analyze complex systems, however, can be both prohibitively expensive and time-consuming. Effective algorithms to allocate intelligently a computing budget for discrete-event simulation experiments are presented in this paper. These algorithms dynamically determine the simulation lengths for all simulation experiments and thus significantly improve simulation efficiency under the constraint of a given computing budget. Numerical illustrations are provided and the algorithms are compared with traditional two-stage ranking-and-selection procedures through numerical experiments. Although the proposed approach is based on heuristics, the numerical results indicate that it is much more efficient than the compared procedures. [ABSTRACT FROM AUTHOR] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=49144617&site=ehost-live
214,Optimal Budget Allocation Rule for Simulation Optimization Using Quadratic Regression in Partitioned Domains.,Chun-Hung Chen,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Jul-15,45,7,1047,16,103264621,10.1109/TSMC.2014.2383997,IEEE,Article,LARGE deviation theory; PROBABILITY theory; QUADRATIC programming; GEOMETRIC function theory; INDUSTRIAL electronics; CYBERNETICS; MATHEMATICAL models; Relay and Industrial Control Manufacturing,Budget allocation; Convergence; large deviation theory; Mathematical model; Modeling; Noise; Optimization; quadratic regression; ranking and selection; Resource management; simulation; Vectors,"Ranking and selection procedures have been successfully applied to enhance the efficiency of simulation in recent years. To further improve the efficiency, one approach is to incorporate the simulation output from across the domain into some response surfaces. In this paper, the domain of interest is divided into adjacent partitions and a quadratic regression function is assumed for the mean of the underlying function in each partition. Using the large deviation theory, an asymptotically optimal allocation rule is proposed with the objective of maximizing the probability of correctly selecting the best design point. The proposed simulation budget allocation rule is implemented in a heuristic sequential allocation algorithm and compared with some existing allocation rules. Numerical results illustrate the effectiveness of the proposed simulation budget allocation rule. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103264621&site=ehost-live
215,Optimal computing budget allocation for Monte Carlo simulation with application to product design,Chun-Hung Chen,Simulation Modelling Practice & Theory,1569190X,,Mar-03,11,1,57,18,9230467,10.1016/S1569-190X(02)00095-3,Elsevier B.V.,Article,MONTE Carlo method; MATHEMATICAL optimization; PRODUCT design; Industrial Design Services,Computing budget allocation; Intelligent simulation; Manufacturing design; Monte Carlo simulation; Yield analysis,"Ordinal optimization has emerged as an efficient technique for simulation and optimization, converging exponentially in many cases. In this paper, we present a new computing budget allocation approach that further enhances the efficiency of ordinal optimization. Our approach intelligently determines the best allocation of simulation trials or samples necessary to maximize the probability of identifying the optimal ordinal solution. We illustrate the approach’s benefits and ease of use by applying it to two electronic circuit design problems. Numerical results indicate the approach yields significant savings in computation time above and beyond the use of ordinal optimization. [Copyright &y& Elsevier] Copyright of Simulation Modelling Practice & Theory is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=9230467&site=ehost-live
216,Optimal Computing Budget Allocation for Stochastic N–$k$ Problem in the Power Grid System.,Chun-Hung Chen,IEEE Transactions on Reliability,189529,,Sep-19,68,3,778,12,138433577,10.1109/TR.2019.2913741,IEEE,Article,GRIDS (Cartography); SYSTEM failures; STOCHASTIC processes; ELECTRIC lines; ASSIGNMENT problems (Programming); BUDGET; MONTE Carlo method; Public Finance Activities; Power and Communication Line and Related Structures Construction; Site Preparation Contractors,Computational modeling; Numerical models; Optimal computing budget allocation (OCBA); Power grids; Reliability; Resource management; stochastic $N$ − 1; Stochastic processes,"The $N$ − $k$ problem is very well known in the power industry and it tries to answer the question whether there exists a set of $k$ lines in a power network with $N$ elements whose removal would cause the failure of the system. In practice, it is common to evaluate a system according to an $N$ −1 criterion, i.e., $k = 1$. While this problem has traditionally been considered in a deterministic setting, stochastic behavior within the system is important especially in the context of extreme events. A number of stochastic Monte Carlo models have been proposed to estimate the probability of cascading failures. In this paper, we deal with simulation budget allocation of the stochastic $N$ –1 problem. More specifically, we assume that a simulation model is able to provide us an estimate of the system failure rate when any line is tripped. It is not difficult to see how simulation of all configurations to some certain accuracy can become computationally expensive with the growth of $N$. Under such a setting, we transform the $N$ –1 problem into a stochastic selection process with optimal computing budget allocation (OCBA): given $N$ configurations, we would like to sequentially allocate a certain number of simulation replications in order to answer the question whether the system is reliable or not. We show through theoretical analysis and numerical experiments that the probability of correctly identifying the system reliability state can be increased by applying OCBA allocation rules in the simulation budget allocation process. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Reliability is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138433577&site=ehost-live
216,Optimal Computing Budget Allocation for Stochastic N–$k$ Problem in the Power Grid System.,John Shortle,IEEE Transactions on Reliability,189529,,Sep-19,68,3,778,12,138433577,10.1109/TR.2019.2913741,IEEE,Article,GRIDS (Cartography); SYSTEM failures; STOCHASTIC processes; ELECTRIC lines; ASSIGNMENT problems (Programming); BUDGET; MONTE Carlo method; Public Finance Activities; Power and Communication Line and Related Structures Construction; Site Preparation Contractors,Computational modeling; Numerical models; Optimal computing budget allocation (OCBA); Power grids; Reliability; Resource management; stochastic $N$ − 1; Stochastic processes,"The $N$ − $k$ problem is very well known in the power industry and it tries to answer the question whether there exists a set of $k$ lines in a power network with $N$ elements whose removal would cause the failure of the system. In practice, it is common to evaluate a system according to an $N$ −1 criterion, i.e., $k = 1$. While this problem has traditionally been considered in a deterministic setting, stochastic behavior within the system is important especially in the context of extreme events. A number of stochastic Monte Carlo models have been proposed to estimate the probability of cascading failures. In this paper, we deal with simulation budget allocation of the stochastic $N$ –1 problem. More specifically, we assume that a simulation model is able to provide us an estimate of the system failure rate when any line is tripped. It is not difficult to see how simulation of all configurations to some certain accuracy can become computationally expensive with the growth of $N$. Under such a setting, we transform the $N$ –1 problem into a stochastic selection process with optimal computing budget allocation (OCBA): given $N$ configurations, we would like to sequentially allocate a certain number of simulation replications in order to answer the question whether the system is reliable or not. We show through theoretical analysis and numerical experiments that the probability of correctly identifying the system reliability state can be increased by applying OCBA allocation rules in the simulation budget allocation process. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Reliability is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138433577&site=ehost-live
217,Ranking and selection for terminating simulation under sequential sampling.,Chun-Hung Chen,IISE Transactions,24725854,,Jul-21,53,7,735,16,149844005,10.1080/24725854.2020.1785647,Taylor & Francis Ltd,Article,ASYMPTOTIC efficiencies; HEURISTIC,design of experiments; optimal computing budget allocation; ranking and selection; sequential sampling; Simulation,"This research develops an efficient ranking and selection procedure to select the best design for terminating simulation under sequential sampling. This approach enables us to obtain an accurate estimate of the mean performance at a particular point using regression in the case of a terminating simulation. The sequential sampling constraint is imposed to fully utilize the information along the simulation replication. The asymptotically optimal simulation budget allocation among all designs is derived concurrently with the optimal simulation run length and optimal number of simulation groups for each design. To implement the simulation budget allocation rule with a fixed finite simulation budget, a heuristic sequential simulation procedure is suggested with the objective of maximizing the probability of correct selection. Numerical experiments confirm the efficiency of the procedure relative to extant approaches. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149844005&site=ehost-live
218,Robust Sampling Budget Allocation Under Deep Uncertainty.,Jie Xu,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Oct-22,52,10,6339,9,159210427,10.1109/TSMC.2022.3144363,IEEE,Article,BUDGET; TECHNOLOGICAL innovations; UNCERTAINTY (Information theory); ENGINEERING design; Public Finance Activities; Engineering Services,Complex system design; Computational modeling; Linear programming; minimax regret; Optimization; Resource management; response surface methodologies; Response surface methodology; sampling budget allocation; Standards; Uncertainty,"A novel methodology is introduced for optimally allocating a sampling budget. Sampling budget allocation problems arise frequently in various settings. For example, in the design of complex engineering systems, given both the complexity of these systems and the imperfect information on new technologies, designers often face deep uncertainty as to system performance. Consequently, designers need to sample multiple alternative designs under a limited budget. This article proposes a minimax regret approach to allocate the sampling budget in the presence of deep uncertainty pertaining to system performance. The objective is to maximize the probability of selecting the design with the minimum–maximum regret under a limited sampling budget and imperfect information. To effectively solve the minimax regret problem, an approximation methodology that provides good solutions with quantifiable uncertainty is developed. The essence of the methodology, which has the added benefit of being generally applicable to any multilevel optimization, is that all but the first level of multilevel optimization can be eliminated via a response surface. By sampling many values of a higher level decision’s variables, solving the next lower level optimization given those samples values, and calibrating a response surface to the objective function value eliminate one required optimization. Doing this repeatedly reduces the complexity of the multilevel optimization to a standard optimization. Regardless of the number of levels in the optimization, repeating this process ultimately leaves one with a single optimization whose objective function can be directly computed, given the highest level variables. Numerical experiments with two sampling allocation examples demonstrate both the benefit of the robust sampling budget allocation versus nonrobust formulations and the effectiveness of the proposed solution approach. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159210427&site=ehost-live
218,Robust Sampling Budget Allocation Under Deep Uncertainty.,Edward Huang,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Oct-22,52,10,6339,9,159210427,10.1109/TSMC.2022.3144363,IEEE,Article,BUDGET; TECHNOLOGICAL innovations; UNCERTAINTY (Information theory); ENGINEERING design; Public Finance Activities; Engineering Services,Complex system design; Computational modeling; Linear programming; minimax regret; Optimization; Resource management; response surface methodologies; Response surface methodology; sampling budget allocation; Standards; Uncertainty,"A novel methodology is introduced for optimally allocating a sampling budget. Sampling budget allocation problems arise frequently in various settings. For example, in the design of complex engineering systems, given both the complexity of these systems and the imperfect information on new technologies, designers often face deep uncertainty as to system performance. Consequently, designers need to sample multiple alternative designs under a limited budget. This article proposes a minimax regret approach to allocate the sampling budget in the presence of deep uncertainty pertaining to system performance. The objective is to maximize the probability of selecting the design with the minimum–maximum regret under a limited sampling budget and imperfect information. To effectively solve the minimax regret problem, an approximation methodology that provides good solutions with quantifiable uncertainty is developed. The essence of the methodology, which has the added benefit of being generally applicable to any multilevel optimization, is that all but the first level of multilevel optimization can be eliminated via a response surface. By sampling many values of a higher level decision’s variables, solving the next lower level optimization given those samples values, and calibrating a response surface to the objective function value eliminate one required optimization. Doing this repeatedly reduces the complexity of the multilevel optimization to a standard optimization. Regardless of the number of levels in the optimization, repeating this process ultimately leaves one with a single optimization whose objective function can be directly computed, given the highest level variables. Numerical experiments with two sampling allocation examples demonstrate both the benefit of the robust sampling budget allocation versus nonrobust formulations and the effectiveness of the proposed solution approach. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159210427&site=ehost-live
218,Robust Sampling Budget Allocation Under Deep Uncertainty.,Chun-Hung Chen,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Oct-22,52,10,6339,9,159210427,10.1109/TSMC.2022.3144363,IEEE,Article,BUDGET; TECHNOLOGICAL innovations; UNCERTAINTY (Information theory); ENGINEERING design; Public Finance Activities; Engineering Services,Complex system design; Computational modeling; Linear programming; minimax regret; Optimization; Resource management; response surface methodologies; Response surface methodology; sampling budget allocation; Standards; Uncertainty,"A novel methodology is introduced for optimally allocating a sampling budget. Sampling budget allocation problems arise frequently in various settings. For example, in the design of complex engineering systems, given both the complexity of these systems and the imperfect information on new technologies, designers often face deep uncertainty as to system performance. Consequently, designers need to sample multiple alternative designs under a limited budget. This article proposes a minimax regret approach to allocate the sampling budget in the presence of deep uncertainty pertaining to system performance. The objective is to maximize the probability of selecting the design with the minimum–maximum regret under a limited sampling budget and imperfect information. To effectively solve the minimax regret problem, an approximation methodology that provides good solutions with quantifiable uncertainty is developed. The essence of the methodology, which has the added benefit of being generally applicable to any multilevel optimization, is that all but the first level of multilevel optimization can be eliminated via a response surface. By sampling many values of a higher level decision’s variables, solving the next lower level optimization given those samples values, and calibrating a response surface to the objective function value eliminate one required optimization. Doing this repeatedly reduces the complexity of the multilevel optimization to a standard optimization. Regardless of the number of levels in the optimization, repeating this process ultimately leaves one with a single optimization whose objective function can be directly computed, given the highest level variables. Numerical experiments with two sampling allocation examples demonstrate both the benefit of the robust sampling budget allocation versus nonrobust formulations and the effectiveness of the proposed solution approach. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159210427&site=ehost-live
219,A Coordinate Optimization Approach for Concurrent Design.,Edward Huang,IEEE Transactions on Automatic Control,189286,,Jul-19,64,7,2913,8,137234542,10.1109/TAC.2018.2872196,IEEE,Article,COORDINATES; CHARGE coupled devices; NEW product development; INFORMATION design; INFORMATION sharing; Marketing Consulting Services,Charge coupled devices; Concurrent design (CCD); Convergence; Elevators; Information management; Linear programming; nonlinear programing; Optimization; Task analysis,"In concurrent design (CCD), multiple design teams execute their tasks simultaneously and then exchange information to update their designs. The process then iterates until the termination criterion is met. When properly controlled and executed, CCD can be an effective method to shorten the time in product development for complex and large-scale projects thanks to its parallel nature. In this note, we propose a coordinate optimization framework to model and control team coordination through information sharing in CCD. It can be shown that under a certain convexity assumption, CCD converges to a globally optimal design if the information sharing intensity is smaller than a certain threshold. Numerical experiments substantiate the theoretical results. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137234542&site=ehost-live
219,A Coordinate Optimization Approach for Concurrent Design.,Jie Xu,IEEE Transactions on Automatic Control,189286,,Jul-19,64,7,2913,8,137234542,10.1109/TAC.2018.2872196,IEEE,Article,COORDINATES; CHARGE coupled devices; NEW product development; INFORMATION design; INFORMATION sharing; Marketing Consulting Services,Charge coupled devices; Concurrent design (CCD); Convergence; Elevators; Information management; Linear programming; nonlinear programing; Optimization; Task analysis,"In concurrent design (CCD), multiple design teams execute their tasks simultaneously and then exchange information to update their designs. The process then iterates until the termination criterion is met. When properly controlled and executed, CCD can be an effective method to shorten the time in product development for complex and large-scale projects thanks to its parallel nature. In this note, we propose a coordinate optimization framework to model and control team coordination through information sharing in CCD. It can be shown that under a certain convexity assumption, CCD converges to a globally optimal design if the information sharing intensity is smaller than a certain threshold. Numerical experiments substantiate the theoretical results. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137234542&site=ehost-live
219,A Coordinate Optimization Approach for Concurrent Design.,Chun-Hung Chen,IEEE Transactions on Automatic Control,189286,,Jul-19,64,7,2913,8,137234542,10.1109/TAC.2018.2872196,IEEE,Article,COORDINATES; CHARGE coupled devices; NEW product development; INFORMATION design; INFORMATION sharing; Marketing Consulting Services,Charge coupled devices; Concurrent design (CCD); Convergence; Elevators; Information management; Linear programming; nonlinear programing; Optimization; Task analysis,"In concurrent design (CCD), multiple design teams execute their tasks simultaneously and then exchange information to update their designs. The process then iterates until the termination criterion is met. When properly controlled and executed, CCD can be an effective method to shorten the time in product development for complex and large-scale projects thanks to its parallel nature. In this note, we propose a coordinate optimization framework to model and control team coordination through information sharing in CCD. It can be shown that under a certain convexity assumption, CCD converges to a globally optimal design if the information sharing intensity is smaller than a certain threshold. Numerical experiments substantiate the theoretical results. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137234542&site=ehost-live
220,Advances in simulation optimization and its applications.,Chun-Hung Chen,IIE Transactions,0740817X,,Jul-13,45,7,683,2,86887348,10.1080/0740817X.2013.778709,Taylor & Francis Ltd,Article,SIMULATION methods & models; MATHEMATICAL optimization; MATHEMATICAL variables; PARALLEL algorithms; MATHEMATICAL bounds; PROBLEM solving,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=86887348&site=ehost-live
221,Efficient Splitting Simulation for Blackout Analysis.,Chun-Hung Chen,IEEE Transactions on Power Systems,8858950,,Jul-15,30,4,1775,9,103304575,10.1109/TPWRS.2014.2359920,IEEE,Article,ELECTRIC lines; ELECTRIC power failures; SIMULATION methods & models; ELECTRIC power distribution grids; ELECTRIC networks; Power and Communication Line and Related Structures Construction; Site Preparation Contractors,Analytical models; Blackout; cascading failure; Computational modeling; Numerical models; Power grids; Power system faults; Power system protection; rare-event probability; Resource management; stochastic simulation,"The analysis of severe blackouts has become an essential part of transmission grid planning and operation. This may include evaluation of rare-event probabilities, which can be difficult to estimate. While simulation offers flexibility to model large complex systems, efficiency remains a big concern when estimating very small probabilities. This paper presents an effective simulation technique to evaluate rare-event probabilities associated with cascading blackouts in an electric grid. We test our technique on an IEEE 118-bus electric network and show that it can dramatically improve simulation efficiency. We also demonstrate that the proposed technique can effectively locate vulnerable links. These are links whose failures lead to the highest probabilities of a blackout event. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Power Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103304575&site=ehost-live
221,Efficient Splitting Simulation for Blackout Analysis.,John Shortle,IEEE Transactions on Power Systems,8858950,,Jul-15,30,4,1775,9,103304575,10.1109/TPWRS.2014.2359920,IEEE,Article,ELECTRIC lines; ELECTRIC power failures; SIMULATION methods & models; ELECTRIC power distribution grids; ELECTRIC networks; Power and Communication Line and Related Structures Construction; Site Preparation Contractors,Analytical models; Blackout; cascading failure; Computational modeling; Numerical models; Power grids; Power system faults; Power system protection; rare-event probability; Resource management; stochastic simulation,"The analysis of severe blackouts has become an essential part of transmission grid planning and operation. This may include evaluation of rare-event probabilities, which can be difficult to estimate. While simulation offers flexibility to model large complex systems, efficiency remains a big concern when estimating very small probabilities. This paper presents an effective simulation technique to evaluate rare-event probabilities associated with cascading blackouts in an electric grid. We test our technique on an IEEE 118-bus electric network and show that it can dramatically improve simulation efficiency. We also demonstrate that the proposed technique can effectively locate vulnerable links. These are links whose failures lead to the highest probabilities of a blackout event. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Power Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103304575&site=ehost-live
222,GO-POLARS: A Steerable Stochastic Search on the Strength of Hyperspherical Coordinates.,Chun-Hung Chen,IEEE Transactions on Automatic Control,189286,,Dec-17,62,12,6458,8,126586035,10.1109/TAC.2017.2657330,IEEE,Article,STOCHASTIC analysis; SEARCH algorithms; HYPERSPHERICAL method; FINITE element method; SIMULATED annealing,Algorithm design and analysis; Approximation algorithms; Convergence; GO-POLARS; gradient; hyperspherical coordinate; Jacobian matrices; Partitioning algorithms; polar coordinate; Probability density function; random search; Stochastic processes,"Search algorithms for optimizing a complex problem are mainly categorized as gradient-driven and stochastic search, each with its advantages and shortcomings. A newly developed algorithm, GO-POLARS, is proposed with a hyperspherical coordinate framework, which could perturb a given direction with well-controlled variation. It designs a steerable stochastic search algorithm that explores toward a promising direction, such as the gradient, at any desired levels. In this note, we provide an analytical study on the hyperspherical coordinates and the corresponding random distributions and, thus, prove the local convergence property of the GO-POLARS. Extensive numerical experiments are illustrated to show its advantages compared to conventional search algorithms. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126586035&site=ehost-live
223,Improving Analytic Hierarchy Process Expert Allocation Using Optimal Computing Budget Allocation.,Edward Huang,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Aug-16,46,8,1140,8,116872521,10.1109/TSMC.2015.2478754,IEEE,Article,ANALYTIC hierarchy process; DECISION making; NUMERICAL analysis,Aircraft; Analytic hierarchy process; Analytic hierarchy process (AHP); Analytical models; Convergence; multicriteria decision making; optimal computing budget allocation (OCBA); Reliability; Resource management,"The analytic hierarchy process (AHP) has been widely applied to multicriteria decision making problems. The AHP aids decision makers to determine the priorities of multiple criteria, and make reasonable decisions. In the AHP, evaluating candidate alternatives requires multiple experts’ evaluations to avoid personal subjectivity. Although having more experts can improve selection quality, inviting more experts also results in higher recruitment cost and longer evaluation time. In this paper, we introduce the idea of optimal computing budget allocation (OCBA) and propose a method, named the AHP_OCBA method, to improve the efficiency of expert allocation. The proposed method optimizes the allocation of experts to maximize the probability of correctly selecting the best alternative in the AHP. This method also can minimize the required number of experts to meet the probability of correct selection. An illustrative example is provided to indicate the implementation of the AHP_OCBA method. We show the improvement of the proposed method compared to proportional and equal allocation rules in the numerical result. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116872521&site=ehost-live
223,Improving Analytic Hierarchy Process Expert Allocation Using Optimal Computing Budget Allocation.,Chun-Hung Chen,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Aug-16,46,8,1140,8,116872521,10.1109/TSMC.2015.2478754,IEEE,Article,ANALYTIC hierarchy process; DECISION making; NUMERICAL analysis,Aircraft; Analytic hierarchy process; Analytic hierarchy process (AHP); Analytical models; Convergence; multicriteria decision making; optimal computing budget allocation (OCBA); Reliability; Resource management,"The analytic hierarchy process (AHP) has been widely applied to multicriteria decision making problems. The AHP aids decision makers to determine the priorities of multiple criteria, and make reasonable decisions. In the AHP, evaluating candidate alternatives requires multiple experts’ evaluations to avoid personal subjectivity. Although having more experts can improve selection quality, inviting more experts also results in higher recruitment cost and longer evaluation time. In this paper, we introduce the idea of optimal computing budget allocation (OCBA) and propose a method, named the AHP_OCBA method, to improve the efficiency of expert allocation. The proposed method optimizes the allocation of experts to maximize the probability of correctly selecting the best alternative in the AHP. This method also can minimize the required number of experts to meet the probability of correct selection. An illustrative example is provided to indicate the implementation of the AHP_OCBA method. We show the improvement of the proposed method compared to proportional and equal allocation rules in the numerical result. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116872521&site=ehost-live
224,Optimal Computing Budget Allocation for regression with gradient information.,Jie Xu,Automatica,51098,,Dec-21,134,,N.PAG,1,153175918,10.1016/j.automatica.2021.109927,Elsevier B.V.,Article,STOCHASTIC systems; REGRESSION analysis,Computing budget allocation; Gradient information; Quadratic model; Simulation optimization; Stochastic systems,"We consider the problem of optimizing the performance of a stochastic system, e.g., a discrete-event system, where the system performance is evaluated using stochastic simulations. Our objective is to allocate simulation budget to maximize the probability of correct selection (PCS) of the best design, where both system performance and gradient information can be obtained simultaneously via simulation. The objective function is assumed to be quadratic, or can be approximated by a quadratic regression model. The main contribution of our work is to utilize gradient information to enhance the efficiency of traditional Optimal Computing Budget Allocation (OCBA). We develop near-optimal rules that determine design points where simulations should be run and the number of runs allocated to each point. Our numerical experiments demonstrate that the proposed approach performs much better than other existing ranking and selection methods, even in cases where derivative information is very noisy and its simulation cost is high. [ABSTRACT FROM AUTHOR] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153175918&site=ehost-live
224,Optimal Computing Budget Allocation for regression with gradient information.,Chun-Hung Chen,Automatica,51098,,Dec-21,134,,N.PAG,1,153175918,10.1016/j.automatica.2021.109927,Elsevier B.V.,Article,STOCHASTIC systems; REGRESSION analysis,Computing budget allocation; Gradient information; Quadratic model; Simulation optimization; Stochastic systems,"We consider the problem of optimizing the performance of a stochastic system, e.g., a discrete-event system, where the system performance is evaluated using stochastic simulations. Our objective is to allocate simulation budget to maximize the probability of correct selection (PCS) of the best design, where both system performance and gradient information can be obtained simultaneously via simulation. The objective function is assumed to be quadratic, or can be approximated by a quadratic regression model. The main contribution of our work is to utilize gradient information to enhance the efficiency of traditional Optimal Computing Budget Allocation (OCBA). We develop near-optimal rules that determine design points where simulations should be run and the number of runs allocated to each point. Our numerical experiments demonstrate that the proposed approach performs much better than other existing ranking and selection methods, even in cases where derivative information is very noisy and its simulation cost is high. [ABSTRACT FROM AUTHOR] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153175918&site=ehost-live
225,2003 IEEE Virtual Reality Conference Guest Editors' Introduction.,Jim Chen,PRESENCE: Teleoperators & Virtual Environments,10547460,,Apr-04,13,2,3,2,13737576,10.1162/1054746041382384,MIT Press,Article,VIRTUAL reality; TELECOMMUNICATION; Other telecommunications; Communication Equipment Repair and Maintenance; All Other Telecommunications; Telecommunications Resellers,,Introduces a series of articles about virtual reality application in telecommunication.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=13737576&site=ehost-live
226,A Model for Understanding How Virtual Reality Aids Complex Conceptual Learning.,Jim Chen,PRESENCE: Teleoperators & Virtual Environments,10547460,,Jun-99,8,3,293,24,2025604,10.1162/105474699566242,MIT Press,Article,VIRTUAL reality; LEARNING; TECHNOLOGICAL innovations,,"Designers and evaluators of immersive virtual reality systems have many ideas concerning how virtual reality can facilitate learning. However, we have little information concerning which of virtual reality's features provide the most leverage for enhancing understanding or how to customize those affordances for different learning environments. In part, this reflects the truly complex nature of learning. Features of a learning environment do not act in isolation; other factors such as the concepts or skills to be learned, individual characteristics, the learning experience, and the interaction experience all play a role in shaping the learning process and its outcomes. Through Project ScienceSpace, we have been trying to identify, use, and evaluate immersive virtual reality's affordances as a means to facilitate the mastery of complex, abstract concepts. In doing so, we are beginning to understand the interplay between virtual reality's features and other important factors in shaping the learning process and learning outcomes for this type of material. In this paper, we present a general model that describes how we think these factors work together and discuss some of the lessons we are learning about virtual reality's affordances in the context of this model for complex conceptual learning. [ABSTRACT FROM AUTHOR] Copyright of PRESENCE: Teleoperators & Virtual Environments is the property of MIT Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=2025604&site=ehost-live
227,Accurate Depth of Field Simulation in Real Time.,Jim Chen,Computer Graphics Forum,1677055,,Mar-07,26,1,15,9,24458379,10.1111/j.1467-8659.2007.00935.x,Wiley-Blackwell,Article,DEPTH of field (Photography); VIRTUAL reality; COMPUTER graphics; IMAGE processing; APPLICATION program interfaces; PIXELS; Photofinishing Laboratories (except One-Hour); One-Hour Photofinishing,depth buffer; depth of field; GPU; I.3.3 Computer Graphics: Picture/Image Generation: Display algorithms; I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism: Virtual reality; I.4.3 Image Processing and Computer Vision: Enhancement: Filtering; leakage; post-processing,"We present a new post processing method of simulating depth of field based on accurate calculations of circles of confusion. Compared to previous work, our method derives actual scene depth information directly from the existing depth buffer, requires no specialized rendering passes, and allows easy integration into existing rendering applications. Our implementation uses an adaptive, two-pass filter, producing a high quality depth of field effect that can be executed entirely on the GPU, taking advantage of the parallelism of modern graphics cards and permitting real time performance when applied to large numbers of pixels. [ABSTRACT FROM AUTHOR] Copyright of Computer Graphics Forum is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24458379&site=ehost-live
228,Binary inheritance learning particle swarm optimisation and its application in thinned antenna array synthesis with the minimum sidelobe level.,Jim Chen,"IET Microwaves, Antennas & Propagation (Wiley-Blackwell)",17518725,,2015,9,13,1386,6,110328148,10.1049/iet-map.2015.0071,Wiley-Blackwell,Article,PARTICLE swarm optimization; ALGORITHM research; EQUATIONS; ANTENNA arrays; COMPUTER algorithms,,"Although particle swarm optimisation (PSO) algorithm is an effective tool to solve the real-number optimisation problem, it cannot be directly adopted to the discrete optimisation. Many discrete PSO versions focused on the different discrete strategies on particle position and speed update equation, but these attempts decreased the performance of the PSO on discrete problems (compared with other intelligent optimisation algorithms). In this study, binary inheritance learning PSO (BILPSO) is proposed and is used to solve thinned antenna array synthesis problems, such as the pattern synthesis of 100-element symmetrical thinned linear array and 20 x 10 symmetrical thinned planar array. The optimisation results show the BILPSO is superior to other discrete PSO versions and other discrete intelligent optimisation algorithms. [ABSTRACT FROM AUTHOR] Copyright of IET Microwaves, Antennas & Propagation (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=110328148&site=ehost-live
229,Learning abstract concepts through interactive playing,Jim Chen,Computers & Graphics,978493,,Feb-06,30,1,10,10,19356557,10.1016/j.cag.2005.10.023,Elsevier B.V.,Article,VISUAL programming languages (Computer science); ARTIFICIAL intelligence; NEURAL computers; SELF-organizing systems,Computational steering; Distributed interactive simulation (DIS); Visualization,"Abstract: In this paper, we present strategies and approaches used to implement a synthetic learning environment which includes distributed interactive simulation, computational steering, interactive visualization, and artificial intelligence for learning abstract scientific concepts and entertainment. We use Navier–Stokes equations as a case study to show our ideas and methods. Students are allowed to interact with one another and engage in realistic collaborative exercises. The resulting technology is a testbed that reflects the benefits of a schoolhouse for training and education, especially for scientific abstract concepts, yet does not have the physical limitation that instructors, students and resources be collocated both spatially and temporally. Our innovative use of computational steering and interactive visualization allows students to visualize and manipulate the physical process, and understand the abstract concept through entertainment. [Copyright &y& Elsevier] Copyright of Computers & Graphics is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=19356557&site=ehost-live
230,Low-rank representation with graph regularization for subspace clustering.,Jim Chen,"Soft Computing - A Fusion of Foundations, Methodologies & Applications",14327643,,Mar-17,21,6,1569,13,121658578,10.1007/s00500-015-1869-0,Springer Nature,Article,SUBSPACES (Mathematics); MATHEMATICAL regularization; ROBUST control; LAPLACIAN matrices; ALGORITHMS,Graph regularization; Low-dimensional subspace; Low-rank representation; Matrix completion; Matrix recovery; Subspace clustering,"In this paper, we propose a low-rank representation method that incorporates graph regularization for robust subspace clustering. We make the assumption that high-dimensional data can be approximated as the union of low-dimensional subspaces of unknown dimension. The proposed method extends the low-rank representation algorithm by incorporating graph regularization with a discriminative dictionary. Existing low-rank representation methods for subspace clustering use noisy data as the dictionary. The proposed technique, however, takes advantage of the discriminative dictionary to seek the lowest-rank representation by virtue of matrix recovery and completion techniques. Moreover, the discriminative dictionary is further used to construct a graph Laplacian to separate the low-rank representation of high-dimensional data. The proposed algorithm can recover the low-dimensional subspace structure from high-dimensional observations (which are often corrupted by gross errors). Simultaneously, the samples are clustered into their corresponding underlying subspaces. Extensive experimental results on benchmark databases demonstrate the efficiency and effectiveness of the proposed algorithm for subspace clustering. [ABSTRACT FROM AUTHOR] Copyright of Soft Computing - A Fusion of Foundations, Methodologies & Applications is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121658578&site=ehost-live
231,Nonlinear Perspective Projections and Magic Lenses: 3D View Deformation.,Jim Chen,IEEE Computer Graphics & Applications,2721716,,Jan/Feb2005,25,1,76,9,15912024,10.1109/MCG.2005.29,IEEE,Article,VIRTUAL reality; VIDEO display terminals; NONLINEAR control theory; IMAGE processing; COMPUTER graphics; CAMERAS; Photographic Equipment and Supplies Merchant Wholesalers; Camera and photographic supplies stores; Electronics Stores; Photographic equipment and supplies merchant wholesalers; Photographic and Photocopying Equipment Manufacturing; Commercial and service industry machinery manufacturing; One-Hour Photofinishing; Photofinishing Laboratories (except One-Hour),,"The article focuses on a technique for generating deformed 3D visual effects using nonlinear perspective projections allows real-time navigation of 3D environments. The work primarily extends traditional 2D image deformation techniques to 3D space and perform the deformation only on the 2D frames generated by the 3D graphics pipeline. This requires no changes on the traditional 3D graphics pipeline. Deformation algorithms are derived from 3D noillinear perspective projections, which consider factors such as depth, view angle, and camera position-parameters 2D distortion algorithms don't consider. Distortion from our methods is more realistic than that of the 2D image distortion. In addition, our algorithms allow partially linear magnification or nonlinear deformation of 3D views in real time with less performance degradation. An experimental system that lets us deform 3D virtual worlds for real-time navigation was also developed. The projection of 3D points is decomposed into 2D image points into a perspective projection and a corresponding function that models deviations from the pinhole camera model. A perspective projection associated with the focal length linearly maps a 3D point with coordinates in the camera-centered coordinate system to an undeformed image point on the projection plane.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=15912024&site=ehost-live
232,PPL: A whole-image processing language,Jim Chen,"Computer Languages, Systems & Structures",14778424,,Apr-08,34,1,18,7,25827291,10.1016/j.cl.2006.07.003,Elsevier B.V.,Article,IMAGE processing; INFORMATION processing; APPLICATION program interfaces; COMPILERS (Computer programs); Photofinishing Laboratories (except One-Hour); One-Hour Photofinishing,Computer language; Imaging; Whole-image processing,"Abstract: This paper presents the design and implementation of the picture processing language (PPL) that extends the syntax and semantics of traditional image processing libraries. PPL provides a rich set of features to support the development of imaging systems. A main aspect is that many of these features treat a whole-image as an individual operand. An efficient memory management scheme is included that allows “in-place operation” with high memory efficiency. The PPL compiler together with an interpreter can work in two modes. The PPL compiler can convert the source code into C files that can be used as macros within a client program. The program can also be executed at run-time by an interpreter. The dual-execution modes make it possible to be used by both imaging researchers and equipment developers. The extended set of PPL instructions can communicate with digital sensors and 3D displays, and store image data into databases across the Internet. The wavelet-based reverse prediction algorithm can speed up the image loading process approximately three times faster than JPEG. The application programming interface (API) of PPL provides all the building blocks for programmers. [Copyright &y& Elsevier] Copyright of Computer Languages, Systems & Structures is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=25827291&site=ehost-live
233,Robust object tracking using enhanced random ferns.,Jim Chen,Visual Computer,1782789,,Apr-14,30,4,351,8,94942098,10.1007/s00371-013-0860-y,Springer Nature,Article,OBJECT tracking (Computer vision); COMPUTER vision; KERNEL operating systems; KERNEL functions; COMPLEX variables,Enhanced random ferns; Hidden classes; Object tracking; On-line clustering,"This paper presents a method to address the problem of long-term robust object tracking in unconstrained environments. An enhanced random fern is proposed and integrated into our tracking framework as the object detector, whose main idea is to exploit the potential distribution properties of feature vectors which are here called hidden classes by on-line clustering of feature space for each leaf-node of ferns. The kernel density estimation technique is then used to evaluate unlabeled samples based on the hidden classes which are set as the data points of the kernel function. Experimental results on challenging real-world video sequences demonstrate the effectiveness and robustness of our approach. Comparisons with several state-of-the-art approaches are provided. [ABSTRACT FROM AUTHOR] Copyright of Visual Computer is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94942098&site=ehost-live
234,Visual tracking with multiple Hough detectors.,Jim Chen,Image & Vision Computing,2628856,,Oct-17,66,,15,11,125311394,10.1016/j.imavis.2017.08.001,Elsevier B.V.,Article,TRACKING & trailing; DETECTORS; HOUGH transforms; HYPOTHESIS; ENTROPY; COMPUTER network resources; EQUIPMENT & supplies,Detector selection; Feature selection; Multiple Hough detectors; Visual tracking,"We propose a visual tracking method using multiple Hough detectors to address the problem of long-term robust object tracking in unconstrained environments. The method constructs the detectors based on the feature selection by the mutual information. These detectors serve to learn the partial appearances of target and synchronously evaluate image locations via the voting based detection with the generalized Hough transform. According to the result of detections, the best detector is selected by the minimum entropy criterion and delivers the final hypotheses for target location. The feature selection allows our tracker to be able to obtain and use the most discriminative parts of target and thus more robust to its changes, e.g. occlusion and deformation. The detector selection can correct undesirable model updates and restore the tracker after tracking failure. Meanwhile, the Hough-based detection can reduce the amount of noise introduced during online self-training and thus effectively prevent the tracker from drifting. The method is evaluated on the CVPR2013 Visual Tracker Benchmark and the experimental results demonstrate our method outperforms other tracking algorithms in terms of both success rate and precision. [ABSTRACT FROM AUTHOR] Copyright of Image & Vision Computing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125311394&site=ehost-live
235,"ACM/Springer Mobile Networks and Applications (MONET) Special Issue on 'Collaborative Computing: Networking, Applications and Worksharing'.",Songqing Chen,Mobile Networks & Applications,1383469X,,Aug-12,17,4,506,2,77569103,10.1007/s11036-012-0369-z,Springer Nature,Editorial,WIRELESS communications -- Congresses; ROUTING (Computer network management); COMPUTER network resources; SENSOR networks; NETWORK routing protocols; CONFERENCES & conventions; Convention and Trade Show Organizers; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),,"Information about the 6th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom) is presented. One paper described two broadcast authentication schemes such as key pool scheme and key chain scheme. Another paper introduced an analytical framework needed in the assessment of cooperative transmissions in sensor networks. Third paper focused on the development of utility-based routing scheme (UDR).",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77569103&site=ehost-live
236,An Application-Level Data Transparent Authentication Scheme without Communication Overhead.,Songqing Chen,IEEE Transactions on Computers,189340,,Jul-10,59,7,943,12,51197325,10.1109/TC.2010.80,IEEE,Article,BANDWIDTHS; AUTHENTICATION (Law); BROADBAND communication systems; WIRELESS communications; PROTOTYPES; Wired Telecommunications Carriers; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,authentication; covert channel; DaTA; data transparent; timing correlation,"With abundant aggregate network bandwidth, continuous data streams are commonly used in scientific and commercial applications. Correspondingly, there is an increasing demand of authenticating these data streams. Existing strategies explore data stream authentication by using message authentication codes (MACs) on a certain number of data packets (a data block) to generate a message digest, then either embedding the digest into the original data, or sending the digest out-of-band to the receiver. Embedding approaches inevitably change the original data, which is not acceptable under some circumstances (e.g., when sensitive information is included in the data). Sending the digest out-of-band incurs additional communication overhead, which consumes more critical resources (e.g., power in wireless devices for receiving information) besides network bandwidth. In this paper, we propose a novel strategy, DaTA, which effectively authenticates data streams by selectively adjusting some interpacket delay. This authentication scheme requires no change to the original data and no additional communication overhead. Modeling-based analysis and experiments conducted on an implemented prototype system in an LAN and over the Internet show that our proposed scheme is efficient and practical. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=51197325&site=ehost-live
236,An Application-Level Data Transparent Authentication Scheme without Communication Overhead.,Xinyuan Wang,IEEE Transactions on Computers,189340,,Jul-10,59,7,943,12,51197325,10.1109/TC.2010.80,IEEE,Article,BANDWIDTHS; AUTHENTICATION (Law); BROADBAND communication systems; WIRELESS communications; PROTOTYPES; Wired Telecommunications Carriers; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,authentication; covert channel; DaTA; data transparent; timing correlation,"With abundant aggregate network bandwidth, continuous data streams are commonly used in scientific and commercial applications. Correspondingly, there is an increasing demand of authenticating these data streams. Existing strategies explore data stream authentication by using message authentication codes (MACs) on a certain number of data packets (a data block) to generate a message digest, then either embedding the digest into the original data, or sending the digest out-of-band to the receiver. Embedding approaches inevitably change the original data, which is not acceptable under some circumstances (e.g., when sensitive information is included in the data). Sending the digest out-of-band incurs additional communication overhead, which consumes more critical resources (e.g., power in wireless devices for receiving information) besides network bandwidth. In this paper, we propose a novel strategy, DaTA, which effectively authenticates data streams by selectively adjusting some interpacket delay. This authentication scheme requires no change to the original data and no additional communication overhead. Modeling-based analysis and experiments conducted on an implemented prototype system in an LAN and over the Internet show that our proposed scheme is efficient and practical. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=51197325&site=ehost-live
237,Developing Subdomain Allocation Algorithms Based on Spatial and Communicational Constraints to Accelerate Dust Storm Simulation.,Songqing Chen,PLoS ONE,19326203,,4/4/16,11,4,1,33,114208716,10.1371/journal.pone.0152250,Public Library of Science,Article,DUST storms; ALGORITHMS; COMPUTER simulation; PREDICTION models; HIGH performance computing; LINEAR programming,Algebra; Algorithms; Applied mathematics; Computer and information sciences; Computing methods; Data visualization; Dust; Graphs; Infographics; Linear programming; Materials by structure; Materials science; Mathematical and statistical techniques; Mathematical functions; Mathematics; Optimization; Physical sciences; Polynomials; Research and analysis methods; Research Article; Simulation and modeling,"Dust storm has serious disastrous impacts on environment, human health, and assets. The developments and applications of dust storm models have contributed significantly to better understand and predict the distribution, intensity and structure of dust storms. However, dust storm simulation is a data and computing intensive process. To improve the computing performance, high performance computing has been widely adopted by dividing the entire study area into multiple subdomains and allocating each subdomain on different computing nodes in a parallel fashion. Inappropriate allocation may introduce imbalanced task loads and unnecessary communications among computing nodes. Therefore, allocation is a key factor that may impact the efficiency of parallel process. An allocation algorithm is expected to consider the computing cost and communication cost for each computing node to minimize total execution time and reduce overall communication cost for the entire simulation. This research introduces three algorithms to optimize the allocation by considering the spatial and communicational constraints: 1) an Integer Linear Programming (ILP) based algorithm from combinational optimization perspective; 2) a K-Means and Kernighan-Lin combined heuristic algorithm (K&K) integrating geometric and coordinate-free methods by merging local and global partitioning; 3) an automatic seeded region growing based geometric and local partitioning algorithm (ASRG). The performance and effectiveness of the three algorithms are compared based on different factors. Further, we adopt the K&K algorithm as the demonstrated algorithm for the experiment of dust model simulation with the non-hydrostatic mesoscale model (NMM-dust) and compared the performance with the MPI default sequential allocation. The results demonstrate that K&K method significantly improves the simulation performance with better subdomain allocation. This method can also be adopted for other relevant atmospheric and numerical modeling. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114208716&site=ehost-live
238,Measurement and Analysis of an Internet Streaming Service to Mobile Devices.,Fei Li,IEEE Transactions on Parallel & Distributed Systems,10459219,,Nov-13,24,11,2240,11,90678082,10.1109/TPDS.2012.324,IEEE,Article,"STREAMING video & television; STREAMING audio; CLOUD computing; MOBILE communication systems software; CACHE memory; TRANSCODING; STREAMING technology; Wireless Telecommunications Carriers (except Satellite); Data Processing, Hosting, and Related Services; Internet Publishing and Broadcasting and Web Search Portals",Cloud computing; heterogeneity; Internet mobile streaming; Mobile communication; Mobile handsets; popularity; Servers; Streaming media; transcoding; Video codecs,"Receiving Internet streaming services on various mobile devices is getting increasingly popular, and cloud platforms have also been gradually employed for delivering streaming services to mobile devices. While a number of studies have been conducted at the client side to understand and characterize Internet mobile streaming delivery, little is known about the server side, particularly for the recent cloud-based Internet mobile streaming delivery. In this work, we aim to investigate the Internet mobile streaming service at the server side. For this purpose, we have collected a 4-month server-side log on the cloud (with 1,002 TB delivered video traffic) from a top Internet mobile streaming service provider serving worldwide mobile users. Through trace analysis, we find that 1) a major challenge for providing Internet mobile streaming services is rooted from the mobile device hardware and software heterogeneity. In this workload, we find over 3,400 different hardware models with more than 100 different screen resolutions running 14 different mobile OS and three audio codecs and four video codecs. 2) To deal with the device heterogeneity, CPU-intensive transcoding is used on the cloud to customize the video to the appropriate versions at runtime for different devices. A video clip could be transcoded into more than 40 different versions to serve requests from different devices. 3) Compared to videos in traditional Internet streaming, mobile streaming videos are typically of much smaller size (a median of 1.68 MBytes) and shorter duration (a median of 2.7 minutes). Furthermore, the daily mobile user accesses are more skewed following a Zipf-like distribution but users' interests also quickly shift. Considering the huge demand of CPU cycles for online transcoding, we further examine server-side caching to reduce the total CPU cycle demand from the cloud. We show that a policy considering different versions of a video altogether outperforms other intuitive ones when the cache size is limited. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=90678082&site=ehost-live
238,Measurement and Analysis of an Internet Streaming Service to Mobile Devices.,Songqing Chen,IEEE Transactions on Parallel & Distributed Systems,10459219,,Nov-13,24,11,2240,11,90678082,10.1109/TPDS.2012.324,IEEE,Article,"STREAMING video & television; STREAMING audio; CLOUD computing; MOBILE communication systems software; CACHE memory; TRANSCODING; STREAMING technology; Wireless Telecommunications Carriers (except Satellite); Data Processing, Hosting, and Related Services; Internet Publishing and Broadcasting and Web Search Portals",Cloud computing; heterogeneity; Internet mobile streaming; Mobile communication; Mobile handsets; popularity; Servers; Streaming media; transcoding; Video codecs,"Receiving Internet streaming services on various mobile devices is getting increasingly popular, and cloud platforms have also been gradually employed for delivering streaming services to mobile devices. While a number of studies have been conducted at the client side to understand and characterize Internet mobile streaming delivery, little is known about the server side, particularly for the recent cloud-based Internet mobile streaming delivery. In this work, we aim to investigate the Internet mobile streaming service at the server side. For this purpose, we have collected a 4-month server-side log on the cloud (with 1,002 TB delivered video traffic) from a top Internet mobile streaming service provider serving worldwide mobile users. Through trace analysis, we find that 1) a major challenge for providing Internet mobile streaming services is rooted from the mobile device hardware and software heterogeneity. In this workload, we find over 3,400 different hardware models with more than 100 different screen resolutions running 14 different mobile OS and three audio codecs and four video codecs. 2) To deal with the device heterogeneity, CPU-intensive transcoding is used on the cloud to customize the video to the appropriate versions at runtime for different devices. A video clip could be transcoded into more than 40 different versions to serve requests from different devices. 3) Compared to videos in traditional Internet streaming, mobile streaming videos are typically of much smaller size (a median of 1.68 MBytes) and shorter duration (a median of 2.7 minutes). Furthermore, the daily mobile user accesses are more skewed following a Zipf-like distribution but users' interests also quickly shift. Considering the huge demand of CPU cycles for online transcoding, we further examine server-side caching to reduce the total CPU cycle demand from the cloud. We show that a policy considering different versions of a video altogether outperforms other intuitive ones when the cache size is limited. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=90678082&site=ehost-live
239,Risk-aware multi-objective optimized virtual machine placement in the cloud.,Songqing Chen,Journal of Computer Security,0926227X,,2018,26,5,707,24,134583443,10.3233/JCS-171104,IOS Press,Article,VIRTUAL machine systems; CLOUD computing security measures; RESOURCE allocation; RISK assessment; COMPUTER network security,Cloud security; multiple objective; risk metrics model; virtual machine placement; VM allocation strategy,"Cloud computing, while becoming more and more popular as a dominant computing platform, introduces new security challenges. When virtual machines are deployed in a cloud environment, virtual machine placement strategies can significantly affect the overall security risks of the entire cloud. In recent years, the attacks are specifically designed to co-locate with target virtual machines in the cloud. The virtual machine placement without considering the security risks may put the users, or even the entire cloud, in danger. In this paper, we present a comprehensive approach to quantify the security risk of cloud environments from network, host and VM. Accordingly, we propose a Security-aware Multi-Objective Optimization based virtual machine Placement scheme (SMOOP) to seek a Pareto-optimal solution that reduces the overall security risks of a cloud, while considering workload balance, resource utilization on CPU, memory, disk, and network traffic. New placement strategies are designed and our evaluation results demonstrate their effectiveness. The security of clouds could be improved with affordable overheads. The latest VM allocation policies are further studied and integrated into our designs to defeat the co-residence attacks. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computer Security is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134583443&site=ehost-live
240,Some special minimum -geodetically connected graphs,Songqing Chen,Discrete Applied Mathematics,0166218X,,Jun-11,159,10,1002,11,60157345,10.1016/j.dam.2011.03.009,Elsevier B.V.,Article,GEODESICS; GRAPH connectivity; PATHS & cycles in graph theory; ROBUST control; GRAPH theory; MATHEMATICAL analysis,Distance invulnerability; Geodetic connectivity; Minimum size graph; Robust system design; Survivable network,"Abstract: A connected graph is -geodetically connected (-GC) if the removal of less than vertices does not affect the distances (lengths of the shortest paths) between any pair of the remaining vertices. As such graphs have important applications in robust system designs, we are interested in the minimum number of edges required to make a -GC graph of order , and characterizing those minimum -GC graphs. When , minimum -GC graphs are not yet known in general, even the minimum number of edges is not determined. In this paper, we will determine all of the minimum -GC graphs for an infinite set of special pairs that were formerly unknown. To derive our results, we also developed new bounds on . Additionally, we show that -GC graphs with small relative optimality gaps can be easily constructed and expanded with great flexibilities, which gives convenient applications for robust system designs. [Copyright &y& Elsevier] Copyright of Discrete Applied Mathematics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60157345&site=ehost-live
241,Using adaptively coupled models and high-performance computing for enabling the computability of dust storm forecasting.,Songqing Chen,International Journal of Geographical Information Science,13658816,,Apr-13,27,4,765,20,87044562,10.1080/13658816.2012.715650,Taylor & Francis Ltd,Article,"DUST storms; WEATHER forecasting; ATMOSPHERIC models; GEOGRAPHIC information systems; COMPUTING platforms; All Other Professional, Scientific, and Technical Services",applied sciences; atmospheric modelling; computing intensity; Cyber GIS; geospatial platform; nested models; parallel computing; spatiotemporal thinking and computing,"Forecasting dust storms for large geographical areas with high resolution poses great challenges for scientific and computational research. Limitations of computing power and the scalability of parallel systems preclude an immediate solution to such challenges. This article reports our research on using adaptively coupled models to resolve the computational challenges and enable the computability of dust storm forecasting by dividing the large geographical domain into multiple subdomains based on spatiotemporal distributions of the dust storm. A dust storm model (Eta-8bin) performs a quick forecasting with low resolution (22 km) to identify potential hotspots with high dust concentration. A finer model, non-hydrostatic mesoscale model (NMM-dust) performs high-resolution (3 km) forecasting over the much smaller hotspots in parallel to reduce computational requirements and computing time. We also adopted spatiotemporal principles among computing resources and subdomains to optimize parallel systems and improve the performance of high-resolution NMM-dust model. This research enabled the computability of high-resolution, large-area dust storm forecasting using the adaptively coupled execution of the two models Eta-8bin and NMM-dust. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Geographical Information Science is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=87044562&site=ehost-live
242,Utilize cloud computing to support dust storm forecasting.,Songqing Chen,International Journal of Digital Earth,17538947,,Jul-13,6,4,338,18,88395673,10.1080/17538947.2012.749949,Taylor & Francis Ltd,Article,"CLOUD computing; DUST storms; HIGH performance computing; PUBLIC health; ENERGY conservation; ELECTRONIC data processing; Data Processing, Hosting, and Related Services; Health and Welfare Funds",Amazon EC2; cloud GIS; CyberGIS; loosely coupled nested model; spatial cloud computing,"The simulations and potential forecasting of dust storms are of significant interest to public health and environment sciences. Dust storms have interannual variabilities and are typical disruptive events. The computing platform for a dust storm forecasting operational system should support a disruptive fashion by scaling up to enable high-resolution forecasting and massive public access when dust storms come and scaling down when no dust storm events occur to save energy and costs. With the capability of providing a large, elastic, and virtualized pool of computational resources, cloud computing becomes a new and advantageous computing paradigm to resolve scientific problems traditionally requiring a large-scale and high-performance cluster. This paper examines the viability for cloud computing to support dust storm forecasting. Through a holistic study by systematically comparing cloud computing using Amazon EC2 to traditional high performance computing (HPC) cluster, we find that cloud computing is emerging as a credible solution for (1) supporting dust storm forecasting in spinning off a large group of computing resources in a few minutes to satisfy the disruptive computing requirements of dust storm forecasting, (2) performing high-resolution dust storm forecasting when required, (3) supporting concurrent computing requirements, (4) supporting real dust storm event forecasting for a large geographic domain by using recent dust storm event in Phoniex, 05 July 2011 as example, and (5) reducing cost by maintaining low computing support when there is no dust storm events while invoking a large amount of computing resource to perform high-resolution forecasting and responding to large amount of concurrent public accesses. [ABSTRACT FROM PUBLISHER] Copyright of International Journal of Digital Earth is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=88395673&site=ehost-live
243,CaptorX: A Class-Adaptive Convolutional Neural Network Reconfiguration Framework.,Xiang Chen,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,2780070,,Mar-22,41,3,530,14,155458664,10.1109/TCAD.2021.3061520,IEEE,Article,CONVOLUTIONAL neural networks; DEEP learning; MOBILE apps; ENERGY consumption,Adaptation models; CNN visualization; Computational modeling; Convolutional neural network (CNN) reconfiguration; distributed learning; Energy consumption; Load modeling; mobile computing; Task analysis; Training; Visualization,"Nowadays, the evolution of deep learning and cloud service significantly promotes neural network-based mobile applications. Although intelligent and prolific, those applications still lack certain flexibility: for classification tasks, neural networks are generally trained with vast classification targets to cover various utilization contexts. However, only partial classes are practically inferred due to individual mobile user preference and application specificity, which causes unnecessary computation consumption. Thus, we proposed CaptorX—a class-adaptive convolutional neural network (CNN) reconfiguration framework to adaptively prune convolutional filters associated with unneeded classes. CaptorX can reconfigure a pretrained full-class CNN model into class-specific lightweight models based on the visualization analysis of convolutional filters’ exclusive functionality for a single class. These lightweight models can be directly deployed to mobile devices without the retraining cost of traditional pruning-based reconfiguration. Furthermore, we can apply the CaptorX framework into a distributed collaboration setting. With dedicated local training regulation and collaborative aggregation schemes, the class-adaptive models on individual mobile devices can further contribute back to the central full-class model. Experiments on representative CNNs and image classification datasets show that, CaptorX can reduce the CNN computation workload up to 50.22% and save 46.58% energy consumption for varied local devices, meanwhile improving accuracy for their targeted classes with better task focus. With our distributed collaboration paradigm, CaptorX also provides further potential to enhance the central model accuracy, while reducing up to 37.58% communication cost compared to traditional distributed learning methods. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155458664&site=ehost-live
244,DiReCtX: Dynamic Resource-Aware CNN Reconfiguration Framework for Real-Time Mobile Applications.,Xiang Chen,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,2780070,,Feb-21,40,2,246,14,148281662,10.1109/TCAD.2020.2995813,IEEE,Article,MOBILE apps; CONVOLUTIONAL neural networks,Adaptation models; Computational modeling; Convolutional neural network (CNN); Dynamic scheduling; Integrated circuit modeling; mobile device; model reconfiguration; neuron pruning; Neurons; Optimization; Real-time systems; resource-aware,"Although convolutional neural networks (CNNs) have been widely applied in various cognitive applications, they are still very computationally intensive for resource-constrained mobile systems. To reduce the resource consumption of CNN computation, many optimization works have been proposed for mobile CNN deployment. However, most works are merely targeting CNN model compression from the perspective of parameter size or model structure, ignoring different resource constraints in mobile systems with respect to memory, energy, and real-time requirement. Moreover, previous works take accuracy as their primary consideration, requiring a time-costing retraining process to compensate the inference accuracy loss after compression. To address these issues, we propose DiReCtX—a dynamic resource-aware CNN model reconfiguration framework. DiReCtX is based on a set of accurate CNN profiling models for different resource consumption and inference accuracy estimation. With manageable consumption/accuracy tradeoffs, DiReCtX can reconfigure a CNN model to meet distinct resource constraint types and levels with expected inference performance maintained. To further achieve fast model reconfiguration in real-time, improved CNN model pruning and its corresponding accuracy tuning strategies are also proposed in DiReCtX. The experiments show that the proposed CNN profiling models can achieve 94.6% and 97.1% accuracy for CNN model resource consumption and inference accuracy estimation. Meanwhile, the proposed reconfiguration scheme of DiReCtX can achieve at most 44.44% computation acceleration, 31.69% memory reduction, and 32.39% energy saving, respectively. On field-tests with state-of-the-art smartphones, DiReCtX can adapt CNN models to various resource constraints in mobile application scenarios with optimal real-time performance. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148281662&site=ehost-live
245,HOW CONVOLUTIONAL NEURAL NETWORKS SEE THE WORLD | A SURVEY OF CONVOLUTIONAL NEURAL NETWORK VISUALIZATION METHODS.,Xiang Chen,Mathematical Foundations of Computing,25778838,,May-18,1,2,149,32,131505811,10.3934/mfc.2018008,American Institute of Mathematical Sciences,Article,ARTIFICIAL neural networks; VISUALIZATION; PEER-to-peer architecture (Computer networks); DYNAMIC programming; SCALABILITY,58F17; CNN feature; CNN visualization; convolutional neural network; Deep learning; network interpretability; network interpretability.; Primary: 58F15; Secondary: 53C35.,"Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc. These achievements benefit from the CNNs' outstanding capability to learn the input features with deep layers of neuron structures and iterative training process. However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the CNNs' internal working mechanism. To improve the CNN interpretability, the CNN visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns. And many CNN visualization works have been proposed in the literature to interpret the CNN in perspectives of network structure, operation, and semantic concept. In this paper, we expect to provide a comprehensive survey of several rep-resentative CNN visualization methods, including Activation Maximization, Network Inversion, Deconvolutional Neural Networks (DeconvNet), and Network Dissection based visualization. These methods are presented in terms of motivations, algorithms, and experiment results. Based on these visualization methods, we also discuss their practical applications to demonstrate the significance of the CNN interpretability in areas of network design, optimization, security enhancement, etc. [ABSTRACT FROM AUTHOR] Copyright of Mathematical Foundations of Computing is the property of American Institute of Mathematical Sciences and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131505811&site=ehost-live
246,REIN the RobuTS: Robust DNN-Based Image Recognition in Autonomous Driving Systems.,Xiang Chen,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,2780070,,Jun-21,40,6,1258,14,150448923,10.1109/TCAD.2020.3033498,IEEE,Article,"IMAGE recognition (Computer vision); AUTONOMOUS vehicles; DRIVERLESS cars; TRAFFIC signs & signals; MOTOR vehicle driving; WEATHER; TRAFFIC safety; Electrical Contractors and Other Wiring Installation Contractors; All Other Support Services; Highway, Street, and Bridge Construction",Autonomous driving; Autonomous vehicles; Data models; deep neural network (NN); Neural networks; Rain; robust image recognition; Task analysis; Training,"In recent years, the neural network (NN) has shown its great potential in image recognition tasks of autonomous driving systems, such as traffic sign recognition, pedestrian detection, etc. However, theoretically well-trained NNs usually fail their performance when facing real-world scenarios. For example, adverse real-world conditions, e.g., bad weather and lighting conditions, can introduce different physical variations and cause considerable accuracy degradation. As for now, the generalization capability of NNs is still one of the most critical challenges for the autonomous driving system. To facilitate the robust image recognition tasks, in this work, we build the RobuTS dataset: a comprehensive Robust Traffic Sign Recognition dataset, which includes images with different environmental variations, e.g., rain, fog, darkening, and blurring. Then to enhance the NN’s generalization capability, we propose two generalization-enhanced training schemes: 1) REIN for robust training without data in adverse scenarios and 2) Self-Teaching (ST) for robust training with unlabeled adverse data. The great advantages of such two training schemes are they are data-free (REIN) and label-free (ST), thus effectively reducing the huge human efforts/cost of on-road driving data collection, as well as the expensive manual data annotation. We conduct extensive experiments to validate our methods’ performance on both classification and detection tasks. For classification tasks, our proposed training algorithms could consistently improve model performance by +15%–25% (REIN) and +16%–30% (ST) in all adverse scenarios of our RobuTS datasets. For detection tasks, our ST could also improve the detector’s performance by +10.1 mean average precision (mAP) on Foggy-Cityscapes, outperforming previous state-of-the-art works by +2.2 mAP. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150448923&site=ehost-live
247,Customizable Scale-Out Key-Value Stores.,Yue Cheng,IEEE Transactions on Parallel & Distributed Systems,10459219,,Sep-20,31,9,2081,16,143316160,10.1109/TPDS.2020.2982640,IEEE,Article,HIGH performance computing; RETAIL stores; APPLICATION stores; SOFTWARE-defined networking; PEER-to-peer architecture (Computer networks); DATA management; NONVOLATILE memory; All Other Miscellaneous Store Retailers (except Tobacco Stores); Commercial and Institutional Building Construction; All other miscellaneous store retailers (except beer and wine-making supplies stores); All other miscellaneous general merchandise stores,application tailored storage; Cloud computing; Distributed databases; Fault tolerance; Fault tolerant systems; HPC KV stores; Key-value stores; Nonvolatile memory; Peer-to-peer computing; scale-out KV stores; Topology,"Enterprise KV stores are often not well suited for HPC applications, and thus cumbersome end-to-end KV design customization is required to meet the needs of modern HPC applications. To this end, in this article we present bespoKV, an adaptive, extensible, and scale-out KV store framework. bespoKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. For the control plane, bespoKVprovides pre-built modules, called controlets, supporting common distributed functionalities (e.g., replication, consistency, and topology) and their various combinations. This decoupling allows bespoKV to take a user-provided single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Such specializations enable innovative uses of KV stores in HPC applications, especially for emerging applications that utilize KV-friendly workloads. We evaluate bespoKV in a local testbed as well as in a public cloud settings. Experiments show that bespoKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes 1.2× to 2.6× better than the state-of-the-art systems. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143316160&site=ehost-live
248,Correlation of rupture dynamics to the nonlinear backscatter response from polymer-shelled ultrasound contrast agents.,Parag Chitnis,IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control,8853010,,Mar-15,62,3,494,8,101560800,10.1109/TUFFC.2014.006828,IEEE,Article,RUPTURES (Structural failure); BACKSCATTERING; STRUCTURAL shells; CONTRAST media; ULTRASONIC imaging; ENCAPSULATION (Catalysis); NONLINEAR oscillations,Acoustics; Backscatter; Microscopy; Polymers; Sociology; Statistics; Ultrasonic imaging,"Polymer-shelled ultrasound contrast agents (UCAs) may expel their encapsulated gas subject to ultrasound- induced shell buckling or rupture. Nonlinear oscillations of this gas bubble can produce a subharmonic component in the ultrasound backscatter. This study investigated the relationship between this gas-release mechanism and shell-thickness? to?radius ratios (STRRs) of polymer-shelled UCAs. Three types of polylactide-shelled UCAs with STRRs of 7.5, 40, and 100 nm/?m were studied. Each UCA population had a nominal mean diameter of 2 ?m. UCAs were subjected to increasing static overpressure ranging from 2 to 330 kPa over a duration of 2 h in a custom-designed test chamber while being imaged using a 200? magnification video microscope at a frame rate of 5 frames/s. Digitized video images were binarized and processed to obtain the cross-sectional area of individual UCAs. Integration of the normalized cross-sectional area over normalized time, defined as buckling factor (Bf), provided a dimensionless parameter for quantifying and comparing the degree of pre-rupture buckling exhibited by the UCAs of different STRRs in response to overpressure. The UCAs with an STRR of 7.5 nm/?m exhibited a distinct shell-buckling phase before shell rupture (Bf < 1), whereas the UCAs with higher STRRs (40 and 100 nm/?m) did not undergo significant prerupture buckling (Bf ? 1). The difference in the overpressure response was correlated with the subharmonic response produced by these UCAs. When excited using 20-MHz ultrasound, individual UCAs (N = 3000) in populations that did not exhibit a buckling phase produced a subharmonic response that was an order of magnitude greater than the UCA population with a prominent pre-rupture buckling phase. These results indicate the mechanism of gas expulsion from these UCAs might be a relevant factor in determining the level of subharmonic response in response to high-frequency ultrasound. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101560800&site=ehost-live
249,Limited-View and Sparse Photoacoustic Tomography for Neuroimaging with Deep Learning.,Siddhartha Sikdar,Scientific Reports,20452322,,5/22/20,10,1,1,12,143387426,10.1038/s41598-020-65235-2,Springer Nature,Article,BRAIN imaging; DEEP learning; OPTICAL imaging sensors; IMAGE quality analysis; ACOUSTIC imaging,,"Photoacoustic tomography (PAT) is a non-ionizing imaging modality capable of acquiring high contrast and resolution images of optical absorption at depths greater than traditional optical imaging techniques. Practical considerations with instrumentation and geometry limit the number of available acoustic sensors and their ""view"" of the imaging target, which result in image reconstruction artifacts degrading image quality. Iterative reconstruction methods can be used to reduce artifacts but are computationally expensive. In this work, we propose a novel deep learning approach termed pixel-wise deep learning (Pixel-DL) that first employs pixel-wise interpolation governed by the physics of photoacoustic wave propagation and then uses a convolution neural network to reconstruct an image. Simulated photoacoustic data from synthetic, mouse-brain, lung, and fundus vasculature phantoms were used for training and testing. Results demonstrated that Pixel-DL achieved comparable or better performance to iterative methods and consistently outperformed other CNN-based approaches for correcting artifacts. Pixel-DL is a computationally efficient approach that enables for real-time PAT rendering and improved image reconstruction quality for limited-view and sparse PAT. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143387426&site=ehost-live
249,Limited-View and Sparse Photoacoustic Tomography for Neuroimaging with Deep Learning.,Parag Chitnis,Scientific Reports,20452322,,5/22/20,10,1,1,12,143387426,10.1038/s41598-020-65235-2,Springer Nature,Article,BRAIN imaging; DEEP learning; OPTICAL imaging sensors; IMAGE quality analysis; ACOUSTIC imaging,,"Photoacoustic tomography (PAT) is a non-ionizing imaging modality capable of acquiring high contrast and resolution images of optical absorption at depths greater than traditional optical imaging techniques. Practical considerations with instrumentation and geometry limit the number of available acoustic sensors and their ""view"" of the imaging target, which result in image reconstruction artifacts degrading image quality. Iterative reconstruction methods can be used to reduce artifacts but are computationally expensive. In this work, we propose a novel deep learning approach termed pixel-wise deep learning (Pixel-DL) that first employs pixel-wise interpolation governed by the physics of photoacoustic wave propagation and then uses a convolution neural network to reconstruct an image. Simulated photoacoustic data from synthetic, mouse-brain, lung, and fundus vasculature phantoms were used for training and testing. Results demonstrated that Pixel-DL achieved comparable or better performance to iterative methods and consistently outperformed other CNN-based approaches for correcting artifacts. Pixel-DL is a computationally efficient approach that enables for real-time PAT rendering and improved image reconstruction quality for limited-view and sparse PAT. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143387426&site=ehost-live
250,"M3VR—A multi-stage, multi-resolution, and multi-volumes-of-interest volume registration method applied to 3D endovaginal ultrasound.",Parag Chitnis,PLoS ONE,19326203,,11/21/19,14,11,1,21,139787859,10.1371/journal.pone.0224583,Public Library of Science,Article,DIAGNOSTIC ultrasonic imaging; ULTRASONIC imaging; COST functions; THREE-dimensional imaging; DIAGNOSTIC imaging; PELVIC floor; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,,"Heterogeneity of echo-texture and lack of sharply delineated tissue boundaries in diagnostic ultrasound images make three-dimensional (3D) registration challenging, especially when the volumes to be registered are considerably different due to local changes. We implemented a novel computational method that optimally registers volumetric ultrasound image data containing significant and local anatomical differences. It is A Multi-stage, Multi-resolution, and Multi-volumes-of-interest Volume Registration Method. A single region registration is optimized first for a close initial alignment to avoid convergence to a locally optimal solution. Multiple sub-volumes of interest can then be selected as target alignment regions to achieve confident consistency across the volume. Finally, a multi-resolution rigid registration is performed on these sub-volumes associated with different weights in the cost function. We applied the method on 3D endovaginal ultrasound image data acquired from patients during biopsy procedure of the pelvic floor muscle. Systematic assessment of our proposed method through cross validation demonstrated its accuracy and robustness. The algorithm can also be applied on medical imaging data of other modalities for which the traditional rigid registration methods would fail. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139787859&site=ehost-live
250,"M3VR—A multi-stage, multi-resolution, and multi-volumes-of-interest volume registration method applied to 3D endovaginal ultrasound.",Siddhartha Sikdar,PLoS ONE,19326203,,11/21/19,14,11,1,21,139787859,10.1371/journal.pone.0224583,Public Library of Science,Article,DIAGNOSTIC ultrasonic imaging; ULTRASONIC imaging; COST functions; THREE-dimensional imaging; DIAGNOSTIC imaging; PELVIC floor; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,,"Heterogeneity of echo-texture and lack of sharply delineated tissue boundaries in diagnostic ultrasound images make three-dimensional (3D) registration challenging, especially when the volumes to be registered are considerably different due to local changes. We implemented a novel computational method that optimally registers volumetric ultrasound image data containing significant and local anatomical differences. It is A Multi-stage, Multi-resolution, and Multi-volumes-of-interest Volume Registration Method. A single region registration is optimized first for a close initial alignment to avoid convergence to a locally optimal solution. Multiple sub-volumes of interest can then be selected as target alignment regions to achieve confident consistency across the volume. Finally, a multi-resolution rigid registration is performed on these sub-volumes associated with different weights in the cost function. We applied the method on 3D endovaginal ultrasound image data acquired from patients during biopsy procedure of the pelvic floor muscle. Systematic assessment of our proposed method through cross validation demonstrated its accuracy and robustness. The algorithm can also be applied on medical imaging data of other modalities for which the traditional rigid registration methods would fail. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139787859&site=ehost-live
250,"M3VR—A multi-stage, multi-resolution, and multi-volumes-of-interest volume registration method applied to 3D endovaginal ultrasound.",Qi Wei,PLoS ONE,19326203,,11/21/19,14,11,1,21,139787859,10.1371/journal.pone.0224583,Public Library of Science,Article,DIAGNOSTIC ultrasonic imaging; ULTRASONIC imaging; COST functions; THREE-dimensional imaging; DIAGNOSTIC imaging; PELVIC floor; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,,"Heterogeneity of echo-texture and lack of sharply delineated tissue boundaries in diagnostic ultrasound images make three-dimensional (3D) registration challenging, especially when the volumes to be registered are considerably different due to local changes. We implemented a novel computational method that optimally registers volumetric ultrasound image data containing significant and local anatomical differences. It is A Multi-stage, Multi-resolution, and Multi-volumes-of-interest Volume Registration Method. A single region registration is optimized first for a close initial alignment to avoid convergence to a locally optimal solution. Multiple sub-volumes of interest can then be selected as target alignment regions to achieve confident consistency across the volume. Finally, a multi-resolution rigid registration is performed on these sub-volumes associated with different weights in the cost function. We applied the method on 3D endovaginal ultrasound image data acquired from patients during biopsy procedure of the pelvic floor muscle. Systematic assessment of our proposed method through cross validation demonstrated its accuracy and robustness. The algorithm can also be applied on medical imaging data of other modalities for which the traditional rigid registration methods would fail. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139787859&site=ehost-live
251,Subharmonic Response of Polymer Contrast Agents Based on the Empirical Mode Decomposition.,Parag Chitnis,IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control,8853010,,Dec-16,63,12,2107,7,119943145,10.1109/TUFFC.2016.2615047,IEEE,Article,CONTRAST media; HILBERT-Huang transform; SUBHARMONIC functions; BACKSCATTERING; TIME series analysis,Acoustics; Backscatter; Chirp; Contrast agent; Empirical mode decomposition; empirical mode decomposition (EMD); Harmonic analysis; polymer-shelled; Polymers; subharmonic; Ultrasonic imaging; ultrasound,"The subharmonic threshold for ultrasound contrast agents has been defined as a 20–25 dB difference between the fundamental and subharmonic (2/1) spectral components of the backscatter signal. However, this Fourier-based criterion assumes a linear time-invariant signal. A more appropriate criterion for short cycle and frequency-modulated waveforms is proposed with an adaptive signal-processing approach based on the empirical mode decomposition (EMD) method. The signal is decomposed into an orthogonal basis known as intrinsic mode functions (IMFs) and a subharmonic threshold is defined with respect to the energy ratio of the subharmonic IMF component to that of the incident signal. The method is applied to backscatter data acquired from two polymer-shelled contrast agents, Philips (#38, mean diameter 2.0 \mu \textm ) and Point Biomedical (#12027, mean diameter 3.9 \mu \textm ). The acoustic backscatter signals are investigated for a single contrast agent subjected to monofrequency (20 MHz, 20 cycles) and chirp (15–25 MHz, 20 cycles) forcing for incident pressures ranging from 0.5 to 2.4 MPa. In comparison to the spectral peak difference (20 dB) criterion, the EMD method is more sensitive in determining subharmonic signals. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119943145&site=ehost-live
252,SVD-Based Separation of Stable and Inertial Cavitation Signals Applied to Passive Cavitation Mapping During HIFU.,Parag Chitnis,IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control,8853010,,May-19,66,5,857,10,136117758,10.1109/TUFFC.2019.2898917,IEEE,Article,CAVITATION; HIGH-intensity focused ultrasound; SINGULAR value decomposition; ACOUSTIC transducers; RADIO frequency; ULTRASONIC imaging,Acoustics; Broadband communication; Cavitation; filtering; Frequency control; Harmonic analysis; high-intensity focused ultrasound (HIFU); Power harmonic filters; Radio frequency; singular value decomposition (SVD); Ultrasonic imaging,"Detection of inertial and stable cavitation is important for guiding high-intensity focused ultrasound (HIFU). Acoustic transducers can passively detect broadband noise from inertial cavitation and the scattering of HIFU harmonics from stable cavitation bubbles. Conventional approaches to cavitation noise diagnostics typically involve computing the Fourier transform of the time-domain noise signal, applying a custom comb filter to isolate the frequency components of interest, followed by an inverse Fourier transform. We present an alternative technique based on singular value decomposition (SVD) that efficiently separates the broadband emissions and HIFU harmonics. Spatiotemporally resolved cavitation detection was achieved using a 128-element, 5-MHz linear-array ultrasound imaging system operating in the receive mode at 15 frames/s. A 1.1-MHz transducer delivered HIFU to tissue-mimicking phantoms and excised liver tissue for a duration of 5 s. Beamformed radio frequency signals corresponding to each scan line in a frame were assembled into a matrix, and SVD was performed. Spectra of the singular vectors obtained from a tissue-mimicking gel phantom were analyzed by computing the peak ratio (${R}$), defined as the ratio of the peak of its fifth-order polynomial fit and the maximum spectral peak. Singular vectors that produced an ${R} < 0.048$ were classified as those representing stable cavitation, i.e., predominantly containing harmonics of HIFU. The projection of data onto this singular base reproduced stable cavitation signals. Similarly, singular vectors that produced an ${R} >0.2$ were classified as those predominantly containing broadband noise associated with inertial cavitation. These singular vectors were used to isolate the inertial cavitation signal. The ${R}$ -value thresholds determined using gel data were then employed to analyze cavitation data obtained from bovine liver ex vivo. The SVD-based method faithfully reproduced the structural details in the spatiotemporal cavitation maps produced using the more cumbersome comb-filter approach with a maximum root-mean-squared error of 10%. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136117758&site=ehost-live
253,Ultrasound‐Responsive Aqueous Two‐Phase Microcapsules for On‐Demand Drug Release.,Parag Chitnis,Angewandte Chemie International Edition,14337851,,Mar-22,,,1,1,155788896,10.1002/ange.202116515,"John Wiley & Sons, Inc.",Article,,Drug Delivery; Focused Ultrasound; Microcapsules; Microfluidics; Polymers,"Traditional implanted drug delivery systems cannot easily change their release profile in real time to respond to physiological changes. Here we present a microfluidic aqueous two‐phase system to generate microcapsules that can release drugs on demand as triggered by focused ultrasound (FUS). The biphasic microcapsules are made of hydrogels with an outer phase of mixed molecular weight (MW) poly(ethylene glycol) diacrylate that mitigates premature payload release and an inner phase of high MW dextran with payload that breaks down in response to FUS. Compound release from microcapsules could be triggered as desired; 0.4 μg of payload was released across 16 on‐demand steps over days. We detected broadband acoustic signals amidst low heating, suggesting inertial cavitation as a key mechanism for payload release. Overall, FUS‐responsive microcapsules are a biocompatible and wirelessly triggerable structure for on‐demand drug delivery over days to weeks. [ABSTRACT FROM AUTHOR] Copyright of Angewandte Chemie International Edition is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155788896&site=ehost-live
254,Ultrasound‐Responsive Aqueous Two‐Phase Microcapsules for On‐Demand Drug Release.,Parag Chitnis,Angewandte Chemie International Edition,14337851,,5/9/22,61,20,1,7,156657328,10.1002/anie.202116515,"John Wiley & Sons, Inc.",Article,DEXTRAN; DRUG delivery systems; ETHYLENE glycol; MOLECULAR weights; All Other Basic Organic Chemical Manufacturing; Other basic organic chemical manufacturing,Drug Delivery; Focused Ultrasound; Microcapsules; Microfluidics; Polymers,"Traditional implanted drug delivery systems cannot easily change their release profile in real time to respond to physiological changes. Here we present a microfluidic aqueous two‐phase system to generate microcapsules that can release drugs on demand as triggered by focused ultrasound (FUS). The biphasic microcapsules are made of hydrogels with an outer phase of mixed molecular weight (MW) poly(ethylene glycol) diacrylate that mitigates premature payload release and an inner phase of high MW dextran with payload that breaks down in response to FUS. Compound release from microcapsules could be triggered as desired; 0.4 μg of payload was released across 16 on‐demand steps over days. We detected broadband acoustic signals amidst low heating, suggesting inertial cavitation as a key mechanism for payload release. Overall, FUS‐responsive microcapsules are a biocompatible and wirelessly triggerable structure for on‐demand drug delivery over days to weeks. [ABSTRACT FROM AUTHOR] Copyright of Angewandte Chemie International Edition is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156657328&site=ehost-live
255,Ultrasound‐Responsive Aqueous Two‐Phase Microcapsules for On‐Demand Drug Release.,Parag Chitnis,Angewandte Chemie,448249,,5/9/22,134,20,1,7,156658787,10.1002/ange.202116515,"John Wiley & Sons, Inc.",Article,DEXTRAN; DRUG delivery systems; ETHYLENE glycol; MOLECULAR weights; All Other Basic Organic Chemical Manufacturing; Other basic organic chemical manufacturing,Drug Delivery; Focused Ultrasound; Microcapsules; Microfluidics; Polymers,"Traditional implanted drug delivery systems cannot easily change their release profile in real time to respond to physiological changes. Here we present a microfluidic aqueous two‐phase system to generate microcapsules that can release drugs on demand as triggered by focused ultrasound (FUS). The biphasic microcapsules are made of hydrogels with an outer phase of mixed molecular weight (MW) poly(ethylene glycol) diacrylate that mitigates premature payload release and an inner phase of high MW dextran with payload that breaks down in response to FUS. Compound release from microcapsules could be triggered as desired; 0.4 μg of payload was released across 16 on‐demand steps over days. We detected broadband acoustic signals amidst low heating, suggesting inertial cavitation as a key mechanism for payload release. Overall, FUS‐responsive microcapsules are a biocompatible and wirelessly triggerable structure for on‐demand drug delivery over days to weeks. [ABSTRACT FROM AUTHOR] Copyright of Angewandte Chemie is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156658787&site=ehost-live
256,All (Mayoral) Politics Is Local?,Sanmay Das,Journal of Politics,223816,,Apr-22,84,2,1021,14,156554546,10.1086/716945,University of Chicago Press,Article,"POLITICAL attitudes; MAYORS; VOTERS; GOVERNORS; POLITICAL oratory; RHETORIC; Executive Offices; Other local, municipal and regional public administration; Marketing Research and Public Opinion Polling; UNITED States politics & government; TWITTER (Web resource)",mayors; nationalization; rhetoric; Twitter,"One of the defining characteristics of modern politics in the United States is the increasing nationalization of elite- and voter-level behavior. Relying on measures of electoral vote shares, previous research has found evidence indicating a significant amount of state-level nationalization. Using an alternative source of data—the political rhetoric used by mayors, state governors, and members of Congress on Twitter—we examine and compare the amount of between-office nationalization throughout the federal system. We find that gubernatorial rhetoric closely matches that of members of Congress, but that there are substantial differences in the topics and content of mayoral speech. These results suggest that, on average, American mayors have largely remained focused on their local mandate. More broadly, our findings suggest a limit to which American politics has become nationalized—in some cases, all politics remains local. [ABSTRACT FROM AUTHOR] Copyright of Journal of Politics is the property of The Southern Political Science Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156554546&site=ehost-live
257,Bayesian co-clustering.,Carlotta Domeniconi,WIREs: Computational Statistics,19395108,,Sep-15,7,5,347,10,108866407,10.1002/wics.1359,Wiley-Blackwell,Article,PROTEINS; BAYESIAN analysis; MOLECULES; MICROBIOLOGY; TEXT mining (Information retrieval); MATHEMATICAL models,Bayesian data mining; Bayesian nonparametrics; co‐clustering; co-clustering; ensemble methods; Markov chain Monte Carlo,"Co-clustering means simultaneously identifying natural clusters in different kinds of objects. Examples include simultaneously clustering customers and products for a recommender application; simultaneously clustering proteins and molecules in microbiology; or simultaneously clustering documents and words in a text mining application. Important insights into a problem can be gained by understanding the interactions between clusters for the different kinds of objects. This paper considers Bayesian models for co-clustering. The Bayesian approach begins by developing a model for the data generating process, and inverting that model through Bayesian inference to infer cluster membership, learn characteristics of the clusters, and fill in missing observations. We consider a basic Bayesian clustering model and several extensions to the model. Experimental evaluations and comparisons among the clustering methods are presented. WIREs Comput Stat 2015, 7:347-356. doi: 10.1002/wics.1359 For further resources related to this article, please visit the . [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108866407&site=ehost-live
258,CMAL: Cost-Effective Multi-Label Active Learning by Querying Subexamples.,Carlotta Domeniconi,IEEE Transactions on Knowledge & Data Engineering,10414347,,May-22,34,5,2091,15,156273244,10.1109/TKDE.2020.3003899,IEEE,Article,ACTIVE learning; ANNOTATIONS,Annotations; Correlation; label correlation; label sparsity; Measurement uncertainty; multi-instance learning; Multi-label active learning; representative; Semantics; Training; Uncertainty,"Multi-label active learning (MAL) aims to learn an accurate multi-label classifier by selecting which examples (or example-label pairs) will be annotated and reducing query effort. MAL is a more complicated and expensive process than single-label active learning, due to one example can be associated with a set of non-exclusive labels and the annotator has to scrutinize the whole example and label space to provide correct annotations. Instead of scrutinizing the whole example for annotation, we may just examine some of its subexamples with respect to a label for annotation. In this way, we can not only save the annotation cost but also speedup the annotation process. Given this observation, we introduce CMAL, a two-stage Cost-effective MAL strategy (CMAL) by querying subexamples. CMAL first selects the most informative example-label pairs by leveraging uncertainty, label correlation and label space sparsity. Specifically, the uncertainty of a label to an example can be reduced if its correlated labels already annotated to the example, and its uncertainty can be reduced also if more examples annotated to this label. Next, CMAL greedily queries the most probable positive subexample-label pairs of the selected example-label pair. In addition, we propose rCMAL to account for the representative of examples to more reliably select example-label pairs in the first stage. Extensive experiments on multi-label datasets from diverse domains show that our proposed CMAL and rCMAL can better save the query cost than state-of-the-art MAL methods. The contribution of leveraging label correlation, label sparsity, and representative for saving cost is also confirmed. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156273244&site=ehost-live
259,"Cooperative driver pathway discovery via fusion of multi-relational data of genes, miRNAs and pathways.",Carlotta Domeniconi,Briefings in Bioinformatics,14675463,,Mar-21,22,2,1984,16,149507185,10.1093/bib/bbz167,Oxford University Press / USA,Article,GENES; RECEIVER operating characteristic curves; MULTISENSOR data fusion; MICRORNA; GENE regulatory networks; ENDOMETRIAL cancer,biological network; cancer gene; cooperative driver pathway; data fusion; microRNA,"Discovering driver pathways is an essential step to uncover the molecular mechanism underlying cancer and to explore precise treatments for cancer patients. However, due to the difficulties of mapping genes to pathways and the limited knowledge about pathway interactions, most previous work focus on identifying individual pathways. In practice, two (or even more) pathways interplay and often cooperatively trigger cancer. In this study, we proposed a new approach called CDPathway to discover cooperative driver pathways. First, CDPathway introduces a driver impact quantification function to quantify the driver weight of each gene. CDPathway assumes that genes with larger weights contribute more to the occurrence of the target disease and identifies them as candidate driver genes. Next, it constructs a heterogeneous network composed of genes, miRNAs and pathways nodes based on the known intra(inter)-relations between them and assigns the quantified driver weights to gene–pathway and gene–miRNA relational edges. To transfer driver impacts of genes to pathway interaction pairs, CDPathway collaboratively factorizes the weighted adjacency matrices of the heterogeneous network to explore the latent relations between genes, miRNAs and pathways. After this, it reconstructs the pathway interaction network and identifies the pathway pairs with maximal interactive and driver weights as cooperative driver pathways. Experimental results on the breast, uterine corpus endometrial carcinoma and ovarian cancer data from The Cancer Genome Atlas show that CDPathway can effectively identify candidate driver genes [area under the receiver operating characteristic curve (AUROC) of |$\geq $| 0.9] and reconstruct the pathway interaction network (AUROC of>0.9), and it uncovers much more known (potential) driver genes than other competitive methods. In addition, CDPathway identifies 150% more driver pathways and 60% more potential cooperative driver pathways than the competing methods. The code of CDPathway is available at http://mlda.swu.edu.cn/codes.php?name=CDPathway. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149507185&site=ehost-live
260,Differentiating isoform functions with collaborative matrix factorization.,Carlotta Domeniconi,Bioinformatics,13674803,,3/15/20,36,6,1864,8,142563680,10.1093/bioinformatics/btz847,Oxford University Press / USA,Article,"MATRIX decomposition; WEB servers; RECEIVER operating characteristic curves; MATRIX functions; LOW-rank matrices; GENE ontology; Data Processing, Hosting, and Related Services",,"Motivation Isoforms are alternatively spliced mRNAs of genes. They can be translated into different functional proteoforms, and thus greatly increase the functional diversity of protein variants (or proteoforms). Differentiating the functions of isoforms (or proteoforms) helps understanding the underlying pathology of various complex diseases at a deeper granularity. Since existing functional genomic databases uniformly record the annotations at the gene-level, and rarely record the annotations at the isoform-level, differentiating isoform functions is more challenging than the traditional gene-level function prediction. Results Several approaches have been proposed to differentiate the functions of isoforms. They generally follow the multi-instance learning paradigm by viewing each gene as a bag and the spliced isoforms as its instances, and push functions of bags onto instances. These approaches implicitly assume the collected annotations of genes are complete and only integrate multiple RNA-seq datasets. As such, they have compromised performance. We propose a data integrative solution (called DisoFun) to D ifferentiate iso form Fun ctions with collaborative matrix factorization. DisoFun assumes the functional annotations of genes are aggregated from those of key isoforms. It collaboratively factorizes the isoform data matrix and gene-term data matrix (storing Gene Ontology annotations of genes) into low-rank matrices to simultaneously explore the latent key isoforms, and achieve function prediction by aggregating predictions to their originating genes. In addition, it leverages the PPI network and Gene Ontology structure to further coordinate the matrix factorization. Extensive experimental results show that DisoFun improves the area under the receiver operating characteristic curve and area under the precision-recall curve of existing solutions by at least 7.7 and 28.9%, respectively. We further investigate DisoFun on four exemplar genes (LMNA, ADAM15, BCL2L1 and CFLAR) with known functions at the isoform-level, and observed that DisoFun can differentiate functions of their isoforms with 90.5% accuracy. Availability and implementation The code of DisoFun is available at mlda.swu.edu.cn/codes.php?name=DisoFun. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142563680&site=ehost-live
261,DMIL-IsoFun: predicting isoform function using deep multi-instance learning.,Carlotta Domeniconi,Bioinformatics,13674803,,12/15/21,37,24,4818,8,154328735,10.1093/bioinformatics/btab532,Oxford University Press / USA,Article,DEEP learning; CONVOLUTIONAL neural networks; HUMAN genes,,"Motivation Alternative splicing creates the considerable proteomic diversity and complexity on relatively limited genome. Proteoforms translated from alternatively spliced isoforms of a gene actually execute the biological functions of this gene, which reflect the functional knowledge of genes at a finer granular level. Recently, some computational approaches have been proposed to differentiate isoform functions using sequence and expression data. However, their performance is far from being desirable, mainly due to the imbalance and lack of annotations at isoform-level, and the difficulty of modeling gene–isoform relations. Result We propose a deep multi-instance learning-based framework (DMIL-IsoFun) to differentiate the functions of isoforms. DMIL-IsoFun firstly introduces a multi-instance learning convolution neural network trained with isoform sequences and gene-level annotations to extract the feature vectors and initialize the annotations of isoforms, and then uses a class-imbalance Graph Convolution Network to refine the annotations of individual isoforms based on the isoform co-expression network and extracted features. Extensive experimental results show that DMIL-IsoFun improves the S min and F max of state-of-the-art solutions by at least 29.6% and 40.8%. The effectiveness of DMIL-IsoFun is further confirmed on a testbed of human multiple-isoform genes, and maize isoforms related with photosynthesis. Availability and implementation The code and data are available at http://www.sdu-idea.cn/codes.php?name=DMIL-Isofun. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154328735&site=ehost-live
262,Hub-based subspace clustering.,Carlotta Domeniconi,Neurocomputing,9252312,,Nov-20,413,,193,17,146412823,10.1016/j.neucom.2020.06.098,Elsevier B.V.,Article,NETWORK hubs; DATA mining; ALGORITHMS,Graph-based meta-features; Hubness; Selective sampling; Subspace clustering,"Data often exists in subspaces embedded within a high-dimensional space. Subspace clustering seeks to group data according to the dimensions relevant to each subspace. This requires the estimation of subspaces as well as the clustering of data. Subspace clustering becomes increasingly challenging in high dimensional spaces due to the curse of dimensionality which affects reliable estimations of distances and density. Recently, another aspect of high-dimensional spaces has been observed, known as the hubness phenomenon, whereby few data points appear frequently as nearest neighbors of the rest of the data. The distribution of neighbor occurrences becomes skewed with increasing intrinsic dimensionality of the data, and few points with high neighbor occurrences emerge as hubs. Hubs exhibit useful geometric properties and have been leveraged for clustering data in the full-dimensional space. In this paper, we study hubs in the context of subspace clustering. We present new characterizations of hubs in relation to subspaces, and design graph-based meta-features to identify a subset of hubs which are well fit to serve as seeds for the discovery of local latent subspaces and clusters. We propose and evaluate a hubness-driven algorithm to find subspace clusters, and show that our approach is superior to the baselines, and is competitive against state-of-the-art subspace clustering methods. We also identify the data characteristics that make hubs suitable for subspace clustering. Such characterization gives valuable guidelines to data mining practitioners. [ABSTRACT FROM AUTHOR] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146412823&site=ehost-live
263,Integrating multiple networks for protein function prediction.,Carlotta Domeniconi,BMC Systems Biology,17520509,,2015,9,1,1,11,101990706,10.1186/1752-0509-9-S1-S3,BioMed Central,Article,"PROTEINS; SYSTEMS biology; COMPUTATIONAL biology; METABOLOMICS; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",,"Background: High throughput techniques produce multiple functional association networks. Integrating these networks can enhance the accuracy of protein function prediction. Many algorithms have been introduced to generate a composite network, which is obtained as a weighted sum of individual networks. The weight assigned to an individual network reflects its benefit towards the protein functional annotation inference. A classifier is then trained on the composite network for predicting protein functions. However, since these techniques model the optimization of the composite network and the prediction tasks as separate objectives, the resulting composite network is not necessarily optimal for the follow-up protein function prediction. Results: We address this issue by modeling the optimization of the composite network and the prediction problems within a unified objective function. In particular, we use a kernel target alignment technique and the loss function of a network based classifier to jointly adjust the weights assigned to the individual networks. We show that the proposed method, called MNet, can achieve a performance that is superior (with respect to different evaluation criteria) to related techniques using the multiple networks of four example species (yeast, human, mouse, and fly) annotated with thousands (or hundreds) of GO terms. Conclusion: MNet can effectively integrate multiple networks for protein function prediction and is robust to the input parameters. Supplementary data is available at https://sites.google.com/site/guoxian85/home/mnet. The Matlab code of MNet is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101990706&site=ehost-live
264,Integrating multiple networks for protein function prediction.,Carlotta Domeniconi,BMC Systems Biology,17520509,,2015 Supplement,9,,1,11,103735921,10.1186/1752-0509-9-S1-S3,BioMed Central,Article,PROTEIN research; BIOMOLECULES; KERNEL (Mathematics); KERNEL functions; MATLAB (Computer software),,"Background: High throughput techniques produce multiple functional association networks. Integrating these networks can enhance the accuracy of protein function prediction. Many algorithms have been introduced to generate a composite network, which is obtained as a weighted sum of individual networks. The weight assigned to an individual network reflects its benefit towards the protein functional annotation inference. A classifier is then trained on the composite network for predicting protein functions. However, since these techniques model the optimization of the composite network and the prediction tasks as separate objectives, the resulting composite network is not necessarily optimal for the follow-up protein function prediction. Results: We address this issue by modeling the optimization of the composite network and the prediction problems within a unified objective function. In particular, we use a kernel target alignment technique and the loss function of a network based classifier to jointly adjust the weights assigned to the individual networks. We show that the proposed method, called MNet, can achieve a performance that is superior (with respect to different evaluation criteria) to related techniques using the multiple networks of four example species (yeast, human, mouse, and fly) annotated with thousands (or hundreds) of GO terms. Conclusion: MNet can effectively integrate multiple networks for protein function prediction and is robust to the input parameters. Supplementary data is available at https://sites.google.com/site/guoxian85/home/mnet. The Matlab code of MNet is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103735921&site=ehost-live
265,Isoform function prediction based on bi-random walks on a heterogeneous network.,Carlotta Domeniconi,Bioinformatics,13674803,,1/1/20,36,1,303,8,141288369,10.1093/bioinformatics/btz535,Oxford University Press / USA,Article,CANCER; GENE ontology; SPECIES diversity; HUMAN abnormalities; RNA splicing,,"Motivation Alternative splicing contributes to the functional diversity of protein species and the proteoforms translated from alternatively spliced isoforms of a gene actually execute the biological functions. Computationally predicting the functions of genes has been studied for decades. However, how to distinguish the functional annotations of isoforms, whose annotations are essential for understanding developmental abnormalities and cancers, is rarely explored. The main bottleneck is that functional annotations of isoforms are generally unavailable and functional genomic databases universally store the functional annotations at the gene level. Results We propose IsoFun to accomplish Isoform Function prediction based on bi-random walks on a heterogeneous network. IsoFun firstly constructs an isoform functional association network based on the expression profiles of isoforms derived from multiple RNA-seq datasets. Next, IsoFun uses the available Gene Ontology annotations of genes, gene–gene interactions and the relations between genes and isoforms to construct a heterogeneous network. After this, IsoFun performs a tailored bi-random walk on the heterogeneous network to predict the association between GO terms and isoforms, thus accomplishing the prediction of GO annotations of isoforms. Experimental results show that IsoFun significantly outperforms the state-of-the-art algorithms and improves the area under the receiver-operating curve (AUROC) and the area under the precision-recall curve (AUPRC) by 17% and 44% at the gene-level, respectively. We further validated the performance of IsoFun on the genes ADAM15 and BCL2L1. IsoFun accurately differentiates the functions of respective isoforms of these two genes. Availability and implementation The code of IsoFun is available at http://mlda.swu.edu.cn/codes.php? name=IsoFun. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141288369&site=ehost-live
266,Isoform function prediction by Gene Ontology embedding.,Carlotta Domeniconi,Bioinformatics,13674803,,10/1/22,38,19,4581,8,159436881,10.1093/bioinformatics/btac576,Oxford University Press / USA,Article,,,"Motivation High-resolution annotation of gene functions is a central task in functional genomics. Multiple proteoforms translated from alternatively spliced isoforms from a single gene are actual function performers and greatly increase the functional diversity. The specific functions of different isoforms can decipher the molecular basis of various complex diseases at a finer granularity. Multi-instance learning (MIL)-based solutions have been developed to distribute gene(bag)-level Gene Ontology (GO) annotations to isoforms(instances), but they simply presume that a particular annotation of the gene is responsible by only one isoform, neglect the hierarchical structures and semantics of massive GO terms (labels), or can only handle dozens of terms. Results We propose an efficacy approach IsofunGO to differentiate massive functions of isoforms by GO embedding. Particularly, IsofunGO first introduces an attributed hierarchical network to model massive GO terms, and a GO network embedding strategy to learn compact representations of GO terms and project GO annotations of genes into compressed ones, this strategy not only explores and preserves hierarchy between GO terms but also greatly reduces the prediction load. Next, it develops an attention-based MIL network to fuse genomics and transcriptomics data of isoforms and predict isoform functions by referring to compressed annotations. Extensive experiments on benchmark datasets demonstrate the efficacy of IsofunGO. Both the GO embedding and attention mechanism can boost the performance and interpretability. Availabilityand implementation The code of IsofunGO is available at http://www.sdu-idea.cn/codes.php?name=IsofunGO. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159436881&site=ehost-live
267,Large Margin Nearest Neighbor Classifiers.,Carlotta Domeniconi,IEEE Transactions on Neural Networks,10459227,,Jul-05,16,4,899,11,17658536,10.1109/TNN.2005.849821,IEEE,Article,PROBABILITY theory; ESTIMATES; MATHEMATICS; COMPUTER systems; TECHNOLOGY; STATISTICAL correlation; Computer systems design and related services (except video game design and development); Computer Systems Design Services,Feature relevance; margin; nearest neighbor classification; support vector machines (SVMs),"The nearest neighbor technique is a simple and appealing approach to addressing classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. The employment of a locally adaptive metric becomes crucial in order to keep class conditional probabilities close to uniform, thereby minimizing the bias of estimates. We propose a technique that computes a locally flexible metric by means of support vector machines (SVMs). The decision function constructed by SVMs is used to determine the most discriminant direction in a neighborhood around the query. Such a direction provides a local feature weighting scheme. We formally show that our method increases the margin in the weighted space where classification takes place. Moreover, our method has the important advantage of online computational efficiency over competing locally adaptive techniques for nearest neighbor classification. We demonstrate the efficacy of our method using both real and simulated data. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17658536&site=ehost-live
268,Matrix factorization-based data fusion for the prediction of lncRNA--disease associations.,Carlotta Domeniconi,Bioinformatics,13674803,,5/1/18,34,9,1529,9,129490337,10.1093/bioinformatics/btx794,Oxford University Press / USA,Article,RNA; MATRICES; DIAGNOSIS; BIOMEDICAL engineering; VECTOR analysis,,"Motivation: Long non-coding RNAs (lncRNAs) play crucial roles in complex disease diagnosis, prognosis, prevention and treatment, but only a small portion of lncRNA--disease associations have been experimentally verified. Various computational models have been proposed to identify lncRNA--disease associations by integrating heterogeneous data sources. However, existing models generally ignore the intrinsic structure of data sources or treat them as equally relevant, while they may not be. Results: To accurately identify lncRNA--disease associations, we propose a Matrix Factorization based LncRNA--Disease Association prediction model (MFLDA in short). MFLDA decomposes data matrices of heterogeneous data sources into low-rank matrices via matrix tri-factorization to explore and exploit their intrinsic and shared structure. MFLDA can select and integrate the data sources by assigning different weights to them. An iterative solution is further introduced to simultaneously optimize the weights and low-rank matrices. Next, MFLDA uses the optimized low-rank matrices to reconstruct the lncRNA--disease association matrix and thus to identify potential associations. In 5-fold cross validation experiments to identify verified lncRNA--disease associations, MFLDA achieves an area under the receiver operating characteristic curve (AUC) of 0.7408, at least 3% higher than those given by state-of-the-art data fusion based computational models. An empirical study on identifying masked lncRNA--disease associations again shows that MFLDA can identify potential associations more accurately than competing models. A case study on identifying lncRNAs associated with breast, lung and stomach cancers show that 38 out of 45 (84%) associations predicted by MFLDA are supported by recent biomedical literature and further proves the capability of MFLDA in identifying novel lncRNA--disease associations. MFLDA is a general data fusion framework, and as such it can be adopted to predict associations between other biological entities. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129490337&site=ehost-live
269,Multi-label zero-shot learning with graph convolutional networks.,Carlotta Domeniconi,Neural Networks,8936080,,Dec-20,132,,333,9,146953319,10.1016/j.neunet.2020.09.010,Elsevier B.V.,Article,LABELS; GRAPH labelings; KNOWLEDGE transfer; LEARNING; GLOBAL production networks; LEARNING goals; Commercial Printing (except Screen and Books); Other printing; Packaging and Labeling Services,Graph Convolutional Networks; Label correlations; Multi-label classification; Zero-shot learning,"The goal of zero-shot learning (ZSL) is to build a classifier that recognizes novel categories with no corresponding annotated training data. The typical routine is to transfer knowledge from seen classes to unseen ones by learning a visual-semantic embedding. Existing multi-label zero-shot learning approaches either ignore correlations among labels, suffer from large label combinations, or learn the embedding using only local or global visual features. In this paper, we propose a Graph Convolution Networks based Multi-label Zero-Shot Learning model, abbreviated as MZSL-GCN. Our model first constructs a label relation graph using label co-occurrences and compensates the absence of unseen labels in the training phase by semantic similarity. It then takes the graph and the word embedding of each seen (unseen) label as inputs to the GCN to learn the label semantic embedding, and to obtain a set of inter-dependent object classifiers. MZSL-GCN simultaneously trains another attention network to learn compatible local and global visual features of objects with respect to the classifiers, and thus makes the whole network end-to-end trainable. In addition, the use of unlabeled training data can reduce the bias toward seen labels and boost the generalization ability. Experimental results on benchmark datasets show that our MZSL-GCN competes with state-of-the-art approaches. [ABSTRACT FROM AUTHOR] Copyright of Neural Networks is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146953319&site=ehost-live
270,Multiview Multi-Instance Multilabel Active Learning.,Carlotta Domeniconi,IEEE Transactions on Neural Networks & Learning Systems,2162237X,,Sep-22,33,9,4311,11,158869777,10.1109/TNNLS.2021.3056436,IEEE,Article,PREDICTION models,Active learning; Biological systems; commonality and individuality; Compounds; Correlation; Drugs; Learning systems; multi-instance multilabel (MIML) learning; multiview learning; Semantics; Uncertainty,"Multiview multi-instance multilabel learning (M3L) is a framework for modeling complex objects. In this framework, each object (or bag) contains one or more instances, is represented with different feature views, and simultaneously annotated with a set of nonexclusive semantic labels. Given the multiplicity of the studied objects, traditional M3L methods generally demand a large number of labeled bags to train a predictive model to annotate bags (or instances) with semantic labels. However, annotating sufficient bags is very expensive and often impractical. In this article, we present an active learning-based M3L approach (M3AL) to reduce the labeling costs of bags and to improve the performance as much as possible. M3AL first adapts the multiview self-representation learning to evacuate the shared and individual information of bags and to learn the shared/individual similarities between bags across/within views. Next, to avoid scrutinizing all the possible labels, M3AL introduces a new query strategy that leverages the shared and individual information, and the diverse instance distribution of bags across views, to select the most informative bag-label pair for the query. Experimental studies on benchmark data sets show that M3AL can significantly reduce the query costs while achieving a better performance than other related competitive methods at the same cost. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks & Learning Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158869777&site=ehost-live
271,Parallel boosted clustering.,Carlotta Domeniconi,Neurocomputing,9252312,,Jul-19,351,,87,14,136417001,10.1016/j.neucom.2019.04.003,Elsevier B.V.,Article,SYSTEMS on a chip; PARALLEL algorithms; GRID cells; DATA distribution; DATA structures; SCALABILITY; Semiconductor and Related Device Manufacturing,Boosting; Clustering; Parallel computation; Stochastic optimization,"Scalability of clustering algorithms is a critical issue in real world clustering applications. Usually, data sampling and parallelization are two common ways to address the scalability issue. Despite their wide utilization in a number of clustering algorithms, they suffer from several major drawbacks. For example, most data sampling can often lead to biased solutions due to its inability in accurately capturing the distribution of the entire data set. On the other hand, the performance of parallelization highly depends on the original clustering routines which are not parallel algorithms in nature, such that customizing each algorithm to be parallel may hurt the clustering performance. To alleviate these problems, we propose a general two-step framework for scalable clustering in this work, where the first step is to obtain skeleton structure of data and the second step is to obtain the final clustering. Concretely, data are first partitioned and located across a two-dimensional grid, and then local clustering algorithms are iteratively applied on the cells of the grid, each providing a set of intermediate core points. These core points represent the dense or central regions of data, which can be centers, modes and means for centroid-based, density-based and probability-based clustering, respectively. Finally, these core points are further used to obtain the final clustering. The proposed framework enjoys several benefits: (1) the local clustering on partitioned cells are conducted in parallel and thus can lead to high speed-up; (2) the clustering on the representative core points can be more robust; (3) the framework can be easily applied to other basic clustering methods and thus achieves a general scalable solution. Theoretical analysis is provided and extensive experimental results have demonstrated the effectiveness and efficiency of the proposed framework. [ABSTRACT FROM AUTHOR] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136417001&site=ehost-live
272,Predicting protein function via downward random walks on a gene ontology.,Carlotta Domeniconi,BMC Bioinformatics,14712105,,Aug-15,16,1,1,13,109221121,10.1186/s12859-015-0713-y,BioMed Central,Article,PROTEIN analysis; RANDOM walks; GENE ontology; HUMAN proteins; PLANT protein analysis; PROTEOMICS; GENOMICS; YEAST; Other specialty-line food merchant wholesalers; All other food manufacturing; All Other Miscellaneous Food Manufacturing,Downward random walk; Function prediction; Gene ontology; Partially annotated proteins,"Background: High-throughput bio-techniques accumulate ever-increasing amount of genomic and proteomic data. These data are far from being functionally characterized, despite the advances in gene (or gene's product proteins) functional annotations. Due to experimental techniques and to the research bias in biology, the regularly updated functional annotation databases, i.e., the Gene Ontology (GO), are far from being complete. Given the importance of protein functions for biological studies and drug design, proteins should be more comprehensively and precisely annotated. Results: We proposed downward Random Walks (dRW) to predict missing (or new) functions of partially annotated proteins. Particularly, we apply downward random walks with restart on the GO directed acyclic graph, along with the available functions of a protein, to estimate the probability of missing functions. To further boost the prediction accuracy, we extend dRW to dRW-kNN. dRW-kNN computes the semantic similarity between proteins based on the functional annotations of proteins; it then predicts functions based on the functions estimated by dRW, together with the functions associated with the k nearest proteins. Our proposed models can predict two kinds of missing functions: (i) the ones that are missing for a protein but associated with other proteins of interest; (ii) the ones that are not available for any protein of interest, but exist in the GO hierarchy. Experimental results on the proteins of Yeast and Human show that dRW and dRW-kNN can replenish functions more accurately than other related approaches, especially for sparse functions associated with no more than 10 proteins. Conclusion: The empirical study shows that the semantic similarity between GO terms and the ontology hierarchy play important roles in predicting protein function. The proposed dRW and dRW-kNN can serve as tools for replenishing functions of partially annotated proteins. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109221121&site=ehost-live
273,Predicting protein functions using incomplete hierarchical labels.,Carlotta Domeniconi,BMC Bioinformatics,14712105,,2015,16,1,171,21,101019327,10.1186/s12859-014-0430-y,BioMed Central,Article,"BIOINFORMATICS; GENE ontology; EMPIRICAL research; BIOCHEMICAL models; BIOCHEMICAL genetics; Research and Development in Biotechnology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Combined similarity; Function prediction; Gene ontology; Incomplete hierarchical labels,"Background Protein function prediction is to assign biological or biochemical functions to proteins, and it is a challenging computational problem characterized by several factors: (1) the number of function labels (annotations) is large; (2) a protein may be associated with multiple labels; (3) the function labels are structured in a hierarchy; and (4) the labels are incomplete. Current predictive models often assume that the labels of the labeled proteins are complete, i.e. no label is missing. But in real scenarios, we may be aware of only some hierarchical labels of a protein, and we may not know whether additional ones are actually present. The scenario of incomplete hierarchical labels, a challenging and practical problem, is seldom studied in protein function prediction. Results In this paper, we propose an algorithm to Predict protein functions using Incomplete hierarchical LabeLs (PILL in short). PILL takes into account the hierarchical and the flat taxonomy similarity between function labels, and defines a Combined Similarity (ComSim) to measure the correlation between labels. PILL estimates the missing labels for a protein based on ComSim and the known labels of the protein, and uses a regularization to exploit the interactions between proteins for function prediction. PILL is shown to outperform other related techniques in replenishing the missing labels and in predicting the functions of completely unlabeled proteins on publicly available PPI datasets annotated with MIPS Functional Catalogue and Gene Ontology labels. Conclusion The empirical study shows that it is important to consider the incomplete annotation for protein function prediction. The proposed method (PILL) can serve as a valuable tool for protein function prediction using incomplete labels. The Matlab code of PILL is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101019327&site=ehost-live
274,Predicting protein functions using incomplete hierarchical labels.,Carlotta Domeniconi,BMC Bioinformatics,14712105,,2015,16,1,1,12,102597410,10.1186/s12859-014-0430-y,BioMed Central,Article,PROTEIN research; PREDICTION models; ALGORITHMS; TAXONOMY; MATHEMATICAL regularization; MATLAB (Computer software),Combined similarity; Function prediction; Gene ontology; Incomplete hierarchical labels,"Background: Protein function prediction is to assign biological or biochemical functions to proteins, and it is a challenging computational problem characterized by several factors: (1) the number of function labels (annotations) is large; (2) a protein may be associated with multiple labels; (3) the function labels are structured in a hierarchy; and (4) the labels are incomplete. Current predictive models often assume that the labels of the labeled proteins are complete, i.e. no label is missing. But in real scenarios, we may be aware of only some hierarchical labels of a protein, and we may not know whether additional ones are actually present. The scenario of incomplete hierarchical labels, a challenging and practical problem, is seldom studied in protein function prediction. Results: In this paper, we propose an algorithm to Predict protein functions using Incomplete hierarchical LabeLs (PILL in short). PILL takes into account the hierarchical and the flat taxonomy similarity between function labels, and defines a Combined Similarity (ComSim) to measure the correlation between labels. PILL estimates the missing labels for a protein based on ComSim and the known labels of the protein, and uses a regularization to exploit the interactions between proteins for function prediction. PILL is shown to outperform other related techniques in replenishing the missing labels and in predicting the functions of completely unlabeled proteins on publicly available PPI datasets annotated with MIPS Functional Catalogue and Gene Ontology labels. Conclusion: The empirical study shows that it is important to consider the incomplete annotation for protein function prediction. The proposed method (PILL) can serve as a valuable tool for protein function prediction using incomplete labels. The Matlab code of PILL is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=102597410&site=ehost-live
275,Reducing Ensembles of Protein Tertiary Structures Generated De Novo via Clustering.,Carlotta Domeniconi,Molecules,14203049,,May-20,25,9,2228,1,143318659,10.3390/molecules25092228,MDPI,Article,TERTIARY structure; SOURCE code; SPACETIME; FORECASTING; QUALITY control,clustering; decoy ensemble; protein structure prediction; reduction; tertiary structure,"Controlling the quality of tertiary structures computed for a protein molecule remains a central challenge in de-novo protein structure prediction. The rule of thumb is to generate as many structures as can be afforded, effectively acknowledging that having more structures increases the likelihood that some will reside near the sought biologically-active structure. A major drawback with this approach is that computing a large number of structures imposes time and space costs. In this paper, we propose a novel clustering-based approach which we demonstrate to significantly reduce an ensemble of generated structures without sacrificing quality. Evaluations are related on both benchmark and CASP target proteins. Structure ensembles subjected to the proposed approach and the source code of the proposed approach are publicly-available at the links provided in Section 1. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143318659&site=ehost-live
275,Reducing Ensembles of Protein Tertiary Structures Generated De Novo via Clustering.,Amarda Shehu,Molecules,14203049,,May-20,25,9,2228,1,143318659,10.3390/molecules25092228,MDPI,Article,TERTIARY structure; SOURCE code; SPACETIME; FORECASTING; QUALITY control,clustering; decoy ensemble; protein structure prediction; reduction; tertiary structure,"Controlling the quality of tertiary structures computed for a protein molecule remains a central challenge in de-novo protein structure prediction. The rule of thumb is to generate as many structures as can be afforded, effectively acknowledging that having more structures increases the likelihood that some will reside near the sought biologically-active structure. A major drawback with this approach is that computing a large number of structures imposes time and space costs. In this paper, we propose a novel clustering-based approach which we demonstrate to significantly reduce an ensemble of generated structures without sacrificing quality. Evaluations are related on both benchmark and CASP target proteins. Structure ensembles subjected to the proposed approach and the source code of the proposed approach are publicly-available at the links provided in Section 1. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143318659&site=ehost-live
276,Theoretical and Empirical Analysis of a Spatial EA Parallel Boosting Algorithm.,Carlotta Domeniconi,Evolutionary Computation,10636560,,Spring2018,26,1,43,24,128237118,10.1162/evco_a_00202,MIT Press,Article,EVOLUTIONARY algorithms; EVOLUTIONARY computation; COMPUTER programming; GENETIC algorithms; NUMERICAL analysis; Computer systems design and related services (except video game design and development); Custom Computer Programming Services; Other Computer Related Services,large margin classifiers; machine learning.; parallel boosting; scalability; Spatial evolutionary algorithms,"Many real-world problems involve massive amounts of data. Under these circumstances learning algorithms often become prohibitively expensive, making scalability a pressing issue to be addressed. A common approach is to perform sampling to reduce the size of the dataset and enable efficient learning. Alternatively, one customizes learning algorithms to achieve scalability. In either case, the key challenge is to obtain algorithmic efficiency without compromising the quality of the results. In this article we discuss a meta-learning algorithm (PSBML) that combines concepts from spatially structured evolutionary algorithms (SSEAs) with concepts from ensemble and boosting methodologies to achieve the desired scalability property. We present both theoretical and empirical analyses which show that PSBML preserves a critical property of boosting, specifically, convergence to a distribution centered around the margin. We then present additional empirical analyses showing that this meta-level algorithm provides a general and effective framework that can be used in combination with a variety of learning classifiers. We perform extensive experiments to investigate the trade-off achieved between scalability and accuracy, and robustness to noise, on both synthetic and real-world data. These empirical results corroborate our theoretical analysis, and demonstrate the potential of PSBML in achieving scalability without sacrificing accuracy. [ABSTRACT FROM AUTHOR] Copyright of Evolutionary Computation is the property of MIT Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128237118&site=ehost-live
277,Active Multilabel Crowd Consensus.,Carlotta Domeniconi,IEEE Transactions on Neural Networks & Learning Systems,2162237X,,Apr-21,32,14,1448,12,149773418,10.1109/TNNLS.2020.2984729,IEEE,Article,CONSENSUS (Social sciences); ACTIVE learning; LABELS; LEARNING strategies; CROWDS; CROWDSOURCING; Packaging and Labeling Services; Commercial Printing (except Screen and Books); Other printing,Active learning (AL); Biological system modeling; Computational modeling; Correlation; cost; Crowdsourcing; Learning systems; multilabel crowd consensus; Reliability; specialty and commonality; Task analysis,"Crowdsourcing is an economic and efficient strategy aimed at collecting annotations of data through an online platform. Crowd workers with different expertise are paid for their service, and the task requester usually has a limited budget. How to collect reliable annotations for multilabel data and how to compute the consensus within budget are an interesting and challenging, but rarely studied, problem. In this article, we propose a novel approach to accomplish active multilabel crowd consensus (AMCC). AMCC accounts for the commonality and individuality of workers and assumes that workers can be organized into different groups. Each group includes a set of workers who share a similar annotation behavior and label correlations. To achieve an effective multilabel consensus, AMCC models workers’ annotations via a linear combination of commonality and individuality and reduces the impact of unreliable workers by assigning smaller weights to their groups. To collect reliable annotations with reduced cost, AMCC introduces an active crowdsourcing learning strategy that selects sample–label–worker triplets. In a triplet, the selected sample and label are the most informative for the consensus model, and the selected worker can reliably annotate the sample at a low cost. Our experimental results on multilabel data sets demonstrate the advantages of AMCC over state-of-the-art solutions on computing crowd consensus and on reducing the budget by choosing cost-effective triplets. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks & Learning Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149773418&site=ehost-live
278,Attributed heterogeneous network fusion via collaborative matrix tri-factorization.,Carlotta Domeniconi,Information Fusion,15662535,,Nov-20,63,,153,13,145698744,10.1016/j.inffus.2020.06.012,Elsevier B.V.,Article,LOW-rank matrices; MATRIX decomposition; MULTISENSOR data fusion; LINEAR network coding; MATRICES; PANCREATIC cancer,Attributed heterogeneous networks; Data fusion; Insufficient relations; LncRNA-disease associations; Matrix factorization,"• A comprehensive data fusion model for attributed multi-relational data is proposed. • AHNF conquers the negative impact of insufficient relations between network nodes. • AHNF avoids the loss when converting attributes into homo-networks for fusion. • AHNF can selectively integrate diverse relational and attribute data sources. • AHNF outperforms state-of-the-art matrix factorization based data fusion solutions. Heterogeneous network based data fusion can encode diverse inter- and intra-relations between objects, and has been sparking increasing attention in recent years. Matrix factorization based data fusion models have been invented to fuse multiple data sources. However, these models generally suffer from the widely-witnessed insufficient relations between nodes and from information loss when heterogeneous attributes of diverse network nodes are transformed into ad-hoc homologous networks for fusion. In this paper, we introduce a general data fusion model called Attributed Heterogeneous Network Fusion (AHNF). AHNF firstly constructs an attributed heterogeneous network composed with different types of nodes and the diverse attribute vectors of these nodes. It uses indicator matrices to differentiate the observed inter-relations from the latent ones, and thus reduces the impact of insufficient relations between nodes. Next, it collaboratively factorizes multiple adjacency matrices and attribute data matrices of the heterogeneous network into low-rank matrices to explore the latent relations between these nodes. In this way, both the network topology and diverse attributes of nodes are fused in a coordinated fashion. Finally, it uses the optimized low-rank matrices to approximate the target relational data matrix of objects and to effectively accomplish the relation prediction. We apply AHNF to predict the lncRNA-disease associations using diverse relational and attribute data sources. AHNF achieves a larger area under the receiver operating curve 0.9367 (by at least 2.14%), and a larger area under the precision-recall curve 0.5937 (by at least 28.53%) than competitive data fusion approaches. AHNF also outperforms competing methods on predicting de novo lncRNA-disease associations, and precisely identifies lncRNAs associated with breast, stomach, prostate, and pancreatic cancers. AHNF is a comprehensive data fusion framework for universal attributed multi-type relational data. The code and datasets are available at http://mlda.swu.edu.cn/codes.php?name=AHNF. [ABSTRACT FROM AUTHOR] Copyright of Information Fusion is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145698744&site=ehost-live
279,Co-Clustering Ensembles Based on Multiple Relevance Measures.,Carlotta Domeniconi,IEEE Transactions on Knowledge & Data Engineering,10414347,,Apr-21,33,4,1389,12,149122312,10.1109/TKDE.2019.2942029,IEEE,Article,MATRIX multiplications; INFORMATION resources; MATHEMATICAL optimization; WEIGHTS & measures,Bipartite graph; Co-clustering; co-clustering ensemble; Fuses; Gene expression; Minimization; multiple relevance measures; Optimization; Robustness; Runtime; trace minimization,"Co-clustering aims at discovering groups of both objects and features from a given data matrix. Co-clustering ensembles can produce robust co-clusters by combining multiple base co-clusterings. However, current co-clustering ensemble solutions either ignore the constraints resulting from feature-to-feature and object-to-object relevance information, or ignore feature-to-object relevance information. In this paper, we advocate that all three information sources contribute to the achievement of good consensus solutions, and propose a co-clustering ensemble (CoCE) approach based on multiple relevance measures. CoCE first evaluates the quality of base co-clusters and consequently measures feature-to-object relevance. The latter, along with feature-to-feature and object-to-object relevance measures, contribute to the definition of a hybrid graph. The consensus process uses the resulting hybrid graph; it's formulated as a trace minimization problem and introduces a block-wise matrix multiplication technique to perform the optimization. Experimental results on various datasets show that CoCE not only frequently outperforms other related co-clustering ensembles, but also has reduced runtime cost and is more robust to poor base co-clusterings. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149122312&site=ehost-live
280,Flexible Cross-Modal Hashing.,Carlotta Domeniconi,IEEE Transactions on Neural Networks & Learning Systems,2162237X,,Jan-22,33,1,304,11,154800821,10.1109/TNNLS.2020.3027729,IEEE,Article,MATHEMATICAL optimization; INFORMATION retrieval; LINEAR programming,Clustering-based match; Correlation; cross-modal hashing; flexibility; Hash functions; Linear programming; optimization; Semantics; STEM; Training; weakly paired data,"Hashing has been widely adopted for large-scale data retrieval in many domains due to its low storage cost and high retrieval speed. Existing cross-modal hashing methods optimistically assume that the correspondence between training samples across modalities is readily available. This assumption is unrealistic in practical applications. In addition, existing methods generally require the same number of samples across different modalities, which restricts their flexibility. We propose a flexible cross-modal hashing approach (FlexCMH) to learn effective hashing codes from weakly paired data, whose correspondence across modalities is partially (or even totally) unknown. FlexCMH first introduces a clustering-based matching strategy to explore the structure of each cluster and, thus, to find the potential correspondence between clusters (and samples therein) across modalities. To reduce the impact of an incomplete correspondence, it jointly optimizes the potential correspondence, the cross-modal hashing functions derived from the correspondence, and a hashing quantitative loss in a unified objective function. An alternative optimization technique is also proposed to coordinate the correspondence and hash functions and reinforce the reciprocal effects of the two objectives. Experiments on public multimodal data sets show that FlexCMH achieves significantly better results than state-of-the-art methods, and it, indeed, offers a high degree of flexibility for practical cross-modal hashing tasks. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks & Learning Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154800821&site=ehost-live
281,Kernel Pooled Local Subspaces for Classification.,Carlotta Domeniconi,"IEEE Transactions on Systems, Man & Cybernetics: Part B",10834419,,Jun-05,35,3,489,14,17176860,10.1109/TSMCB.2005.846641,IEEE,Article,STATISTICAL correlation; KERNEL functions; FUNCTIONAL analysis; LEARNING; DISCRIMINANT analysis; MULTIVARIATE analysis,,"We investigate the use of subspace analysis methods for learning low-dimensional representations for classification. We propose a kernel-pooled local discriminant subspace method and compare it against competing techniques: kernel principal component analysis (KPCA) and generalized discriminant analysis (GDA) in classification problems. We evaluate the classification performance of the nearest-neighbor rule with each subspace representation. The experimental results using several data sets demonstrate the effectiveness and performance superiority of the kernel-pooled subspace method over competing methods such as KPCA and GDA in some classification problems. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part B is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17176860&site=ehost-live
282,Semi-supervised classification based on random subspace dimensionality reduction,Carlotta Domeniconi,Pattern Recognition,313203,,Mar-12,45,3,1119,17,66944275,10.1016/j.patcog.2011.08.024,Elsevier B.V.,Article,SUPERVISED learning; PATTERN recognition systems; INVARIANT subspaces; DIMENSION reduction (Statistics); GRAPHIC methods; EXPERIMENTS,Dimensionality reduction; Ensembles of classifiers; Graph construction; Random subspaces; Semi-supervised classification,"Abstract: Graph structure is vital to graph based semi-supervised learning. However, the problem of constructing a graph that reflects the underlying data distribution has been seldom investigated in semi-supervised learning, especially for high dimensional data. In this paper, we focus on graph construction for semi-supervised learning and propose a novel method called Semi-Supervised Classification based on Random Subspace Dimensionality Reduction, SSC-RSDR in short. Different from traditional methods that perform graph-based dimensionality reduction and classification in the original space, SSC-RSDR performs these tasks in subspaces. More specifically, SSC-RSDR generates several random subspaces of the original space and applies graph-based semi-supervised dimensionality reduction in these random subspaces. It then constructs graphs in these processed random subspaces and trains semi-supervised classifiers on the graphs. Finally, it combines the resulting base classifiers into an ensemble classifier. Experimental results on face recognition tasks demonstrate that SSC-RSDR not only has superior recognition performance with respect to competitive methods, but also is robust against a wide range of values of input parameters. [Copyright &y& Elsevier] Copyright of Pattern Recognition is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=66944275&site=ehost-live
283,Weighted matrix factorization on multi-relational data for LncRNA-disease association prediction.,Carlotta Domeniconi,Methods,10462023,,Feb-20,173,,32,12,142144930,10.1016/j.ymeth.2019.06.015,Academic Press Inc.,Article,MATRIX decomposition; LOW-rank matrices; DATA fusion (Statistics); MULTISENSOR data fusion; NON-coding RNA,Data fusion; LncRNA-disease associations; Matrix factorization; Multiple heterogeneous networks,"• We proposed a weighted matrix factorization based data fusion approach to predict associations between lncRNAs and diseases. • Our approach can selectively integrate multiple inter-relational and intra-relational data sources. • Our approach can explore and exploit the intrinsic and shared structure of heterogeneous data sources. • Our approach is readily available for various link prediction problems. Influx evidences show that red long non-coding RNAs (lncRNAs) play important roles in various critical biological processes, and they afffect the development and progression of various human diseases. Therefore, it is necessary to precisely identify the lncRNA-disease associations. The identification precision can be improved by developing data integrative models. However, current models mainly need to project heterogeneous data onto the homologous networks, and then merge these networks into a composite one for integrative prediction. We recognize that this projection overrides the individual structure of the heterogeneous data, and the combination is impacted by noisy networks. As a result, the performance is compromised. Given that, we introduce a weighted matrix factorization model on multi-relational data to predict LncRNA-disease associations (WMFLDA). WMFLDA firstly uses a heterogeneous network to capture the inter(intra)-associations between different types of nodes (including genes, lncRNAs, and Disease Ontology terms). Then, it presets weights to these inter-association and intra-association matrices of the network, and cooperatively decomposes these matrices into low-rank ones to explore the underlying relationships between nodes. Next, it jointly optimizes the low-rank matrices and the weights. After that, WMFLDA approximates the lncRNA-disease association matrix using the optimized matrices and weights, and thus to achieve the prediction. WMFLDA obtains a much better performance than related data integrative solutions across different experiment settings and evaluation metrics. It can not only respect the intrinsic structures of individual data sources, but can also fuse them with selection. [ABSTRACT FROM AUTHOR] Copyright of Methods is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142144930&site=ehost-live
284,A heuristic scheme for multivariate set partitioning problems with application to classifying heterogeneous populations for multiple binary attributes.,Hadi El-Amine,IISE Transactions,24725854,,Jun-22,54,6,537,13,156075381,10.1080/24725854.2021.1959964,Taylor & Francis Ltd,Article,SEXUALLY transmitted diseases; POLYNOMIAL time algorithms; HEURISTIC; MEDICAL screening; ALGORITHMS; UNITED States; All Other Miscellaneous Ambulatory Health Care Services,combinatorial optimization; constrained shortest path; group testing; polynomial-time algorithm; Set partitioning problem,"We provide a novel heuristic approach to solve a class of multivariate set partitioning problems in which each item is characterized by three attribute values. The scheme first identifies a series of orderings of the items and then solves a corresponding sequence of shortest path problems. We provide theoretical findings on the structure of an optimal solution that motivate the design of the proposed heuristic scheme. The proposed algorithm runs in polynomial-time and is independent of the number of groups in the partition, making it more efficient than existing algorithms. To measure the performance of our solutions, we construct bounds for special instances which allow us to provide optimality gaps. We conduct an extensive numerical experiment in which we solve a large number of problem instances and show that our proposed approach converges to the global optimal solution in the vast majority of cases and in the case it does not, it yields very low optimality gaps. We demonstrate our findings with an application in the context of classifying a large heterogeneous population as positive or negative for multiple binary attributes as efficiently as possible. We conduct a case study on the screening of three of the most prevalent sexually transmitted diseases in the United States. The resulting solutions are shown to be within 2.6% of optimality and lead to a 26% cost saving over current screening practices. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156075381&site=ehost-live
285,An Exact Algorithm for Large-Scale Continuous Nonlinear Resource Allocation Problems with Minimax Regret Objectives.,Hadi El-Amine,INFORMS Journal on Computing,10919856,,Summer2021,33,3,1213,16,152160091,10.1287/ijoc.2020.0999,INFORMS: Institute for Operations Research,Article,RESOURCE allocation; DIFFERENTIABLE functions; ALGORITHMS; ROBUST optimization,Benders-type decomposition; continuous nonlinear optimization; interval minimax regret; robust optimization,"We study a large-scale resource allocation problem with a convex, separable, not necessarily differentiable objective function that includes uncertain parameters falling under an interval uncertainty set, considering a set of deterministic constraints. We devise an exact algorithm to solve the minimax regret formulation of this problem, which is NP-hard, and we show that the proposed Benders-type decomposition algorithm converges to an ɛ -optimal solution in finite time. We evaluate the performance of the proposed algorithm via an extensive computational study, and our results show that the proposed algorithm provides efficient solutions to large-scale problems, especially when the objective function is differentiable. Although the computation time takes longer for problems with nondifferentiable objective functions as expected, we show that good quality, near-optimal solutions can be achieved in shorter runtimes by using our exact approach. We also develop two heuristic approaches, which are partially based on our exact algorithm, and show that the merit of the proposed exact approach lies in both providing an ɛ -optimal solution and providing good quality near-optimal solutions by laying the foundation for efficient heuristic approaches. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152160091&site=ehost-live
286,Optimal clustering of frequency data with application to disease risk categorization.,Hadi El-Amine,IISE Transactions,24725854,,Aug-22,54,8,728,13,157056506,10.1080/24725854.2021.1973158,Taylor & Francis Ltd,Article,STATISTICS; DISTRIBUTION (Probability theory); DATA distribution; COMBINATORIAL analysis; MEDICAL screening; HIV testing kits; All Other Miscellaneous Ambulatory Health Care Services; Pharmaceutical and medicine manufacturing; In-Vitro Diagnostic Substance Manufacturing,Cluster analysis; constrained shortest path; disease risk categorization; hierarchical clustering; set partitioning problem,"We provide a clustering procedure for a special type of dataset, known as frequency data, which counts the frequency of a certain binary outcome. An interpretation of the data as a discrete distribution enables us to extract statistical information, which we embed within an optimization-based framework. Our analysis of the resulting combinatorial optimization problem allows us to reformulate it as a more tractable network flow problem. This, in turn, enables the construction of exact algorithms that converge to the optimal solution in quadratic time. In addition, to be able to handle large-scale datasets, we provide two hierarchical heuristic algorithms that run in linearithmic time. Our moment-based method results in clustering solutions that are shown to perform well for a family of applications. We illustrate the benefits of our findings through a case study on HIV risk categorization within the context of large-scale screening through group testing. Our results on CDC data show that the proposed clustering framework consistently outperforms other popular clustering methods. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157056506&site=ehost-live
287,Optimal Screening of Populations with Heterogeneous Risk Profiles Under the Availability of Multiple Tests.,Hadi El-Amine,INFORMS Journal on Computing,10919856,,Winter2022,34,1,150,15,155827900,10.1287/ijoc.2020.1051,INFORMS: Institute for Operations Research,Article,MEDICAL screening; POLYNOMIAL time algorithms; COMBINATORIAL optimization; UNITED States; All Other Miscellaneous Ambulatory Health Care Services,group testing; heterogeneous population; multiple tests; network flow; polynomial time algorithm,"We study the design of large-scale group testing schemes under a heterogeneous population (i.e., subjects with potentially different risk) and with the availability of multiple tests. The objective is to classify the population as positive or negative for a given binary characteristic (e.g., the presence of an infectious disease) as efficiently and accurately as possible. Our approach examines components often neglected in the literature, such as the dependence of testing cost on the group size and the possibility of no testing, which are especially relevant within a heterogeneous setting. By developing key structural properties of the resulting optimization problem, we are able to reduce it to a network flow problem under a specific, yet not too restrictive, objective function. We then provide results that facilitate the construction of the resulting graph and finally provide a polynomial time algorithm. Our case study, on the screening of HIV in the United States, demonstrates the substantial benefits of the proposed approach over conventional screening methods. Summary of Contribution: This paper studies the problem of testing heterogeneous populations in groups in order to reduce costs and hence allow for the use of more efficient tests for high-risk groups. The resulting problem is a difficult combinatorial optimization problem that is NP-complete under a general objective. Using structural properties specific to our objective function, we show that the problem can be cast as a network flow problem and provide a polynomial time algorithm. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155827900&site=ehost-live
288,An EM Algorithm for Ion-Channel Current Estimation.,Yariv Ephraim,IEEE Transactions on Signal Processing,1053587X,,Jan-08,56,1,26,8,28153258,10.1109/TSP.2007.906743,IEEE,Article,EXPECTATION-maximization algorithms; PARAMETER estimation; MARKOV processes; ION channels; MAXIMUM likelihood statistics; CELL membranes,Ion-channel; Markov modulated process,"Parameter estimation of a continuous-time Markov chain observed through a discrete-time memoryless channel is studied. An expectation-maximization (EM) algorithm for maximum likelihood estimation of the parameter of this hidden Markov process is developed and applied to a simple example of modeling ion-channel currents in living cell membranes. The approach follows that of Asmussen, Nerman and Olsson, and Rydén, for EM estimation of an underlying continuous-time Markov chain. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Signal Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=28153258&site=ehost-live
289,An EM Algorithm for Markov Modulated Markov Processes.,Yariv Ephraim,IEEE Transactions on Signal Processing,1053587X,,Feb-09,57,2,463,8,39060423,10.1109/TSP.2008.2007919,IEEE,Article,EXPECTATION-maximization algorithms; ALGORITHMS; NUMERICAL solutions for Markov processes; STOCHASTIC processes; EXCESSIVE measures (Mathematics); FINITE element method; POISSON'S equation; POISSON processes,Expectation-maximization (EM) algorithm; Markov modulated Markov processes; Markov modulated Poisson processes,"An expectation-maximization (EM) algorithm for estimating the parameter of a Markov modulated Markov process in the maximum likelihood sense is developed. This is a doubly stochastic random process with an underlying continuous-time finite-state homogeneous Markov chain. Conditioned on that chain, the observable process is a continuous-time finite-state nonhomogeneous Markov chain. The generator of the observable process at any given time is determined by the state of the under- lying Markov chain at that time. The parameter of the process comprises the set of generators for the underlying and conditional Markov chains. The proposed approach generalizes an earlier approach by Rydén for estimating the parameter of a Markov modulated Poisson process. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Signal Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=39060423&site=ehost-live
290,Explicit Causal Recursive Estimators for Continuous-Time Bivariate Markov Chains.,Brian Mark,IEEE Transactions on Signal Processing,1053587X,,May-14,62,10,2709,10,95801785,10.1109/TSP.2014.2314434,IEEE,Article,MARKOV processes; STOCHASTIC processes; STATISTICS; RADON; NUMERICAL integration,Educational institutions; Estimation; Hidden Markov models; Markov processes; Materials; Random processes; Real-time systems; recursive estimation,"A bivariate Markov chain comprises a pair of random processes which are jointly Markov. In this paper, both processes are assumed to be continuous-time with finite state space. One of the two processes is observable, while the other is an underlying process which affects the statistical properties of the observable process. Neither the observable, nor the underlying process , is required to be a Markov chain. Examples of bivariate Markov chains include the Markov modulated Markov process (MMMP), the Markov modulated Poisson process (MMPP), and the batch Markovian arrival process (BMAP). We develop explicit causal recursions for estimating the number of jumps from one state to another, and the total sojourn time in each state, of a general bivariate Markov chain. Explicit causal recursions of these statistics were previously developed for the MMMP and the MMPP using the transformation of measure approach. We argue that this approach cannot be extended to a general bivariate Markov chain. Instead, we modify the approach developed by Rydén for noncausal estimation of the same statistics of an MMPP, and use the state augmentation approach of Zeitouni and Dembo and a matrix recursion from Stiller and Radons, to derive the causal recursions. The new recursions do not require any numerical integration or sampling scheme of the continuous-time bivariate Markov chain. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Signal Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95801785&site=ehost-live
290,Explicit Causal Recursive Estimators for Continuous-Time Bivariate Markov Chains.,Yariv Ephraim,IEEE Transactions on Signal Processing,1053587X,,May-14,62,10,2709,10,95801785,10.1109/TSP.2014.2314434,IEEE,Article,MARKOV processes; STOCHASTIC processes; STATISTICS; RADON; NUMERICAL integration,Educational institutions; Estimation; Hidden Markov models; Markov processes; Materials; Random processes; Real-time systems; recursive estimation,"A bivariate Markov chain comprises a pair of random processes which are jointly Markov. In this paper, both processes are assumed to be continuous-time with finite state space. One of the two processes is observable, while the other is an underlying process which affects the statistical properties of the observable process. Neither the observable, nor the underlying process , is required to be a Markov chain. Examples of bivariate Markov chains include the Markov modulated Markov process (MMMP), the Markov modulated Poisson process (MMPP), and the batch Markovian arrival process (BMAP). We develop explicit causal recursions for estimating the number of jumps from one state to another, and the total sojourn time in each state, of a general bivariate Markov chain. Explicit causal recursions of these statistics were previously developed for the MMMP and the MMPP using the transformation of measure approach. We argue that this approach cannot be extended to a general bivariate Markov chain. Instead, we modify the approach developed by Rydén for noncausal estimation of the same statistics of an MMPP, and use the state augmentation approach of Zeitouni and Dembo and a matrix recursion from Stiller and Radons, to derive the causal recursions. The new recursions do not require any numerical integration or sampling scheme of the continuous-time bivariate Markov chain. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Signal Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95801785&site=ehost-live
292,"Disparities in knowledge, attitude, and practices of infection prevention and control of Lassa fever among health care workers at The Federal Medical Centre, Owo, Ondo State, Nigeria.",Tokunbo Fadahunsi,Pan African Medical Journal,19378688,,Jan-Apr2021,38,,1,17,152718061,10.11604/pamj.2021.38.357.26208,Pan African Medical Journal,Article,MEDICAL personnel; LASSA fever; INFECTION prevention; INFECTION control; ATTITUDE (Psychology); LABORATORY personnel; NIGERIA,health workers; knowledge; Lassa fever; Nigeria; prevention,"Introduction: the knowledge and practices on Lassa fever (LF) infection prevention and control (IPC) remains poor among health workers in Nigeria despite LF endemicity. This study aimed to evaluate the knowledge, attitude, and practices of healthcare workers at the Federal Medical Centre, Owo towards LF. Methods: this was a cross- sectional study among 451 healthcare workers who were enrolled using a simple random sampling technique. Data were collected using a semi- structured interviewer-administered questionnaire and analyzed with SPSS version 23. Adequate knowledge, positive attitude, and good practice of LF infection, prevention, and control were determined by the proportion of respondents who scored >80% in each category. Descriptive statistics were done. Associations were explored using Chi- square tests. Results: the mean age of respondents was 37.95±8.43 years, and 169 (37.5%) were doctors. The mean overall knowledge score was 18.33±2.14, and 236 (52.3%) had appropriate knowledge, 109 (24.2%) had a positive attitude, while 351 (77.8%) demonstrated adequate preventive practices towards LFIPC. Laboratory scientists had five times the odds of appropriate knowledge of LF IPC (OR=4.886; 95%CI: 1.580- 15.107). Pharmacists had ten times odds of positive attitude towards LF IPC (OR=10.093; 95%CI= 1.055- 95.516). Pharmacists had nine times odds of good LF IPC practices (OR=8.755; 95%CI=1.028-74.531). Conclusion: disparities in knowledge, attitude, and practices of LF IPC exist among healthcare workers. To strengthen IPC, intervention strategies like training to address such gaps are needed. [ABSTRACT FROM AUTHOR] Copyright of Pan African Medical Journal is the property of Pan African Medical Journal and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152718061&site=ehost-live
293,A methodology for ensuring fair allocation of CSOC effort for alert investigation.,Rajesh Ganesan,International Journal of Information Security,16155262,,Apr-19,18,2,199,20,135326761,10.1007/s10207-018-0407-3,Springer Nature,Article,METHODOLOGY; INTERNET security; SERVICE level agreements; INTERNET traffic; COMPUTER simulation; Internet Publishing and Broadcasting and Web Search Portals; Wired Telecommunications Carriers,Adaptive queueing; Cybersecurity analysts; Deficit model; Dynamic weighted queueing; Fair CSOC effort allocation; Integrated queueing approach; Simulation,"A Cyber Security Operations Center (CSOC) often sells services by entering into a service level agreement (SLA) with various customers (organizations) whose network traffic is monitored through sensors. The sensors produce data that are processed by automated systems (such as the intrusion detection system) that issue alerts. All alerts need further investigation by human analysts. The alerts are triaged into high-, medium-, and low-priority alerts, and the high-priority alerts are investigated first by cybersecurity analysts—a process known as priority queueing. In unexpected situations such as (i) higher than expected high-priority alert generation from some sensors, (ii) not enough analysts at the CSOC in a given time interval, and (iii) a new type of alert, which increases the time to analyze alerts from some sensors, the priority queueing mechanism leads to two major issues. The issues are: (1) some sensors with normal levels of alert generation are being analyzed less than those with excessive high-priority alerts, with the potential for complete starvation of alert analysis for sensors with only medium- or low-priority alerts, and (2) the above ad hoc allocation of CSOC effort to sensors with excessive high-priority alerts over other sensors results in SLA violations, and there is no enforcement mechanism to ensure the matching between the SLA and the actual service provided by a CSOC. This paper develops a new dynamic weighted alert queueing mechanism (DWQ) which relates the CSOC effort as per SLA to the actual allocated in practice, and ensures via a technical enforcement system that the total CSOC effort is proportionally divided among customers such that fairness is guaranteed in the long run. The results indicate that the DWQ mechanism outperforms priority queueing method by not only analyzing high-priority alerts first but also ensuring fairness in CSOC effort allocated to all its customers and providing a starvation-free alert investigation process. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Information Security is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135326761&site=ehost-live
294,A methodology to measure and monitor level of operational effectiveness of a CSOC.,Rajesh Ganesan,International Journal of Information Security,16155262,,Apr-18,17,2,121,14,128214788,10.1007/s10207-017-0365-1,Springer Nature,Article,INTRUSION detection systems (Computer security); INTERNET security; JOB absenteeism; SYSTEM downtime; COMPUTER worms; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,Cybersecurity operations center; Intrusion detection; Level of operational effectiveness; Situational awareness of CSOC; Total time for alert investigation,"In a cybersecurity operations center (CSOC), under normal operating conditions in a day, sufficient numbers of analysts are available to analyze the amount of alert workload generated by intrusion detection systems (IDSs). For the purpose of this paper, this means that the cybersecurity analysts can fully investigate each and every alert that is generated by the IDSs in a reasonable amount of time. However, there are a number of disruptive factors that can adversely impact the normal operating conditions such as (1) higher alert generation rates from a few IDSs, (2) new alert patterns that decreases the throughput of the alert analysis process, and (3) analyst absenteeism. The impact of all the above factors is that the alerts wait for a long duration before being analyzed, which impacts the readiness of the CSOC. It is imperative that the readiness of the CSOC be quantified, which in this paper is defined as the level of operational effectiveness (LOE) of a CSOC. LOE can be quantified and monitored by knowing the exact deviation of the CSOC conditions from normal and how long it takes for the condition to return to normal. In this paper, we quantify LOE by defining a new metric called total time for alert investigation (TTA), which is the sum of the waiting time in the queue and the analyst investigation time of an alert after its arrival in the CSOC database. A dynamic TTA monitoring framework is developed in which a nominal average TTA per hour (avgTTA/hr) is established as the baseline for normal operating condition using individual TTA of alerts that were investigated in that hour. At the baseline value of avgTTA/hr, LOE is considered to be <italic>ideal</italic>. Also, an upper-bound (threshold) value for avgTTA/hr is established, below which the LOE is considered to be <italic>optimal</italic>. Several case studies illustrate the impact of the above disruptive factors on the dynamic behavior of avgTTA/hr, which provide useful insights about the current LOE of the system. Also, the effect of actions taken to return the CSOC to its normal operating condition is studied by varying both the amount and the time of action, which in turn impacts the dynamic behavior of avgTTA/hr. Results indicate that by using the insights learnt from measuring, monitoring, and controlling the dynamic behavior of avgTTA/hr, a manager can quantify and color-code the LOE of the CSOC. Furthermore, the above insights allow for a deeper understanding of acceptable downtime for the IDS, acceptable levels for absenteeism, and the recovery time and effort needed to return the CSOC to its ideal LOE. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Information Security is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128214788&site=ehost-live
295,A Multire solution Analysis-Assisted Reinforcement Learning Approach to Run-by-Run Control.,Rajesh Ganesan,IEEE Transactions on Automation Science & Engineering,15455955,,Apr-07,4,2,182,12,24812355,10.1109/TASE.2006.879915,IEEE,Article,"PROCESS control systems; MANUFACTURING processes; REINFORCEMENT learning; WAVELETS (Mathematics); STOCHASTIC systems; STOCHASTIC processes; Industrial Process Furnace and Oven Manufacturing; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables; Process, Physical Distribution, and Logistics Consulting Services",Chemical mechanical planarization (CMP); exponentially weighted moving average (EWMA); multiresolution; reinforcement learning; run-by-run control; wavelet; wavelet-modulated reinforcement learning-run-by run (WRL-RbR),"In recent years, the run-by run (RbR) control mechanism has emerged as a useful tool for keeping complex semi-conductor manufacturing processes on target during repeated short production runs. Many types of RbR controllers exist in the literature of which the exponentially weighted moving average (EWMA) controller is widely used in the industry. However, EWMA controllers are known to have several limitations. For example, in the presence of multiscale disturbances and lack of accurate process models, the performance of EWMA controller deteriorates and often fails to control the process. Also, the control of complex manufacturing processes requires sensing of multiple parameters that may be spatially distributed. New control strategies that can successfully use spatially distributed sensor data are required. This paper presents a new multiresolution analysis (wavelet) assisted reinforcement learning (RL)-based control strategy that can effectively deal with both multiscale disturbances in processes and the lack of process models. The novel idea of a wavelet-aided RL-based controller represents a paradigm shift in the control of large-scale stochastic dynamic systems of which the control problem is a subset. Henceforth, we refer our new control strategy as a WRL-RbR controller. The WRL-RbR controller is tested on a multiple-input-multiple-output chemical mechanical planarization process of wafer fabrication for which the process model is available. Results show that the RL controller outperforms EWMA-based controllers for low autocorrelation. The new controller also performs quite well for strongly auto-correlated processes for which the EWMA controllers are known to fail. Convergence analysis of the new breed of the WRL-RbR controller is presented. Further enhancement of the controller to deal with model-free processes and for inputs coming from spatially distributed environments are also discussed. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24812355&site=ehost-live
296,A Multiscale Bayesian SPRT Approach for Online Process Monitoring.,Rajesh Ganesan,IEEE Transactions on Semiconductor Manufacturing,8946507,,Aug-08,21,3,399,14,34138387,10.1109/TSM.2008.2001217,IEEE,Article,"QUALITY control; PRODUCTION engineering; FACTORY management; SEMICONDUCTOR manufacturing; SEMICONDUCTOR industry; MANUFACTURING processes; BAYESIAN analysis; Industrial Process Furnace and Oven Manufacturing; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables; Semiconductor and Related Device Manufacturing",Bayesian sequential probability ratio test (SPRT); chemical-mechanical planarization (CMP); coefficient of friction; end point detection (EPD); multiresolution analysis; wavelet,"Online monitoring of complex processes, such as semiconductor manufacturing processes, often requires the need to analyze sensor data with multiple characteristics. Some of these characteristics include nonstationary behavior, non-Gaussian distribution, high frequency of data generation, and multiscale (multiple frequencies) noise that mask the true nature of the process. Furthermore, it is necessary to implement process monitoring schemes that take into consideration the cost associated with sampling and incorrect decision making without sacrificing sensitivity, robustness, and ease of implementation. In this paper, a novel multiscale Bayesian sequential probability ratio test (MBSPRT) is developed, which is shown to be efficient in monitoring processes with the above characteristics. The MBSPRT method is also made suitable for online application by developing a moving block data processing strategy, which can match the data processing speed with the rate of data acquisition. The efficacy of the MBSPRT method was tested via detection of the end point occurrence in a chemical-mechanical planarization (CMP) process of semiconductor manufacturing using coefficient of friction (CoF) data. The proposed methodology offers a cost effective alternative to the traditional end point method, which is based on expensive metrology. Test results from both oxide and copper metal CMP are presented which show that MBSPRT is uniquely capable of identifying the start and finish of the end point event. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34138387&site=ehost-live
297,Accuracy of reinforcement learning algorithms for predicting aircraft taxi-out times: A case-study of Tampa Bay departures,Rajesh Ganesan,Transportation Research: Part C,0968090X,,Dec-10,18,6,950,13,53303754,10.1016/j.trc.2010.03.003,Elsevier B.V.,Article,MACHINE learning; DYNAMIC programming; SYSTEMS engineering; REINFORCEMENT learning; INTERNATIONAL airports; DATABASES; FLIGHT delays & cancellations (Airlines); TAMPA International Airport,Flight delay; Reinforcement learning; Taxi-out prediction,"Abstract: Taxi-out delay is a significant portion of the block time of a flight. Uncertainty in taxi-out times reduces predictability of arrival times at the destination. This in turn results in inefficient use of airline resources such as aircraft, crew, and ground personnel. Taxi-out time prediction is also a first step in enabling schedule modifications that would help mitigate congestion and reduce emissions. The dynamically changing operation at the airport makes it difficult to accurately predict taxi-out time. In this paper we investigate the accuracy of taxi out time prediction using a nonparametric reinforcement learning (RL) based method, set in the probabilistic framework of stochastic dynamic programming. A case-study of Tampa International Airport (TPA) shows that on an average, with 93.7% probability, on any given day, our predicted mean taxi-out time for any given quarter, matches the actual mean taxi-out time for the same quarter with a standard error of 1.5min. Also, for individual flights, the taxi-out time of 81% of them were predicted accurately within a standard error of 2min. The predictions were done 15min before gate departure. Gate OUT, wheels OFF, wheels ON, and gate IN (OOOI) data available in the Aviation System Performance Metric (ASPM) database maintained by the Federal Aviation Administration (FAA) was used to model and analyze the problem. The prediction accuracy is high even without the use of detailed track data. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303754&site=ehost-live
297,Accuracy of reinforcement learning algorithms for predicting aircraft taxi-out times: A case-study of Tampa Bay departures,Lance Sherry,Transportation Research: Part C,0968090X,,Dec-10,18,6,950,13,53303754,10.1016/j.trc.2010.03.003,Elsevier B.V.,Article,MACHINE learning; DYNAMIC programming; SYSTEMS engineering; REINFORCEMENT learning; INTERNATIONAL airports; DATABASES; FLIGHT delays & cancellations (Airlines); TAMPA International Airport,Flight delay; Reinforcement learning; Taxi-out prediction,"Abstract: Taxi-out delay is a significant portion of the block time of a flight. Uncertainty in taxi-out times reduces predictability of arrival times at the destination. This in turn results in inefficient use of airline resources such as aircraft, crew, and ground personnel. Taxi-out time prediction is also a first step in enabling schedule modifications that would help mitigate congestion and reduce emissions. The dynamically changing operation at the airport makes it difficult to accurately predict taxi-out time. In this paper we investigate the accuracy of taxi out time prediction using a nonparametric reinforcement learning (RL) based method, set in the probabilistic framework of stochastic dynamic programming. A case-study of Tampa International Airport (TPA) shows that on an average, with 93.7% probability, on any given day, our predicted mean taxi-out time for any given quarter, matches the actual mean taxi-out time for the same quarter with a standard error of 1.5min. Also, for individual flights, the taxi-out time of 81% of them were predicted accurately within a standard error of 2min. The predictions were done 15min before gate departure. Gate OUT, wheels OFF, wheels ON, and gate IN (OOOI) data available in the Aviation System Performance Metric (ASPM) database maintained by the Federal Aviation Administration (FAA) was used to model and analyze the problem. The prediction accuracy is high even without the use of detailed track data. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303754&site=ehost-live
298,Adaptive Alert Management for Balancing Optimal Performance among Distributed CSOCs using Reinforcement Learning.,Rajesh Ganesan,IEEE Transactions on Parallel & Distributed Systems,10459219,,Jan-20,31,1,16,18,143316119,10.1109/TPDS.2019.2927977,IEEE,Article,DECISION support systems; DYNAMIC programming; STOCHASTIC programming,and adaptive resource allocation; centralized alert management; Computer security; Decision making; Distributed cybersecurity operations center (CSOC); level of operational effectiveness; Measurement; Monitoring; Organizations; Reinforcement learning; Sensors,"Large organizations typically have Cybersecurity Operations Centers (CSOCs) distributed at multiple locations that are independently managed, and they have their own cybersecurity analyst workforce. Under normal operating conditions, the CSOC locations are ideally staffed such that the alerts generated from the sensors in a work-shift are thoroughly investigated by the scheduled analysts in a timely manner. Unfortunately, when adverse events such as increase in alert arrival rates or alert investigation rates occur, alerts have to wait for a longer duration for analyst investigation, which poses a direct risk to organizations. Hence, our research objective is to mitigate the impact of the adverse events by dynamically and autonomously re-allocating alerts to other location(s) such that the performances of all the CSOC locations remain balanced. This is achieved through the development of a novel centralized adaptive decision support system whose task is to re-allocate alerts from the affected locations to other locations. This re-allocation decision is non-trivial because the following must be determined: (1) timing of a re-allocation decision, (2) number of alerts to be re-allocated, and (3) selection of the locations to which the alerts must be distributed. The centralized decision-maker (henceforth referred to as agent) continuously monitors and controls the level of operational effectiveness-LOE (a quantified performance metric) of all the locations. The agent's decision-making framework is based on the principles of stochastic dynamic programming and is solved using reinforcement learning (RL). In the experiments, the RL approach is compared with both rule-based and load balancing strategies. By simulating real-world scenarios, learning the best decisions for the agent, and applying the decisions on sample realizations of the CSOC's daily operation, the results show that the RL agent outperforms both approaches by generating (near-) optimal decisions that maintain a balanced LOE among the CSOC locations. Furthermore, the scalability experiments highlight the practicality of adapting the method to a large number of CSOC locations. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143316119&site=ehost-live
299,Improving quality of prediction in highly dynamic environments using approximate dynamic programming.,Rajesh Ganesan,Quality & Reliability Engineering International,7488017,,Nov-10,26,7,717,16,54861143,10.1002/qre.1127,Wiley-Blackwell,Article,DYNAMIC programming; DECISION making; AIRPORT traffic control; INTERNATIONAL airports; REGRESSION analysis; Air Traffic Control,air transportation; approximate dynamic programming; quality of prediction; reinforcement learning; taxi-out time,"In many applications, decision making under uncertainty often involves two steps-prediction of a certain quality parameter or indicator of the system under study and the subsequent use of the prediction in choosing actions. The prediction process is severely challenged by highly dynamic environments that particularly involve sequential decision making, such as air traffic control at airports in which congestion prediction is critical for smooth departure operations. Taxi-out time of a flight is an excellent indicator of surface congestion and is a quality parameter used in the assessment of airport delays. The regression, queueing, and moving average models have been shown to perform poorly in predicting taxi-out times because they are slow in adapting to the changing airport dynamics. This paper presents an approximate dynamic programming approach (reinforcement learning, RL) to taxi-out time prediction. The taxi-out prediction performance was tested on flight data obtained from the Federal Aviation Administration's (FAA) Aviation System Performance Metrics (ASPM) database on Detroit International (DTW), Washington Reagan National (DCA), Boston (BOS), New York John F. Kennedy (JFK) and Tampa International (TPA) airports. For example, at the Boston airport (presented in detail) the prediction accuracy by RL model was 14running-average model. In general, the RL model was 35-50% more accurate than the regression model for all of the above airports. Copyright © 2010 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of Quality & Reliability Engineering International is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=54861143&site=ehost-live
299,Improving quality of prediction in highly dynamic environments using approximate dynamic programming.,Lance Sherry,Quality & Reliability Engineering International,7488017,,Nov-10,26,7,717,16,54861143,10.1002/qre.1127,Wiley-Blackwell,Article,DYNAMIC programming; DECISION making; AIRPORT traffic control; INTERNATIONAL airports; REGRESSION analysis; Air Traffic Control,air transportation; approximate dynamic programming; quality of prediction; reinforcement learning; taxi-out time,"In many applications, decision making under uncertainty often involves two steps-prediction of a certain quality parameter or indicator of the system under study and the subsequent use of the prediction in choosing actions. The prediction process is severely challenged by highly dynamic environments that particularly involve sequential decision making, such as air traffic control at airports in which congestion prediction is critical for smooth departure operations. Taxi-out time of a flight is an excellent indicator of surface congestion and is a quality parameter used in the assessment of airport delays. The regression, queueing, and moving average models have been shown to perform poorly in predicting taxi-out times because they are slow in adapting to the changing airport dynamics. This paper presents an approximate dynamic programming approach (reinforcement learning, RL) to taxi-out time prediction. The taxi-out prediction performance was tested on flight data obtained from the Federal Aviation Administration's (FAA) Aviation System Performance Metrics (ASPM) database on Detroit International (DTW), Washington Reagan National (DCA), Boston (BOS), New York John F. Kennedy (JFK) and Tampa International (TPA) airports. For example, at the Boston airport (presented in detail) the prediction accuracy by RL model was 14running-average model. In general, the RL model was 35-50% more accurate than the regression model for all of the above airports. Copyright © 2010 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of Quality & Reliability Engineering International is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=54861143&site=ehost-live
300,Maintaining the level of operational effectiveness of a CSOC under adverse conditions.,Rajesh Ganesan,International Journal of Information Security,16155262,,Jun-22,21,3,637,15,157133985,10.1007/s10207-021-00573-4,Springer Nature,Article,BUSINESS hours; TIME management; DYNAMIC models,Alert analysis; Cybersecurity operations center; Discarding alerts; Level of operational effectiveness; Reinforcement learning,"The level of operational effectiveness (LOE) is a color-coded performance metric that is monitored by the cybersecurity operations center (CSOC). It is determined using the average time to analyze alerts (AvgTTA) in every hour of shift operation, where the time to analyze an alert (TTA) is the sum of waiting time in the queue and investigation time by the analysts. Ideally, the CSOC managers would set a predetermined baseline target for AvgTTA to be maintained for every hour of shift operation. However, due to adverse events, an imbalance may exist if the alert arrival rate far exceeds the service rate, resulting in high AvgTTA or low LOE. Upon exhausting all the analyst resources, the only option available to a CSOC manager is to discard alerts for restoring the LOE of the CSOC. The paper proposes two strategies: the value-based strategy is developed using a static optimization model while the reinforcement learning-based strategy is developed using a dynamic optimization model. The paper compares various strategies for discarding alerts and measures the following desiderata for comparing them: (1) minimize the number of alerts discarded, (2) ensure highest utilization of analysts, (3) determine the optimal time at which the alerts must be discarded in a shift, and (4) maintain the best possible LOE closest to the baseline target LOE. Results indicate that, overall, the RL strategy is the best performer among all strategies that guarantees the AvgTTA below the threshold value in every hour of shift operation while discarding the fewest number of alerts under adverse events. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Information Security is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157133985&site=ehost-live
301,Real-time monitoring of complex sensor data using wavelet-based multiresolution analysis.,Rajesh Ganesan,International Journal of Advanced Manufacturing Technology,2683768,,Dec-08,39,6-May,543,16,34649819,10.1007/s00170-007-1237-z,Springer Nature,Article,"MANUFACTURING processes; ENGINEERING instruments; ELECTRONIC data processing; PRODUCTION engineering; ELECTRIC conductivity; DETECTORS; Data Processing, Hosting, and Related Services; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables; Industrial Process Furnace and Oven Manufacturing",Nano-scale; Non-stationary data; Process monitoring; Semiconductor processes; Wavelet analysis,"Advancements made in sensor technology have resulted in complex sensor data that captures multiple process events. Real-time monitoring of complex manufacturing processes, such as nano-scale semiconductor polishing, often requires analysis of such complex sensor data. The multiple events in a process occur at multiple scales or frequencies (also referred to as multiscale) and are localized at different points in time. Recent literature contains several wavelet decomposition based multiscale sensor data analysis techniques including those that are developed for process monitoring applications, such as tool-life monitoring, bearing defect monitoring, and monitoring of ultra-precision processes. However, most of the above mentioned wavelet-based sensor data analysis techniques are designed for offline implementation. In an offline method, one can perform wavelet decomposition of longer data lengths in order to capture information needed for monitoring. However, this is computationally involved and needs longer processing time, which becomes a serious challenge in online (real time) applications. This paper first presents a complete online multiscale process monitoring methodology. The methodology is designed to deal with real-time analysis and testing of very high rate of process data collected by sensors. This is particularly critical and becomes a challenge for high rate of data collection by the sensors which pose additional difficulty of matching data processing rate with the data acquisition rate. The methodology is capable of displaying the analysis results through real time graphs for ease of process supervisory decision making. The methodology is demonstrated via a nano-scale silicon wafer polishing application. Sufficient details of the application are provided to assist readers in implementing this methodology for other processes. The results show that the methodology has the ability to deal with high rate of data collection as well as multiscale event detection. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Advanced Manufacturing Technology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34649819&site=ehost-live
302,Two Can Play That Game: An Adversarial Evaluation of a Cyber-Alert Inspection System.,Rajesh Ganesan,ACM Transactions on Intelligent Systems & Technology,21576904,,May-20,11,3,1,20,143388917,10.1145/3377554,Association for Computing Machinery,Article,GAME theory; REINFORCEMENT learning; DECISION making; MARKOV processes; MODEL theory,adversarial reinforcement learning; Cyber-security operations center; game theory,"Cyber-security is an important societal concern. Cyber-attacks have increased in numbers as well as in the extent of damage caused in every attack. Large organizations operate a Cyber Security Operation Center (CSOC), which forms the first line of cyber-defense. The inspection of cyber-alerts is a critical part of CSOC operations (defender or blue team). Recent work proposed a reinforcement learning (RL) based approach for the defender's decision-making to prevent the cyber-alert queue length from growing large and overwhelming the defender. In this article, we perform a red team (adversarial) evaluation of this approach. With the recent attacks on learning-based decision-making systems, it is even more important to test the limits of the defender's RL approach. Toward that end, we learn several adversarial alert generation policies and the best response against them for various defender's inspection policy. Surprisingly, we find the defender's policies to be quite robust to the best response of the attacker. In order to explain this observation, we extend the earlier defender's RL model to a game model with adversarial RL, and show that there exist defender policies that can be robust against any adversarial policy. We also derive a competitive baseline from the game theory model and compare it to the defender's RL approach. However, when we go further to exploit the assumptions made in the Markov Decision Process (MDP) in the defender's RL model, we discover an attacker policy that overwhelms the defender. We use a double oracle like approach to retrain the defender with episodes from this discovered attacker policy. This made the defender robust to the discovered attacker policy and no further harmful attacker policies were discovered. Overall, the adversarial RL and double oracle approach in RL are general techniques that are applicable to other RL usage in adversarial environments. [ABSTRACT FROM AUTHOR] Copyright of ACM Transactions on Intelligent Systems & Technology is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143388917&site=ehost-live
303,A spherical projection of a complex Hilbert space is conformal iff it is the stereographic projection.,Yotam Gingold,Differential Geometry--Dynamical Systems,1454511X,,2018,20,,38,33,131541853,,Balkan Society of Geometers (Societatea Balcanica a Geometrilor),Article,HILBERT space; NONLINEAR analysis; METRIC projections; CONFORMAL geometry; COMPACTIFICATION (Mathematics),angle; bijection; Boundization; compactification; Complex Hilbert space; conformal; directions at infinity; eye; geometry; infinity; inner product space; nonlinear; nonlinear transformations; perspective; projective; similarity; sphere; spherical bowl; stereographic projection,"We consider a family of nonlinear projections that map a complex Hilbert space onto a bounded ""bowl"" shaped subset of a sphere. Our main result states that a projection is conformal iff it is the stereographic projection and iff the projection renders all certain pairs of triangles induced by the projection to be similar. It follows that various so called ""compactifications"" that are given in the literature are special members of this family of nonlinear projections. These include the stereographic projection and the Poincare compactification. Background and motivation are discussed and several examples illustrating the results are provided. [ABSTRACT FROM AUTHOR] Copyright of Differential Geometry--Dynamical Systems is the property of Balkan Society of Geometers (Societatea Balcanica a Geometrilor) and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131541853&site=ehost-live
304,Foreword to the Special Section on Expressive 2016.,Yotam Gingold,Computers & Graphics,978493,,Jun-17,65,,A1,1,123309984,10.1016/j.cag.2017.05.004,Elsevier B.V.,Article,MOTION estimation (Signal processing); OPTICAL motion tracking (Computer vision),,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123309984&site=ehost-live
305,Hyperspectral Inverse Skinning.,Yotam Gingold,Computer Graphics Forum,1677055,,Sep-20,39,6,49,17,145974300,10.1111/cgf.13903,Wiley-Blackwell,Article,INVERSE problems; MOTION capture (Human mechanics); AFFINE geometry; SKIN; GEOMETRIC modeling,affine geometry; animation; Computing methodologies → Mesh geometry models; deformation; hyperspectral unmixing; linear blend skinning; Motion processing,"In example‐based inverse linear blend skinning (LBS), a collection of poses (e.g. animation frames) are given, and the goal is finding skinning weights and transformation matrices that closely reproduce the input. These poses may come from physical simulation, direct mesh editing, motion capture or another deformation rig. We provide a re‐formulation of inverse skinning as a problem in high‐dimensional Euclidean space. The transformation matrices applied to a vertex across all poses can be thought of as a point in high dimensions. We cast the inverse LBS problem as one of finding a tight‐fitting simplex around these points (a well‐studied problem in hyperspectral imaging). Although we do not observe transformation matrices directly, the 3D position of a vertex across all of its poses defines an affine subspace, or flat. We solve a 'closest flat' optimization problem to find points on these flats, and then compute a minimum‐volume enclosing simplex whose vertices are the transformation matrices and whose barycentric coordinates are the skinning weights. We are able to create LBS rigs with state‐of‐the‐art reconstruction error and state‐of‐the‐art compression ratios for mesh animation sequences. Our solution does not consider weight sparsity or the rigidity of recovered transformations. We include observations and insights into the closest flat problem. Its ideal solution and optimal LBS reconstruction error remain an open problem. [ABSTRACT FROM AUTHOR] Copyright of Computer Graphics Forum is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145974300&site=ehost-live
306,Interacting with Self-Similarity.,Yotam Gingold,Computer-Aided Design,104485,,Jan-21,130,,N.PAG,1,146933449,10.1016/j.cad.2020.102931,Elsevier B.V.,Article,"MAP design; ALGORITHMS; HYBRID systems; GEOMETRY; Other printing; Book, Periodical, and Newspaper Merchant Wholesalers",Interaction; Shape analysis; Signature; Similarity,"Shape similarity is a fundamental problem in geometry processing, enabling applications such as surface correspondence, segmentation, and edit propagation. For example, a user may paint a stroke on one finger of a model and desire the edit to propagate to all fingers. Automatic approaches have difficulty matching user expectations, either due to an algorithm's inability to guess the scale at which the user is intending to edit or due to underlying deficiencies in the similarity metric (e.g., semantic information not present in the geometry). We propose an approach to interactively design self-similarity maps. We investigate two primitive operations, useful in a variety of scenarios: region and curve similarity. Users select example similar and dissimilar regions. Starting with an automatically generated multi-scale shape signature, our approach solves for a scale parameter and thresholds that group the example regions as specified. We propose a new Smooth Shape Diameter Signature (SSDS) as a more efficient alternative to the Heat or Wave Kernel Signature. If no such parameters can be found, our approach modifies the shape signature itself. Given a curve drawn on the surface, we perform hybrid discrete/continuous optimization to find similar curves elsewhere. We apply our approach for interactive editing scenarios: propagating mesh geometry, patterns duplication, and segmentation. • An approach for interactively defining self-similarity across a class of signatures. • The Smooth Shape Diameter Signature (SSDS), a new descriptive shape signature. • A method for distortion-free curve propagation via tangent-space curve unrolling. [ABSTRACT FROM AUTHOR] Copyright of Computer-Aided Design is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146933449&site=ehost-live
307,PosterChild: Blend‐Aware Artistic Posterization.,Yotam Gingold,Computer Graphics Forum,1677055,,Jul-21,40,4,87,13,151432863,10.1111/cgf.14343,Wiley-Blackwell,Article,ALGORITHMS; POLYMER blends; IMAGE processing; Photofinishing Laboratories (except One-Hour); One-Hour Photofinishing,Applied computing → Fine arts; CCS Concepts; Computing methodologies → Non‐photorealistic rendering; Image processing,"Posterization is an artistic effect which converts continuous images into regions of constant color with smooth boundaries, often with an artistically recolored palette. Artistic posterization is extremely time‐consuming and tedious. We introduce a blend‐aware algorithm for generating posterized images with palette‐based control for artistic recoloring. Our algorithm automatically extracts a palette and then uses multi‐label optimization to find blended‐color regions in terms of that palette. We smooth boundaries away from image details with frequency‐guided median filtering. We evaluate our algorithm with a comparative user study and showcase its ability to produce compelling posterizations of a variety of inputs. Our parameters provide artistic control and enable cohesive, real‐time recoloring after posterization pre‐processing. [ABSTRACT FROM AUTHOR] Copyright of Computer Graphics Forum is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151432863&site=ehost-live
308,Using isophotes and shadows to interactively model normal and height fields.,Yotam Gingold,Computers & Graphics,978493,,Oct-16,59,,130,13,117587399,10.1016/j.cag.2016.02.004,Elsevier B.V.,Article,SHADES & shadows; MATHEMATICAL models; IMAGE processing; SILHOUETTES; Photofinishing Laboratories (except One-Hour); One-Hour Photofinishing,Isophotes; Non-photorealistic rendering; Sketching,"We introduce an interactive modeling tool for designing (a) a smooth 3D normal field from the isophotes of a discretely shaded 2D image and (b) lifting the normal field into a smooth height field given a cast shadow. Block or cartoon shading is a visual style in which artists depict a smoothly shaded 3D object using a small number of discrete brightness values, manifested as regions or bands of constant color. In our approach, artists trace isophotes, or curves of constant brightness, along the boundaries between constant color bands. Our algorithm first estimates light directions and computes 3D normals along the object silhouette and at intersections between isophotes from different light sources. We then propagate these 3D normals smoothly along isophotes, and subsequently throughout the interior of the shape. We describe our user interface for editing isophotes and correcting unintended normals produced by our algorithm. We also describe a technique for lifting the generated normal field into a height field given the boundary of the shadow cast by the object. We validate our approach with a perceptual experiment and comparisons to ground truth data. Finally, we present a set of 3D renderings created using our interface. [ABSTRACT FROM AUTHOR] Copyright of Computers & Graphics is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117587399&site=ehost-live
309,Dynamics of a single buoyant plume in a FENE-P fluid.,Robert Handler,Physics of Fluids,10706631,,2017,29,9,1,6,125456524,10.1063/1.4986749,American Institute of Physics,Article,BUOYANCY; PLUMES (Fluid dynamics); NONLINEAR elastic fracture; WEISSENBERG effect; RAYLEIGH-Benard convection,,"The dynamics of a single laminar buoyant plume in a FENE-P (finitely extensible nonlinear elastic- Peterlin) fluid has been investigated by performing a series of direct simulations over a range of Weissenberg numbers. Examination of this flow has given insight into the heat transfer reduction phenomenon observed recently in more complex Rayleigh-Benard turbulence. The simulations, which were performed with a Rayleigh number of 2.53 × 106 and a maximal polymeric extension of L = 100, show that the wall heat flux is reduced by about 28% at a Weissenberg number of 20, which is in reasonable agreement with the results obtained in Rayleigh-Benard turbulence. In addition, the global flow kinetic energy was reduced by about one order of magnitude. [ABSTRACT FROM AUTHOR] Copyright of Physics of Fluids is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125456524&site=ehost-live
310,Estimating air-water gas transfer velocity during low wind condition with and without buoyancy.,Robert Handler,Geophysical Research Abstracts,10297006,,2018,20,,1383,1,137520771,,Copernicus Gesellschaft mbH,Article,VELOCITY; WINDS; GASES; BUOYANCY,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137520771&site=ehost-live
311,Numerical simulation of the swirling flow of a finitely extensible non-linear elastic Peterlin fluid.,Robert Handler,Physics of Fluids,10706631,,Oct-20,32,10,1,12,146790407,10.1063/5.0021469,American Institute of Physics,Article,FLOW simulations; SWIRLING flow; COMPUTER simulation; FLOW velocity; POLYMER solutions; FLOW visualization; FLOW instability,,"Viscoelastic fluids have been shown to undergo instabilities even at very low Reynolds numbers, and these instabilities can give rise to a phenomenon called elastic turbulence. This phenomenon, observed experimentally in viscoelastic polymer solutions, is driven by the strong coupling between the fluid velocity and the elasticity of the flow. To explore the emergence of these instabilities in a viscoelastic flow, we have chosen to explore, by means of direct numerical simulations, a particular case called von Kármán swirling flow. The simulations employ the finitely extensible nonlinear Peterlin model to represent the dynamics of a dilute polymer solution. Notably, a log-conformation technique is used to solve the governing equations. This method is useful in overcoming the high Weissenberg number problem. The results obtained from the simulations were generally in good agreement with experiments. The torque on the top plate was decomposed into Newtonian and polymeric components, and it was found that the polymeric component was dominant. In addition, flow visualizations revealed that a toroidal vortex was strongly correlated with the distribution of the stresses on the rotating plate. [ABSTRACT FROM AUTHOR] Copyright of Physics of Fluids is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146790407&site=ehost-live
312,"Polyacrylamide in glycerol solutions from an atomistic perspective of the energetics, structure, and dynamics.",Robert Handler,AIP Advances,21583226,,Aug-20,10,8,1,10,145401079,10.1063/5.0020850,American Institute of Physics,Article,POLYACRYLAMIDE; RADIAL distribution function; GLYCERIN; ATOMIC charges; MOLECULAR dynamics; ELECTRIC potential; Soap and cleaning compound manufacturing; Soap and Other Detergent Manufacturing,,"All-atom molecular dynamics is used to investigate the structural, energetic, and dynamical properties of polyacrylamide (PAM) oligomers of different lengths solvated in pure glycerol, a 90:10 glycerol–water mixture, and pure water. We predict that the oligomers' globular structure is obtained only when the modeling strategy considers the solvent as a continuous background. Meanwhile, for all-atom modeled solvents, the glycerol solutions display a strong tendency of trapping the oligomers in instantaneous elongated random coiled structures that remain locked-in over tens of nanoseconds. In pure water, the oligomers acquire considerably shorter random coiled structures of increased flexibility. The all-atom force field, generalized amber force field, is modified by including restrained electrostatic potential atomic charges for both glycerol and PAM. Three PAM oligomer lengths containing 10, 20, and 30 monomers are considered in detail by monitoring the radius of gyration, end-to-end distance, intra-potential energy, and solvent–oligomer interaction energies for decades of nanoseconds. The density and radial distribution function of glycerol solutions are calculated when modeled with the modified atomic charges, showing a very good agreement with the experimental results at temperatures around 300 K. Glycerol has multiple applications, including its use in gel formation for PAM gel electrophoresis. Our findings are relevant for the design of sensors based on microfluidics and tailored pharmaceutical buffer solutions. [ABSTRACT FROM AUTHOR] Copyright of AIP Advances is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145401079&site=ehost-live
313,"Structure, energetics and thermodynamics of PLGA condensed phases from Molecular Dynamics.",Robert Handler,Polymer,323861,,Oct-20,206,,N.PAG,1,146057721,10.1016/j.polymer.2020.122903,Elsevier B.V.,Article,CONDENSED matter; MOLECULAR dynamics; THERMODYNAMICS; RADIAL distribution function; ATOMIC charges; GLASS transition temperature; SOLUBILITY,Caloric curve; Glass transition; Poly(lactic-co-glycolic acid) (PLGA); Response thermodynamic properties; Solid-state stability,"Poly-lactic- co -glycolic acid (PLGA) is a biodegradable co-polymer with common use in nanoparticle drug encapsulation. Although well studied experimentally, the mechanical behavior of PLGA is not well understood at the atomic level. Here, we develop atomic charges for the all-atom Generalized Amber Force Field (GAFF) and conduct all-atom molecular dynamics simulations of PLGA with a 50:50 ratio between its two constituent monomers for five samples of the polymer condensed phases that span 1579 u to 20183 u in molecular weight. We predict several PLGA properties that will improve the knowledge of its atomistic organization in the glassy solid, rubber, and liquid states. We report the impact of molecular weight on cohesive energy, solubility, thermodynamic response properties, structural properties related to chain entanglement, and glass transition temperatures. Properties are compared against known experimental values when available. We find that the restrained electrostatic potential atomic charges are better for simulating the caloric curve leading to the glass transition temperature, which agrees very well with experiments. • Poly(lactic acid-co-glycolic acid) newly parametrized atomic charges yield a glass transition temperature that agrees excellently with experiments. • Energetics and structural characteristics of the glassy-solid and rubbery-liquid are predicted for the first time. • Fingerprints for controlling the polymer function are derived from the polymer chains thermal behavior. • Predictions of cohesive energy, Hildebrand parameter, and radial distribution functions await for experimental discovery. [ABSTRACT FROM AUTHOR] Copyright of Polymer is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146057721&site=ehost-live
314,The nature of bubble entrapment in a Lamb–Oseen vortex.,Robert Handler,Physics of Fluids,10706631,,Jun-21,33,6,1,5,151175717,10.1063/5.0053658,American Institute of Physics,Letter,REYNOLDS number; TURBULENT flow; TURBULENCE; QUASI-equilibrium; BUBBLES,,"Bubble trajectories in the presence of a decaying Lamb–Oseen vortex are calculated using a modified Maxey–Riley equation. Some bubbles are shown to get trapped within the vortex in quasi-equilibrium states. All the trapped bubbles exit the vortex at a time that is only a function of the Galilei number and the vortex Reynolds number. The set of initial bubble locations that lead to entrapment is numerically determined to show the capturing potential of a single vortex. The results provide insight into the likelihood of bubble entrapment within vortical structures in turbulent flows. [ABSTRACT FROM AUTHOR] Copyright of Physics of Fluids is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151175717&site=ehost-live
315,Detecting and classifying online dark visual propaganda.,Mahdi Hashemi,Image & Vision Computing,2628856,,Sep-19,89,,95,11,138459637,10.1016/j.imavis.2019.06.001,Elsevier B.V.,Article,PROPAGANDA; WORLD Wide Web; ONLINE social networks; WEBSITES; ORGANIZATIONAL communication; Internet Publishing and Broadcasting and Web Search Portals,Convolutional neural networks; Dark Web; Deep learning; Image classification; Machine learning; Violent extremist organizations,"The staggering increase in the amount of information on the World Wide Web (referred to as Web) has made Web page classification essential to retrieve useful information while filtering out unwanted, futile, or harmful contents. This massive information-sharing platform is occasionally abused for propagating extreme and radical ideologies and posing threats to national security and citizens. Detecting the so called dark material has gained more impetus following the recent outbreak of extremist groups and radical ideologies across the Web. The goal of this project, being the first of its own, is to surveil online social networks (OSN) and Web for real-time detection of visual propaganda by violent extremist organizations (VEOs). This is valuable not only for flagging and removing such content from OSN and Web, but also to provide military insight and narrative context inside VEOs. Visual propaganda by VEOs are not only detected, but also further classified based on the type of VEO and focus or intent of the image into hard propaganda, soft propaganda, symbolic propaganda, landscape, and organizational communications. Over 1.2 million images were automatically collected from suspicious OSN accounts and Web pages over a course of four years. Out of which, 120,000 images were manually classified to provide the training data for a convolutional neural network. An overall generalization accuracy of 97.02% and F 1 of 97.89% were achieved for a binary classification or mere detection of visual VEO propaganda and an overall generalization accuracy of 86.08% and F 1 ¯ of 85.76% for an eight-way classification based on the intent of the image. [ABSTRACT FROM AUTHOR] Copyright of Image & Vision Computing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138459637&site=ehost-live
318,idDock+: Integrating Machine Learning in Probabilistic Search for Protein-Protein Docking.,Irina Hashmi,Journal of Computational Biology,10665277,,Sep-15,22,9,806,17,109251700,10.1089/cmb.2015.0108,"Mary Ann Liebert, Inc.",Article,"MOLECULAR interactions; PROTEIN-protein interactions; MOLECULAR docking; MACHINE learning; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",ensemble learning; informatics-driven; probabilistic search; Protein-protein docking; stochastic optimization,"Predicting the three-dimensional native structures of protein dimers, a problem known as protein-protein docking, is key to understanding molecular interactions. Docking is a computationally challenging problem due to the diversity of interactions and the high dimensionality of the configuration space. Existing methods draw configurations systematically or at random from the configuration space. The inaccuracy of scoring functions used to evaluate drawn configurations presents additional challenges. Evidence is growing that optimization of a scoring function is an effective technique only once the drawn configuration is sufficiently similar to the native structure. Therefore, in this article we present a method that employs optimization of a sophisticated energy function, FoldX, only to locally improve a promising configuration. The main question of how promising configurations are identified is addressed through a machine learning method trained a priori on an extensive dataset of functionally diverse protein dimers. To deal with the vast configuration space, a probabilistic search algorithm operates on top of the learner, feeding to it configurations drawn at random. We refer to our method as idDock+, for nformatics-riven ocking. idDock+is tested on 15 dimers of different sizes and functional classes. Analysis shows that on all systems idDock+finds a near-native structure and is comparable in accuracy to other state-of-the-art methods. idDock+ represents one of the first highly efficient hybrid methods that combines fast machine learning models with demanding optimization of sophisticated energy scoring functions. Our results indicate that this is a promising direction to improve both efficiency and accuracy in docking. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251700&site=ehost-live
318,idDock+: Integrating Machine Learning in Probabilistic Search for Protein-Protein Docking.,Amarda Shehu,Journal of Computational Biology,10665277,,Sep-15,22,9,806,17,109251700,10.1089/cmb.2015.0108,"Mary Ann Liebert, Inc.",Article,"MOLECULAR interactions; PROTEIN-protein interactions; MOLECULAR docking; MACHINE learning; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",ensemble learning; informatics-driven; probabilistic search; Protein-protein docking; stochastic optimization,"Predicting the three-dimensional native structures of protein dimers, a problem known as protein-protein docking, is key to understanding molecular interactions. Docking is a computationally challenging problem due to the diversity of interactions and the high dimensionality of the configuration space. Existing methods draw configurations systematically or at random from the configuration space. The inaccuracy of scoring functions used to evaluate drawn configurations presents additional challenges. Evidence is growing that optimization of a scoring function is an effective technique only once the drawn configuration is sufficiently similar to the native structure. Therefore, in this article we present a method that employs optimization of a sophisticated energy function, FoldX, only to locally improve a promising configuration. The main question of how promising configurations are identified is addressed through a machine learning method trained a priori on an extensive dataset of functionally diverse protein dimers. To deal with the vast configuration space, a probabilistic search algorithm operates on top of the learner, feeding to it configurations drawn at random. We refer to our method as idDock+, for nformatics-riven ocking. idDock+is tested on 15 dimers of different sizes and functional classes. Analysis shows that on all systems idDock+finds a near-native structure and is comparable in accuracy to other state-of-the-art methods. idDock+ represents one of the first highly efficient hybrid methods that combines fast machine learning models with demanding optimization of sophisticated energy scoring functions. Our results indicate that this is a promising direction to improve both efficiency and accuracy in docking. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251700&site=ehost-live
319,Enhanced Security Authentication Based on Convolutional-LSTM Networks.,Monson Hayes,Sensors (14248220),14248220,,Aug-21,21,16,5379,1,152145933,10.3390/s21165379,MDPI,Article,DEEP learning; STATISTICAL decision making; CHANNEL estimation; PHYSICAL layer security; SHORT-term memory,classification algorithms; deep learning; physical layer security; wireless networks,"The performance of classical security authentication models can be severely affected by imperfect channel estimation as well as time-varying communication links. The commonly used approach of statistical decisions for the physical layer authenticator faces significant challenges in a dynamically changing, non-stationary environment. To address this problem, this paper introduces a deep learning-based authentication approach to learn and track the variations of channel characteristics, and thus improving the adaptability and convergence of the physical layer authentication. Specifically, an intelligent detection framework based on a Convolutional-Long Short-Term Memory (Convolutional-LSTM) network is designed to deal with channel differences without knowing the statistical properties of the channel. Both the robustness and the detection performance of the learning authentication scheme are analyzed, and extensive simulations and experiments show that the detection accuracy in time-varying environments is significantly improved. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152145933&site=ehost-live
320,Face recognition for vehicle personalization with near infrared frame differencing.,Monson Hayes,IEEE Transactions on Consumer Electronics,983063,,Aug-16,62,3,316,9,119139415,10.1109/TCE.2016.7613199,IEEE,Article,"HUMAN facial recognition software; NEAR infrared radiation; HOUSEHOLD electronics; MATHEMATICAL programming; IDENTITY management systems; Household Appliances, Electric Housewares, and Consumer Electronics Merchant Wholesalers; Consumer Electronics Repair and Maintenance; Electronics Stores; Consumer Electronics and Appliances Rental",Cameras; Consumer grade biometrics; Face; face recognition; illumination invariant; Light emitting diodes; Lighting; near infrared; vehicle personalization; Vehicles; Videos,"This paper describes a system of practical technologies to implement an illumination-robust, consumer grade biometric system based on face recognition to be used in the automotive market. Most current face recognition systems are compromised in accuracy by ambient illumination changes. Outdoor applications including vehicle personalization pose an especially challenging environment for face recognition. The point of this research is to investigate practical face recognition used for identity management in order to minimize algorithmic complexity while making the system robust to ambient illumination changes. First, a frame differencing method is presented with an active near-infrared illumination control that produces images independent of the ambient illumination. Second, an end-to-end face recognition system is presented including foreground/background segmentation, motion detection, face detection, motion interpolation, pose clustering and face recognition modules. It is shown that the frame differencing method makes the modules more robust to the ambient illumination variation. Vehicular application videos were taken in extremely challenging outdoor illumination and shadowing conditions and used to test each module. Finally, extensive test results of vehicular scenario are provided to evaluate the end-to-end systems1. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Consumer Electronics is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119139415&site=ehost-live
321,Three-Dimensional Pose Estimation for Laboratory Mouse From Monocular Images.,Monson Hayes,IEEE Transactions on Image Processing,10577149,,Sep-19,28,9,4273,15,137295220,10.1109/TIP.2019.2908796,IEEE,Article,"LABORATORY mice; LABORATORIES; MICE; STREET addresses; MEASURE theory; HARDWARE stores; MEDICAL research; Home and auto supplies stores; All Other General Merchandise Stores; Hardware Stores; Veterinary Services; Testing Laboratories; Medical Laboratories; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); All Other Animal Production",3D pose estimation; Lenses; Mice; Monitoring; mouse behavior detection; mouse ethology; mouse home-cage monitoring; Pose estimation; Structured forests; Three-dimensional displays; trajectory features; Two dimensional displays,"Video-based activity and behavior analysis of mice has garnered wide attention in biomedical research. Animal facilities hold large numbers of mice housed in “home-cages” densely stored within ventilated racks. Automated analysis of mice activity in their home-cages can provide a new set of sensitive measures for detecting abnormalities and time-resolved deviation from the baseline behavior. Large-scale monitoring in animal facilities requires minimal footprint hardware that integrates seamlessly with the ventilated racks. The compactness of hardware imposes the use of fisheye lenses positioned in close proximity to the cage. In this paper, we propose a systematic approach to accurately estimate the 3D pose of the mouse from single-monocular fisheye-distorted images. Our approach employs a novel adaptation of a structured forest algorithm. We benchmark our algorithm against existing methods. We demonstrate the utility of the pose estimates in predicting mouse behavior in a continuous video. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Image Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137295220&site=ehost-live
322,Training neural network classifiers through Bayes risk minimization applying unidimensional Parzen windows.,Monson Hayes,Pattern Recognition,313203,,May-18,77,,204,12,128046308,10.1016/j.patcog.2017.12.018,Elsevier B.V.,Article,ARTIFICIAL neural networks; ARTIFICIAL intelligence; ALGORITHMS; PROBABILITY theory; SUPPORT vector machines,Bayes risk; Binary classification; Parzen windows,"A new training algorithm for neural networks in binary classification problems is presented. It is based on the minimization of an estimate of the Bayes risk by using Parzen windows applied to the final one-dimensional nonlinear transformation of the samples to estimate the probability of classification error. This leads to a very general approach to error minimization and training, where the risk that is to be minimized is defined in terms of integrated one-dimensional Parzen windows, and the gradient descent algorithm used to minimize this risk is a function of the window that is used. By relaxing the constraints that are typically applied to Parzen windows when used for probability density function estimation, for example by allowing them to be non-symmetric or possibly infinite in duration, an entirely new set of training algorithms emerge. In particular, different Parzen windows lead to different cost functions, and some interesting relationships with classical training methods are discovered. Experiments with synthetic and real benchmark datasets show that with the appropriate choice of window, fitted to the specific problem, it is possible to improve the performance of neural network classifiers over those that are trained using classical methods. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128046308&site=ehost-live
323,The Cost on System Performance of Requirements on Differentiable Variables.,George Hazelrigg,Journal of Mechanical Design,10500472,,May-21,143,5,1,7,150160346,10.1115/1.4049336,American Society of Mechanical Engineers,Article,SYSTEMS design; TECHNICAL specifications; SYSTEMS engineering,design methodology; design optimization; design process; design theory and methodology; systems engineering,"System design is commonly thought of as a process of maximizing a design objective subject to constraints, among which are the system requirements. Given system-level requirements, a convenient management approach is to disaggregate the system into subsystems and to ""flowdown"" the system-level requirements to the subsystem or lower levels. We note, however, that requirements truly are constraints, and they typically impose a penalty on system performance. Furthermore, disaggregation of the system-level requirements into the flowdown requirements creates added sets of constraints, all of which have the potential to impose further penalties on overall system performance. This is a highly undesirable effect of an otherwise beneficial system design management process. This article derives conditions that may be imposed on the flowdown requirements to assure that they do not penalize overall system performance beyond the system-level requirement. [ABSTRACT FROM AUTHOR] Copyright of Journal of Mechanical Design is the property of American Society of Mechanical Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150160346&site=ehost-live
324,Toward a Theory of Systems Engineering.,George Hazelrigg,Journal of Mechanical Design,10500472,,Jan-22,144,1,1,8,154031022,10.1115/1.4051873,American Society of Mechanical Engineers,Article,SYSTEMS engineering; ENGINEERING systems; PHYSICAL laws; TEST systems; TEST validity,decision theory; design optimization; design theory and methodology; systems design; systems engineering,"The derivation of a theory of systems engineering has long been complicated by the fact that there is little consensus within the systems engineering community regarding precisely what systems engineering is, what systems engineers do, and what might constitute reasonable systems engineering practices. To date, attempts at theories fail to accommodate even a sizable fraction of the current systems engineering community, and they fail to present a test of validity of systems theories, analytical methods, procedures, or practices. This article presents a more theoretical and more abstract approach to the derivation of a theory of systems engineering that has the potential to accommodate a broad segment of the systems engineering community and present a validity test. It is based on a simple preference statement: ""I want the best system I can get."" From this statement, it is argued that a very rich theory can be obtained. However, most engineering disciplines are framed around a core set of widely accepted physical laws; to the authors' knowledge, this is the first attempt to frame an engineering discipline around a preference. [ABSTRACT FROM AUTHOR] Copyright of Journal of Mechanical Design is the property of American Society of Mechanical Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154031022&site=ehost-live
325,A Mechanistic Model of Annual Sulfate Concentrations in the United States.,Lucas Henneman,Journal of the American Statistical Association,1621459,,Sep-22,117,539,1082,12,159083313,10.1080/01621459.2022.2027774,Taylor & Francis Ltd,Article,COAL-fired power plants; CHEMICAL processes; EICOSAPENTAENOIC acid; AIR pollution; SULFATES; AIR quality; POWER plants; SULFUR dioxide; UNITED States; UNITED States. Environmental Protection Agency; Fossil Fuel Electric Power Generation; Power and Communication Line and Related Structures Construction; Other Basic Inorganic Chemical Manufacturing; All other basic inorganic chemical manufacturing; Administration of Air and Water Resource and Solid Waste Management Programs,Air pollution; Ornstein-Uhlenbeck process; SDEs; Space-time processes; Spatial statistics,"Understanding how individual pollution sources contribute to ambient sulfate pollution is critical for assessing past and future air quality regulations. Since attribution to specific sources is typically not encoded in spatial air pollution data, we develop a mechanistic model which we use to estimate, with uncertainty, the contribution of ambient sulfate concentrations attributable specifically to sulfur dioxide (SO2) emissions from individual coal-fired power plants in the central United States. We propose a multivariate Ornstein–Uhlenbeck (OU) process approximation to the dynamics of the underlying space-time chemical transport process, and its distributional properties are leveraged to specify novel probability models for spatial data that are viewed as either a snapshot or time-averaged observation of the OU process. Using US EPA SO2 emissions data from 193 power plants and state-of-the-art estimates of ground-level annual mean sulfate concentrations, we estimate that in 2011—a time of active power plant regulatory action—existing flue-gas desulfurization (FGD) technologies at 66 power plants reduced population-weighted exposure to ambient sulfate by 1.97 μg/m3 (95% CI: 1.80–2.15). Furthermore, we anticipate future regulatory benefits by estimating that installing FGD technologies at the five largest SO2-emitting facilities would reduce human exposure to ambient sulfate by an additional 0.45 μg/m3 (95% CI: 0.33–0.54). for this article are available online. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Statistical Association is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159083313&site=ehost-live
326,Counterfactual time series analysis of short-term change in air pollution following the COVID-19 state of emergency in the United States.,Lucas Henneman,Scientific Reports,20452322,,12/7/21,11,1,1,13,153996231,10.1038/s41598-021-02776-0,Springer Nature,Article,COVID-19 pandemic; AIR pollution; BOX-Jenkins forecasting; AIR quality standards; PARTICULATE matter; AUTOREGRESSIVE models; NEVADA,,"Lockdown measures implemented in response to the COVID-19 pandemic produced sudden behavioral changes. We implement counterfactual time series analysis based on seasonal autoregressive integrated moving average models (SARIMA), to examine the extent of air pollution reduction attained following state-level emergency declarations. We also investigate whether these reductions occurred everywhere in the US, and the local factors (geography, population density, and sources of emission) that drove them. Following state-level emergency declarations, we found evidence of a statistically significant decrease in nitrogen dioxide (NO2) levels in 34 of the 36 states and in fine particulate matter (PM2.5) levels in 16 of the 48 states that were investigated. The lockdown produced a decrease of up to 3.4 µg/m3 in PM2.5 (observed in California) with range (− 2.3, 3.4) and up to 11.6 ppb in NO2 (observed in Nevada) with range (− 0.6, 11.6). The state of emergency was declared at different dates for different states, therefore the period ""before"" the state of emergency in our analysis ranged from 8 to 10 weeks and the corresponding ""after"" period ranged from 8 to 6 weeks. These changes in PM2.5 and NO2 represent a substantial fraction of the annual mean National Ambient Air Quality Standards (NAAQS) of 12 µg/m3 and 53 ppb, respectively. As expected, we also found evidence that states with a higher percentage of mobile source emissions (obtained from 2014) experienced a greater decline in NO2 levels after the lockdown. Although the socioeconomic restrictions are not sustainable, our results provide a benchmark to estimate the extent of achievable air pollution reductions. Identification of factors contributing to pollutant reduction can help guide state-level policies to sustainably reduce air pollution. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153996231&site=ehost-live
327,Racial/Ethnic Disparities in Nationwide PM2.5 Concentrations: Perils of Assuming a Linear Relationship.,Lucas Henneman,Environmental Health Perspectives,916765,,Jul-22,130,7,077701-1,3,158397459,10.1289/ehp11048,Superintendent of Documents,Article,"PARTICULATE matter; RACE; INDOOR air pollution; PSYCHOSOCIAL factors; HEALTH equity; ETHNIC groups; ENVIRONMENTAL justice; METROPOLITAN areas; RESIDENTIAL patterns; ENVIRONMENTAL exposure; MEDICAL research; UNITED States; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"The article discusses research which explores the departures from linearity in the association between racial/ethnic composition and particulate matter 2:5 micrometers in aerodynamic diameter concentration across the U.S. in 2010. Topics covered include a comparison of the strength of the nonlinear association to the linear one and high-light implications for the quantification of resulting racial/ethnic disparities, and segments of the population for whom exposure has been underestimated.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158397459&site=ehost-live
328,Multiple platelet-rich plasma preparations can solubilize freeze-dried chitosan formulations to form injectable implants for orthopedic indications.,Caroline Hoemann,Bio-Medical Materials & Engineering,9592989,,2019,30,4,349,16,138732649,10.3233/BME-191058,IOS Press,Article,PLATELET-rich plasma; ORTHOPEDIC implants; CALCIUM chloride; MOLAR mass; CHITOSAN; PLANT growth promoting substances,Chitosan; injectable implants; platelet-rich plasma,"BACKGROUND: Platelet-rich plasma (PRP) has been used to solubilize freeze-dried chitosan (CS) formulations to form injectable implants for tissue repair. OBJECTIVE: To determine whether the in vitro performance of the formulations depends on the type of PRP preparation used to solubilize CS. METHODS: Formulations containing 1% (w/v) CS with varying degrees of deacetylation (DDA 80.5–84.8%) and number average molar mass (Mn 32–55 kDa), 1% (w/v) trehalose and 42.2 mM calcium chloride were freeze-dried. Seven different PRP preparations were used to solubilize the formulations. Controls were recalcified PRP. RESULTS: CS solubilization was achieved with all PRP preparations. CS-PRP formulations were less runny than their corresponding PRP controls. All CS-PRP formulations had a clotting time below 9 minutes, assessed by thromboelastography, while the leukocyte-rich PRP controls took longer to coagulate (>32 min), and the leukocyte-reduced PRP controls did not coagulate in this dynamic assay. In glass culture tubes, all PRP controls clotted, expressed serum and retracted (43–82% clot mass lost) significantly more than CS-PRP clots (no mass lost). CS dispersion was homogenous within CS-PRP clots. CONCLUSIONS: In vitro performance of the CS-PRP formulations was comparable for all types of PRPs assessed. [ABSTRACT FROM AUTHOR] Copyright of Bio-Medical Materials & Engineering is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138732649&site=ehost-live
329,Synthetic anionic surfaces can replace microparticles in stimulating burst coagulation of blood plasma.,Caroline Hoemann,Colloids & Surfaces B: Biointerfaces,9277765,,Mar-19,175,,596,10,134616141,10.1016/j.colsurfb.2018.11.066,Elsevier B.V.,Article,BLOOD viscosity; FEAR of blood; CARDIOVASCULAR system; BLOOD diseases; HUMAN anatomy,Blood coagulation; Contact osteogenesis; Contact pathway; FXIa; FXII; Hydroxyapatite; Microparticles; Plasma enhanced chemical vapor deposition (PECVD); Thrombin; Thromboelastography,"Graphical abstract Highlights • Microparticle-depleted blood plasma failed to coagulate in plastic cups. • Polyethylene-carboxylate L-PPE:O nanocoatings were created by glow-discharge plasma. • Hydroxyapatite particles, glass microbeads, and L-PPE:O coatings had anionic surfaces. • Anionic surfaces induced burst coagulation of microparticle-depleted plasma via FXII. • Microparticles and anionic surfaces can activate thrombin without platelet activation. Abstract Biomaterials are frequently evaluated for pro-coagulant activity but usually in the presence of microparticles (MPs), cell-derived vesicles in blood plasma whose phospholipid surfaces allow coagulation factors to set up as functional assemblies. We tested the hypothesis that synthetic anionic surfaces can catalyze burst thrombin activation in human blood plasma in the absence of MPs. In a thromboelastography (TEG) assay with plastic sample cups and pins, recalcified human citrated platelet-poor plasma spontaneously burst-coagulated but with an unpredictable clotting time whereas plasma depleted of MPs by ultracentrifugation failed to coagulate. Coagulation of MP-depleted plasma was restored in a dose-dependent manner by glass microbeads, hydroxyapatite nanoparticles (HA NPs), and carboxylic acid-containing anionic nanocoatings of TEG cups and pins (coated by glow-discharge plasma-polymerized ethylene containing oxygen, L-PPE:O with 4.4 and 6.8 atomic % [COOH]). Glass beads lost their pro-coagulant activity in MP-depleted plasma after their surfaces were nanocoated with hydrophobic plasma-polymerized hexamethyl disiloxane (PP-HMDSO). In FXII-depleted MP-depleted plasma, glass microbeads failed to induce coagulation, however, FXIa was sufficient to induce coagulation in a dose-dependent manner, with no effect of glass beads. These data suggest that anionic surfaces of crystalline, organic, and amorphous solid synthetic materials catalyze explosive thrombin generation in MP-depleted plasma by activating the FXII-dependent intrinsic contact pathway. The data also show that microparticles are pro-coagulant surfaces whose activity has been largely overlooked in many coagulation studies to-date. These results suggest a possible mechanism by which anionic biomaterial surfaces induce bone healing by contact osteogenesis, through fibrin clot formation in the absence of platelet activation. [ABSTRACT FROM AUTHOR] Copyright of Colloids & Surfaces B: Biointerfaces is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134616141&site=ehost-live
330,UDP-glucose dehydrogenase (UGDH) activity is suppressed by peroxide and promoted by PDGF in fibroblast-like synoviocytes: Evidence of a redox control mechanism.,Caroline Hoemann,PLoS ONE,19326203,,9/15/22,17,9,1,24,159132848,10.1371/journal.pone.0274420,Public Library of Science,Article,NAD (Coenzyme); SYNOVIAL membranes; GLUTATHIONE reductase; PEROXIDES; SERUM-free culture media; SYNOVIAL fluid; CYTOSOL; All other basic inorganic chemical manufacturing; Other Basic Inorganic Chemical Manufacturing,,"UDP-glucose dehydrogenase (UGDH) generates essential precursors of hyaluronic acid (HA) synthesis, however mechanisms regulating its activity are unclear. We used enzyme histostaining and quantitative image analysis to test whether cytokines that stimulate HA synthesis upregulate UGDH activity. Fibroblast-like synoviocytes (FLS, from N = 6 human donors with knee pain) were cultured, freeze-thawed, and incubated for 1 hour with UDP-glucose, NAD+ and nitroblue tetrazolium (NBT) which allows UGDH to generate NADH, and NADH to reduce NBT to a blue stain. Compared to serum-free medium, FLS treated with PDGF showed 3-fold higher UGDH activity and 6-fold higher HA release, but IL-1beta/TGF-beta1 induced 27-fold higher HA release without enhancing UGDH activity. In selected proliferating cells, UGDH activity was lost in the cytosol, but preserved in the nucleus. Cell-free assays led us to discover that diaphorase, a cytosolic enzyme, or glutathione reductase, a nuclear enzyme, was necessary and sufficient for NADH to reduce NBT to a blue formazan dye in a 1-hour timeframe. Primary synovial fibroblasts and transformed A549 fibroblasts showed constitutive diaphorase/GR staining activity that varied according to supplied NADH levels, with relatively stronger UGDH and diaphorase activity in A549 cells. Unilateral knee injury in New Zealand White rabbits (N = 3) stimulated a coordinated increase in synovial membrane UGDH and diaphorase activity, but higher synovial fluid HA in only 2 out of 3 injured joints. UGDH activity (but not diaphorase) was abolished by N-ethyl maleimide, and inhibited by peroxide or UDP-xylose. Our results do not support the hypothesis that UGDH is a rate-liming enzyme for HA synthesis under catabolic inflammatory conditions that can oxidize and inactivate the UGDH active site cysteine. Our novel data suggest a model where UGDH activity is controlled by a redox switch, where intracellular peroxide inactivates, and high glutathione and diaphorase promote UGDH activity by maintaining the active site cysteine in a reduced state, and by recycling NAD+ from NADH. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159132848&site=ehost-live
331,Congestion pricing applications to manage high temporal demand for public services and their relevance to air space management.,Karla Hoffman,Transport Policy,0967070X,,Jul-13,28,,28,14,89278783,10.1016/j.tranpol.2012.02.004,Elsevier B.V.,Article,"CONGESTION pricing; MUNICIPAL services; AIRSPACE (Law); SURVEYS; GOVERNMENT agencies; POLITICAL science; SOCIAL sciences; Other federal government public administration; Other provincial and territorial public administration; Other local, municipal and regional public administration; Other General Government Support; Administration of Public Health Programs; Administration of Urban Planning and Community and Rural Development; Administration of Air and Water Resource and Solid Waste Management Programs; Administration of Housing Programs; Research and Development in the Social Sciences and Humanities",Airspace congestion; Auctions; Market-clearing mechanism; Runway service standards,"Abstract: This paper surveys pricing mechanisms used by government agencies to manage congestion, as well as highlights the many political and social issues that have to be addressed in order to implement a pricing mechanism. This survey was undertaken in order to be able to understand how congestion pricing could be used to help manage airspace capacity. This is an important question since a 2008 analysis by the Joint Economic Committee of the US Congress suggested that domestic air traffic delays in 2007 cost the economy as much as $41 billion, including $19 billion in increased operational costs for the airlines and $12 billion worth of lost time for passengers. The paper begins by surveying roadway congestion approaches throughout the world. We survey the successes that peak pricing charges have had on reducing congestion. We also report the other benefits that such practices have had: improving the public transportation network, improving the economy of the region, reducing carbon emissions, and creating new urban living spaces. We next examine other applications of congestion pricing, including managing demand for canal and bridges passage, port usage, access to city centers, and peak use of energy resources. The paper ends with a proposal for a two-staged approach to the management of air space and runway congestion. The first stage imposes a service standard on runway access that is consistent with an airport's capacity during good weather days. Then, when weather reduces capacity either in the airspace or on runways, we propose a congestion pricing mechanism that charges flights based on the amount of congestion the flight imposes on the entire system. [Copyright &y& Elsevier] Copyright of Transport Policy is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89278783&site=ehost-live
332,Estimating domestic US airline cost of delay based on European model.,Karla Hoffman,Transportation Research: Part C,0968090X,,Aug-13,33,,311,13,89307131,10.1016/j.trc.2011.10.003,Elsevier B.V.,Article,FLIGHT delays & cancellations (Airlines); AERONAUTICAL flights; COMMERCIAL aeronautics; AIRLINE industry; FEEDBACK control systems; TRANSPORTATION research; UNITED States; EUROPE; Scheduled air transportation; Non-scheduled specialty flying services; Nonscheduled Chartered Passenger Air Transportation; Scheduled Freight Air Transportation; Scheduled Passenger Air Transportation; Other support activities for transportation; All Other Support Activities for Transportation,Airline delay costs; Airline delays; Component; Economic modeling of airlines,"Abstract: Researchers are applying more holistic approaches to the feedback control of the air transportation system. Many of these approaches rely on economic feedback, including the cost of delays to the airlines. Establishing an accurate mechanism for estimating the cost of a delays for each portion of a flight (gate costs, taxiing in and out costs, and en-route costs) is useful for many aspects of modeling airline behavior and for better understanding the likely impact of regulations. Cook et al. (2004) developed a rigorous methodology and collected data for estimating the components of airline delay costs for various segments of a scheduled flight. The model, based on confidential information from European airlines for 12 types of aircraft ca. 2003, was not transparent with regards to how each of the major components of cost (crew costs, fuel costs, maintenance, depreciation, etc.) impacted that total. This paper describes the development of an airline cost model, based on the Eurocontrol model. The airline cost model explicitly identifies the components of airline costs, is based on US airline cost data, and includes 111 aircraft types. The new model is designed to allow costs to be updated whenever the basic costs change. It considers the type of the aircraft when making calculations, both from the perspective of fuel burn and other costs and uses publically available data on burn rates and block-hour direct operating costs (BHDOC) to obtain these estimates. A case-study analysis of airline costs of operation at 19 major US airports is provided. [Copyright &y& Elsevier] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89307131&site=ehost-live
332,Estimating domestic US airline cost of delay based on European model.,Lance Sherry,Transportation Research: Part C,0968090X,,Aug-13,33,,311,13,89307131,10.1016/j.trc.2011.10.003,Elsevier B.V.,Article,FLIGHT delays & cancellations (Airlines); AERONAUTICAL flights; COMMERCIAL aeronautics; AIRLINE industry; FEEDBACK control systems; TRANSPORTATION research; UNITED States; EUROPE; Scheduled air transportation; Non-scheduled specialty flying services; Nonscheduled Chartered Passenger Air Transportation; Scheduled Freight Air Transportation; Scheduled Passenger Air Transportation; Other support activities for transportation; All Other Support Activities for Transportation,Airline delay costs; Airline delays; Component; Economic modeling of airlines,"Abstract: Researchers are applying more holistic approaches to the feedback control of the air transportation system. Many of these approaches rely on economic feedback, including the cost of delays to the airlines. Establishing an accurate mechanism for estimating the cost of a delays for each portion of a flight (gate costs, taxiing in and out costs, and en-route costs) is useful for many aspects of modeling airline behavior and for better understanding the likely impact of regulations. Cook et al. (2004) developed a rigorous methodology and collected data for estimating the components of airline delay costs for various segments of a scheduled flight. The model, based on confidential information from European airlines for 12 types of aircraft ca. 2003, was not transparent with regards to how each of the major components of cost (crew costs, fuel costs, maintenance, depreciation, etc.) impacted that total. This paper describes the development of an airline cost model, based on the Eurocontrol model. The airline cost model explicitly identifies the components of airline costs, is based on US airline cost data, and includes 111 aircraft types. The new model is designed to allow costs to be updated whenever the basic costs change. It considers the type of the aircraft when making calculations, both from the perspective of fuel burn and other costs and uses publically available data on burn rates and block-hour direct operating costs (BHDOC) to obtain these estimates. A case-study analysis of airline costs of operation at 19 major US airports is provided. [Copyright &y& Elsevier] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89307131&site=ehost-live
333,Optimum Airport Capacity Utilization under Congestion Management: A Case Study of New York LaGuardia Airport.,Karla Hoffman,Transportation Planning & Technology,3081060,,Feb-08,31,1,93,20,28462559,10.1080/03081060701835779,Taylor & Francis Ltd,Article,TRAFFIC congestion; AIRLINE industry; SUPPLY & demand; FLIGHT delays & cancellations (Airlines); AIRPORT management; UNITED States; Scheduled air transportation; Scheduled Passenger Air Transportation; Other Airport Operations,airline; airport management; Congestion management; flight delays; regulation; supply–demand; supply-demand; upgauging,"In the United States, most airports do not place any limitations on airline schedules. At a few major airports, the current scheduling restrictions (mostly administrative measures) have not been sufficiently strict to avoid consistent delays and have raised debates about both the efficiency and the fairness of the allocations. With a forecast of 1.1 billion yearly air travelers within the US by 2015, airport expansion and technology enhancement alone are not enough to cope with the competition-driven scheduling practices of the airline industry. The policy legacy needs to change to be consistent with airport capacities. Flights on US airlines arrived late more often in the first four months of 2007 than in any other year since the government began tracking delays, and flight cancellations increased 91% over 2006. With a forecast of 1.1 billion yearly air travelers within the US by 2015, airport expansion and technology enhancement alone are not enough to cope with the competition-driven scheduling practices of the airline industry. Our research studies how flight schedules might change if airlines were required to restrict their schedules to runway capacity. To obtain these schedules, we model a profit-seeking, single benevolent airline whose goal is to maintain current competitive prices and service as many current passengers as possible, while remaining profitable. Our case study demonstrates that at Instrument Meteorological Conditions (IMC) runway rates, the market can find profitable flight schedules that reduce substantially the average flight delay to less than 6 minutes while simultaneously satisfying virtually all of the current demand with average prices remaining unchanged. This is accomplished through significant upgauging to high-demand markets. [ABSTRACT FROM AUTHOR] Copyright of Transportation Planning & Technology is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=28462559&site=ehost-live
334,A multi-objective production planning problem with the consideration of time and cost in clinical trials.,Edward Huang,Expert Systems with Applications,9574174,,Jun-19,124,,25,14,134849047,10.1016/j.eswa.2019.01.038,Elsevier B.V.,Article,"PRODUCTION planning; PROBLEM solving; CLINICAL trials; PRODUCTION quantity; PHARMACEUTICAL industry; SUPPLY chains; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Pharmaceutical and medicine manufacturing; Pharmaceutical Preparation Manufacturing; Pharmaceuticals and pharmacy supplies merchant wholesalers; Drugs and Druggists' Sundries Merchant Wholesalers; Process, Physical Distribution, and Logistics Consulting Services",Clinical trial; Multi-stage stochastic programming; Production and distribution planning; Progressive hedging algorithm; Supply chain management,"Highlights • The proposed algorithm gives better solutions than the direct solution approach. • The operational cost decreases with the increase of the production quantity. • We quantify the tradeoff between the duration and the total cost of clinical trials. • The tradeoff can be used to determine the optimal production quantity. Abstract Under increasingly challenging circumstances, pharmaceutical companies try to reduce the overproduction of clinical drugs, which is commonly seen in the pharmaceutical industry. When the overproduction is simply reduced without an efficient coordination of the inventories in the supply chain, the stock-outs at clinical sites and clinical trial delay can hardly be avoided. In this study, we propose a multi-objective model to optimize the production quantity, where the clinical trial duration and the total production and operational costs are minimized. The problem is formulated as a multi-stage stochastic programming model to capture the dynamic inventory allocation process in the supply chains. Since this problem's solving time and required memory can increase significantly with the increase of the stage and scenario numbers, the progressive hedging algorithm is applied as the solution approach in this paper. In the numerical experiments, we study this algorithm's performance and compare the solving efficiency with the direct solution approach. In addition, we identify the optimal production quantity of clinical drugs and give a discussion about the tradeoffs between the clinical trial delay and total cost. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems with Applications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134849047&site=ehost-live
335,A Study on the Optimal Inventory Allocation for Clinical Trial Supply Chains.,Edward Huang,Applied Mathematical Modelling,0307904X,,Oct-21,98,,161,24,151558812,10.1016/j.apm.2021.04.029,Elsevier B.V.,Article,"SUPPLY chains; STOCHASTIC programming; WAREHOUSES; CLINICAL trials; INVENTORIES; OPERATING costs; PRODUCTION planning; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Process, Physical Distribution, and Logistics Consulting Services; General Warehousing and Storage; Other Warehousing and Storage; Commercial and Institutional Building Construction",Clinical trials; Inventory allocation; Stochastic programming; Supply chain management,"• A multi-period inventory allocation in multi-echelon clinical supply chains is investigated. • The reverse replenishment, transshipment, and stochastic demand are addressed. • A rolling horizon-based two-stage stochastic programming model is developed. • An algorithm extending Benders decomposition for clinical trial supply chains is proposed. • The reformulation method and row generation strategy are developed. With increasing competition in the pharmaceutical industry, pharmaceutical companies pay more attention to improving the efficiency of clinical trial supply chains to reduce the drug supplying cost, which takes up a considerable part of the total research and development expense. To improve the efficiency of clinical trial supply chains, this study investigates the inventory levels of clinical drugs in each period at a distribution center and clinics considering the reverse replenishment, transshipment, and generalized stockout cost. The inventory allocation problem in clinical trial supply chains is formulated as a rolling horizon-based two-stage stochastic mixed-integer model where the minimal operational cost constitutes the underage cost at the production planning level of clinical trial supply chains. An algorithm extending Benders decomposition is proposed as the solution approach. We also derive several structural results and develop the reformulation method and row generation strategy to improve the efficiency of the optimization process. The effectiveness of our approach is demonstrated in the numerical experiment. [ABSTRACT FROM AUTHOR] Copyright of Applied Mathematical Modelling is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151558812&site=ehost-live
336,Combating Sex Trafficking: The Role of the Hotel—Moral and Ethical Questions.,Edward Huang,Religions,20771444,,Feb-22,13,2,138,1,155523047,10.3390/rel13020138,MDPI,Article,SEX trafficking; HUMAN trafficking victims; SEX crimes; SEX industry; CIVIL procedure; HOTEL chains; HOSPITALITY; ATLANTIC Coast (U.S.); Courts; Hotels (except Casino Hotels) and Motels; All other miscellaneous store retailers (except beer and wine-making supplies stores),geographical crime; hospitality industry; human trafficking; illicit supply chain; sex trafficking,"Legitimate companies are key facilitators of human trafficking. These corporate facilitators include not only websites providing advertisements for commercial sex services but also hotels and motels. Analysis of all active federal criminal sex trafficking cases in 2018 and 2019 reveals that in approximately 80% of these cases, victims were exploited at either hotels or motels. This paper studies the prevalence of the hospitality industry in the crime of sex trafficking and the failure of this industry to address this problem until recent civil suits were filed by victims against individual hotels and chains. Drawing on the civil cases filed in federal courts by victims of human trafficking between 2015 and 2021 along the East Coast of the United States, this paper assesses the characteristics of these hotels and the conditions in the hotels that facilitated sex trafficking. The paper then explores the moral and ethical problems posed by the facilitating role of hotel owners/operators in sex trafficking either through collusion or failure to act on and/or report evidence of individual abuse. Suggestions on how to address the problem are provided. [ABSTRACT FROM AUTHOR] Copyright of Religions is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155523047&site=ehost-live
337,Job Dispatch Control for Production Lines With Overlapped Time Window Constraints.,Edward Huang,IEEE Transactions on Semiconductor Manufacturing,8946507,,May-18,31,2,206,9,129455497,10.1109/TSM.2018.2826530,IEEE,Article,SEMICONDUCTOR manufacturing; CONSTRAINTS (Physics); PRODUCT quality; PATTERN recognition systems; MICROFABRICATION; Semiconductor and Related Device Manufacturing,Job scheduling; Job shop scheduling; Microsoft Windows; Process control; semiconductor process modeling; Servers; Time factors; time windows; Workstations,"Semiconductor manufacturing processes often come with stringent requirements for product quality. In order to satisfy these requirements, time window constraints have been imposed. Typically, violation of the time windows can result in a lot being either reworked or scrapped. The presence of overlapped time window constraints renders the control of production lines very challenging as it involves managing the production process of many lots across multiple workstations. We study the performance of a production line with deterministic service times and predetermined, overlapped time windows. We solve the P|rj|C\max problem to estimate the queue time at each workstation, and develop an algorithm to decide whether a new lot can be released for processing at its first workstation so as to meet all time window constraints. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129455497&site=ehost-live
338,Job Scheduling at Cascading Machines.,Edward Huang,IEEE Transactions on Automation Science & Engineering,15455955,,Oct-17,14,4,1634,9,125562292,10.1109/TASE.2017.2698919,IEEE,Article,CASCADING style sheets; PRODUCTION scheduling; DYNAMIC programming; MATHEMATICAL optimization; CUSTOMER services,Cascading; cluster tool; completion time minimization; Dynamic programming; Job shop scheduling; Processor scheduling; Schedules; scheduling,"We consider the serial batching scheduling problem in which a group of machines can process multiple jobs continuously to reduce the processing times of the second and subsequent jobs. The maximum batch size is finite. Since all jobs in the same batch are loaded and unloaded simultaneously, a completed job has to wait for the others. We examine how to schedule all jobs to minimize the total completion time. If the batch size is only one, i.e., a single job per batch, the total processing time will be longer, since no reduction in processing time is possible. As a consequence, the total completion time will also be longer. On the other hand, if the batch size is large, the total completion time can be large. Each job has to wait until all jobs in the same batch are completed. We identify several optimality properties of the optimal batching sequence. These properties are used to develop a dynamic programming algorithm to optimize the batching sequences efficiently. The complexity of the proposed method depends only on the maximum batch size and the number of jobs. The improvement achieved with the proposed method when compared with two other batching rules is illustrated using two practical applications.</p><p>Note to Practitioners— Machines with job cascading (or cluster tools) are commonly seen in semiconductor manufacturing processes. While meeting production move targets is a common goal for the nondelayed jobs, catching up with the schedule is an important task for the delayed jobs to achieve higher customer service level. From the viewpoint of scheduling, reducing the delay is equivalent to reducing completion time. Hence, we propose scheduling algorithms to reduce the completion time by taking advantage of the special structure of a cascading machine. In contrast to existing scheduling theory, the proposed algorithm can be efficient due to the small maximum batch size of real cascading machines. In addition to the theoretical contributions, this paper aims at solving practical problems through a rigorous approach. The finding and insight from this paper can be used to enhance shop floor control in a semiconductor fab. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125562292&site=ehost-live
339,Optimal assignment of airport baggage unloading zones to outgoing flights.,Edward Huang,Transportation Research: Part E,13665545,,Oct-16,94,,110,13,118078450,10.1016/j.tre.2016.07.012,Elsevier B.V.,Article,BAGGAGE handling in airports; CARGO handling; CHUTES; STOCHASTIC analysis; STOCHASTIC processes; Other Airport Operations; Marine Cargo Handling,Airport; Baggage handling system (BHS); Design; Optimization,"The outbound airport baggage handling system (BHS) consists of a set of unloading zones (chutes) which are assigned to outgoing flights. Airport baggage operations have inherent uncertainties such as flight delays and varying number of bags. In this paper, the chute assignment problem is modeled as a Stochastic Vector Assignment Problem (SVAP) and multiple extensions are presented to incorporate the various design needs of the airport. A real airport outbound BHS is presented. This case study also guided the optimization models’ design process. The performance of the optimization models is compared with the methods used in practice and literature. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part E is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=118078450&site=ehost-live
340,Optimal inventory control in a multi-period newsvendor problem with non-stationary demand.,Edward Huang,Advanced Engineering Informatics,14740346,,Jan-15,29,1,139,7,100798048,10.1016/j.aei.2014.12.002,Elsevier B.V.,Article,"INVENTORY control; OPTIMAL control theory; NEWSVENDOR model; SUPPLY chains; ECONOMIC demand; STOCHASTIC programming; Process, Physical Distribution, and Logistics Consulting Services; All Other Support Services",Inventory models; Multi-stage stochastic programming; Newsvendor models; Progressive hedging methods,"The optimal control of inventory in supply chains plays a key role in the competiveness of a corporation. The inventory cost can account for half of company’s logistics cost. The classical inventory models, e.g., newsvendor and EOQ models, assume either a single or infinite planning periods. However, these models may not be applied to perishable products which usually have a certain shelf life. To optimize the total logistic cost for perishable products, this paper presents a multi-period newsvendor model, and the problem is formulated as a multi-stage stochastic programming model with integer recourse decisions. We extend the progressive hedging method to solve the model efficiently. A numerical example and its sensitivity analysis are demonstrated. [ABSTRACT FROM AUTHOR] Copyright of Advanced Engineering Informatics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100798048&site=ehost-live
341,Robust model for the assignment of outgoing flights on airport baggage unloading areas.,Edward Huang,Transportation Research: Part E,13665545,,Jul-18,115,,110,16,129997127,10.1016/j.tre.2018.04.012,Elsevier B.V.,Article,BAGGAGE handling in airports; ROBUST optimization; UNCERTAINTY; LOADING & unloading; CONSUMERS; Other Airport Operations,Airport; Baggage Handling System (BHS); Robust optimization,"One of the major functions of airports is to handle the baggage of customers from check-in counters to aircrafts. To have an efficient baggage handling system (BHS), the assignment of flights on unloading areas must incorporate uncertainties. We develop a robust optimization (RO) model to find a robust plan with constant performance stability in terms of future uncertainty realization. We construct a BHS simulation model to compare different assignment models. The results indicated that the total number of manually handled baggage using the robust plan is 58% lower than that using the current assignment. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part E is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129997127&site=ehost-live
342,Study structure may compromise understanding of longitudinal decision regret stability: A systematic review.,Edward Huang,Patient Education & Counseling,7383991,,Aug-20,103,8,1507,11,143857460,10.1016/j.pec.2020.03.011,Elsevier B.V.,journal article,REGRET; META-analysis; ONLINE databases; TREATMENT effectiveness; TIME perspective; PROSTATE tumors treatment; PATIENT participation; SYSTEMATIC reviews; DECISION making; QUALITY of life; EMOTIONS; PROSTATE tumors,Cancer; Decision regret; Study structure; Time horizon,"<bold>Objectives: </bold>To perform a systematic review of decision regret studies in cancer patients to determine if regret is longitudinally stable, and whether these study structures account for late-emerging treatment effects.<bold>Methods: </bold>Online databases including the George Mason Libraries, Global Health, Nursing and Allied Health, and PubMed were searched to identify decision regret studies with longitudinal components in patients with cancer.<bold>Results: </bold>A total of 845 unique citations were identified; 20 studies met inclusion criteria. Data was also collected on the time horizon for 90 studies; 47 % of studies evaluated regret at time points of one year or less, although this has increased significantly in prostate cancer citations since 2010. Regret was infrequent, affecting less than 20 % of patients, and often stable. Effect sizes in studies where decision regret changed over time were small to negligible.<bold>Conclusion: </bold>Longitudinal effects can influence the expression of decision regret, yet many studies are not designed to collect long-term data; prostate cancer studies may be particularly disadvantaged. The degree of this influence in current studies is small, though this outcome must be interpreted with caution.<bold>Practice Implications: </bold>Providers should be aware of the risk of late-emerging regret and counsel patients appropriately. [ABSTRACT FROM AUTHOR] Copyright of Patient Education & Counseling is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143857460&site=ehost-live
343,Verifying SysML activity diagrams using formal transformation to Petri nets.,Edward Huang,Systems Engineering,10981241,,Jan-20,23,1,118,18,141131403,10.1002/sys.21524,Wiley-Blackwell,Article,"PETRI nets; UNIFIED modeling language; CHARTS, diagrams, etc.; MATHEMATICAL notation; HUMAN behavior models; SET theory",model transformation; model‐based systems engineering; system behavior modeling,"The development of contemporary systems is an extremely complex process. One approach to modeling system behavior uses activity diagrams from Unified Modeling Language (UML)/System Modeling Language (SysML), providing a standard object‐oriented graphical notation and enhancing reusability. However, UML/SysML activity diagrams do not directly support the kind of analysis needed to verify the system behavior, such as might be available with a Petri net (PN) model. We show that a behavior model represented by a set of fUML‐compliant modeling elements in UML/SysML activity diagrams can be transformed into an equivalent PN, so that the analysis capability of PN can be applied. We define a formal mathematical notation for a set of modeling elements in activity diagrams, show the mapping rules between PN and activity diagrams, and propose a formal transformation algorithm. Two example system behavior models represented by UML/SysML activity diagrams are used for illustration. [ABSTRACT FROM AUTHOR] Copyright of Systems Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141131403&site=ehost-live
344,Business Continuity Planning.,Joy Hughes,University Business,10976671,,Sep-06,9,9,23,2,22535112,,LRP Media Group,Article,"SCHOOL risk management; UNIVERSITY & college administration; RISK assessment; VIRGINIA; GEORGE Mason University; Colleges, Universities, and Professional Schools",,The article provides information on the Enterprise Executive Risk Management Group (EERMG) business continuity plan developed by George Mason University in Virginia. EERMG benefits not only the university but individual departments as well in assessing risks. The group identifies which departments were most likely to use business continuity planning instead of requiring every department to fill out risk assessment forms. It operates in trenches where people's concerns are heard and their proposal on policies are acted upon.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22535112&site=ehost-live
345,Presidents and Campus Cybersecurity.,Joy Hughes,Educause Review,15276619,,Nov/Dec2005,40,6,118,2,18913701,,EDUCAUSE,Article,"COMPUTER security; DATA protection; UNIVERSITIES & colleges; UNITED States; BERKELEY (Calif.); BALTIMORE (Md.); FAIRFAX (Va.); CALIFORNIA; MARYLAND; VIRGINIA; UNIVERSITY of California, Berkeley; UNIVERSITY of Maryland at Baltimore; GEORGE Mason University; Colleges, Universities, and Professional Schools",,"This article focuses on the efforts of U.S. college and university presidents to improve their cybersecurity efforts. In an open letter to the campus community on April 4, 2005, Robert J. Birgeneau, the chancellor of the University of California-Berkeley, promised to engage one of the nation's data-security management firms to conduct an external audit of how the campus handles personal information. Freeman A. Hrabowski III, the president of the University of Maryland-Baltimore County, is passionate about auditing. Hrabowski seized the opportunity presented by campus construction projects to authorize a redesign of the university network around security. In November 2002 Alan Merten, president of George Mason University, formed a Privacy and Security Compliance Team.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18913701&site=ehost-live
346,A cross-sectional investigation of SARS-CoV-2 seroprevalence and associated risk factors in children and adolescents in the United States.,Brett Hunter,PLoS ONE,19326203,,11/8/21,16,11,1,14,153460925,10.1371/journal.pone.0259823,Public Library of Science,Article,SARS-CoV-2; COVID-19; SEROPREVALENCE; TEENAGERS; JUVENILE diseases; ANTIBODY titer; ETHNICITY; VIRGINIA,,"Background: Pediatric SARS-CoV-2 data remain limited and seropositivity rates in children were reported as <1% early in the pandemic. Seroepidemiologic evaluation of SARS-CoV-2 in children in a major metropolitan region of the US was performed. Methods: Children and adolescents ≤19 years were enrolled in a cross-sectional, observational study of SARS-CoV-2 seroprevalence from July-October 2020 in Northern Virginia, US. Demographic, health, and COVID-19 exposure information was collected, and blood analyzed for SARS-CoV-2 spike protein total antibody. Risk factors associated with SARS-CoV-2 seropositivity were analyzed. Orthogonal antibody testing was performed, and samples were evaluated for responses to different antigens. Results: In 1038 children, the anti-SARS-CoV-2 total antibody positivity rate was 8.5%. After multivariate logistic regression, significant risk factors included Hispanic ethnicity, public or absent insurance, a history of COVID-19 symptoms, exposure to person with COVID-19, a household member positive for SARS-CoV-2 and multi-family or apartment dwelling without a private entrance. 66% of seropositive children had no symptoms of COVID-19. Secondary analysis included orthogonal antibody testing with assays for 1) a receptor binding domain specific antigen and 2) a nucleocapsid specific antigen had concordance rates of 80.5% and 79.3% respectively. Conclusions: A much higher burden of SARS-CoV-2 infection, as determined by seropositivity, was found in children than previously reported; this was also higher compared to adults in the same region at a similar time. Contrary to prior reports, we determined children shoulder a significant burden of COVID-19 infection. The role of children's disease transmission must be considered in COVID-19 mitigation strategies including vaccination. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153460925&site=ehost-live
346,A cross-sectional investigation of SARS-CoV-2 seroprevalence and associated risk factors in children and adolescents in the United States.,Jiayang Sun,PLoS ONE,19326203,,11/8/21,16,11,1,14,153460925,10.1371/journal.pone.0259823,Public Library of Science,Article,SARS-CoV-2; COVID-19; SEROPREVALENCE; TEENAGERS; JUVENILE diseases; ANTIBODY titer; ETHNICITY; VIRGINIA,,"Background: Pediatric SARS-CoV-2 data remain limited and seropositivity rates in children were reported as <1% early in the pandemic. Seroepidemiologic evaluation of SARS-CoV-2 in children in a major metropolitan region of the US was performed. Methods: Children and adolescents ≤19 years were enrolled in a cross-sectional, observational study of SARS-CoV-2 seroprevalence from July-October 2020 in Northern Virginia, US. Demographic, health, and COVID-19 exposure information was collected, and blood analyzed for SARS-CoV-2 spike protein total antibody. Risk factors associated with SARS-CoV-2 seropositivity were analyzed. Orthogonal antibody testing was performed, and samples were evaluated for responses to different antigens. Results: In 1038 children, the anti-SARS-CoV-2 total antibody positivity rate was 8.5%. After multivariate logistic regression, significant risk factors included Hispanic ethnicity, public or absent insurance, a history of COVID-19 symptoms, exposure to person with COVID-19, a household member positive for SARS-CoV-2 and multi-family or apartment dwelling without a private entrance. 66% of seropositive children had no symptoms of COVID-19. Secondary analysis included orthogonal antibody testing with assays for 1) a receptor binding domain specific antigen and 2) a nucleocapsid specific antigen had concordance rates of 80.5% and 79.3% respectively. Conclusions: A much higher burden of SARS-CoV-2 infection, as determined by seropositivity, was found in children than previously reported; this was also higher compared to adults in the same region at a similar time. Contrary to prior reports, we determined children shoulder a significant burden of COVID-19 infection. The role of children's disease transmission must be considered in COVID-19 mitigation strategies including vaccination. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153460925&site=ehost-live
347,Evolution of tumefactive lesions in multiple sclerosis: A 12-year study with serial imaging in a single patient.,Vasiliki Ikonomidou,Multiple Sclerosis Journal,13524585,,Oct-13,19,11,1539,5,90378632,10.1177/1352458513498124,Sage Publications Inc.,Case Study,MAGNETIC resonance imaging; DIAGNOSTIC imaging; MAGNETIZATION transfer; DISEASE relapse; DEMYELINATION; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,magnetic resonance imaging; magnetization transfer imaging; Multiple sclerosis; recovery; tissue-specific imaging; tumefactive lesion,"We describe the acute presentation and the long-term evolution of recurrent tumefactive lesions (TLs) in a patient with relapsing–remitting multiple sclerosis. Five TLs occurred on three different occasions over a period of 12 years and these were followed by 73 serial magnetic resonance images (MRI). TL evolution was described by means of magnetization transfer imaging (MTI) and cerebrospinal fluid tissue specific imaging (TSI) over the follow-up period. During the study period, the patient had three clinical relapses with only minimal disability progression. MTI demonstrated that only the peripheral portion of each TL reverted to pre-lesional MT ratios within six months’ post-enhancement. Recurring TLs may present a similar pattern of evolution that may be associated with a long-term favourable clinical outcome. [ABSTRACT FROM PUBLISHER] Copyright of Multiple Sclerosis Journal is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=90378632&site=ehost-live
348,Generalized min-max bound-based MRI pulse sequence design framework for wide-range T1 relaxometry: A case study on the tissue specific imaging sequence.,Vasiliki Ikonomidou,PLoS ONE,19326203,,2/21/17,12,2,1,20,121368819,10.1371/journal.pone.0172573,Public Library of Science,Case Study,MAGNETIC resonance imaging; PULSE measurement; DIAGNOSTIC imaging; MONTE Carlo method; MATHEMATICAL optimization; Other Electronic and Precision Equipment Repair and Maintenance; Diagnostic Imaging Centers,Algebra; Anatomy; Biology and life sciences; Body fluids; Cerebrospinal fluid; Diagnostic medicine; Diagnostic radiology; Earth sciences; Geology; Imaging techniques; Linear algebra; Magnetic resonance imaging; Mathematical and statistical techniques; Mathematics; Medicine and health sciences; Monte Carlo method; Nervous system; NMR relaxation; NMR spectroscopy; Optimization; Perturbation (geology); Physical sciences; Physics; Physiology; Radiology and imaging; Relaxation (physics); Research and analysis methods; Research Article; Sedimentary geology; Spectrum analysis techniques; Statistical methods; Statistics (mathematics); Vector spaces,"This paper proposes a new design strategy for optimizing MRI pulse sequences for T1 relaxometry. The design strategy optimizes the pulse sequence parameters to minimize the maximum variance of unbiased T1 estimates over a range of T1 values using the Cramér-Rao bound. In contrast to prior sequences optimized for a single nominal T1 value, the optimized sequence using our bound-based strategy achieves improved precision and accuracy for a broad range of T1 estimates within a clinically feasible scan time. The optimization combines the downhill simplex method with a simulated annealing process. To show the effectiveness of the proposed strategy, we optimize the tissue specific imaging (TSI) sequence. Preliminary Monte Carlo simulations demonstrate that the optimized TSI sequence yields improved precision and accuracy over the popular driven-equilibrium single-pulse observation of T1 (DESPOT1) approach for normal brain tissues (estimated T1 700–2000 ms at 3.0T). The relative mean estimation error (MSE) for T1 estimation is less than 1.7% using the optimized TSI sequence, as opposed to less than 7.0% using DESPOT1 for normal brain tissues. The optimized TSI sequence achieves good stability by keeping the MSE under 7.0% over larger T1 values corresponding to different lesion tissues and the cerebrospinal fluid (up to 5000 ms). The T1 estimation accuracy using the new pulse sequence also shows improvement, which is more pronounced in low SNR scenarios. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121368819&site=ehost-live
349,Heterogeneity of Multiple Sclerosis White Matter Lesions Detected With T2*-Weighted Imaging at 7.0 Tesla.,Vasiliki Ikonomidou,Journal of Neuroimaging,10512284,,Sep-15,25,5,799,8,108899965,10.1111/jon.12193,Wiley-Blackwell,Article,MULTIPLE sclerosis; WHITE matter (Nerve tissue); IRON in the body; MAGNETIC resonance imaging of the brain; BRAIN imaging,7.0 tesla; iron; magnetic resonance imaging; Multiple sclerosis; T2*‐weighted; T2*-weighted,"ABSTRACT BACKGROUND AND PURPOSE Postmortem studies in multiple sclerosis (MS) indicate that in some white matter lesions (WM-Ls), iron is detectable with T2*-weighted (T2*-w), and its reciprocal R2* relaxation rate, magnetic resonance imaging (MRI) at 7.0 Tesla (7T). This iron appears as a hyperintense rim in R2* images surrounding a hypointense core. We describe how this observation relates to clinical/radiological characteristics of patients, in vivo. METHODS We imaged 16 MS patients using 3T and 7T scanners. WM-Ls were identified on T1-w/T2-w 3T-MRIs. Thereafter, WM-Ls with a rim of elevated R2* at 7T were counted and compared to their appearance on conventional MRIs. RESULTS We counted 36 WM-Ls presenting a rim of elevated R2* in 10 patients. Twenty-three (64%) lesions coincided with focal WM-Ls on T2-w MRIs; 13 (36%) coincided with only portions of larger lesions on T2-w images; and 20 (56%) corresponded to a hypointense chronic black hole. WM-Ls presenting a rim of elevated R2* were seen in both relapsing-remitting patients with low disability and in those with long-standing secondary progressive MS. CONCLUSIONS WM-Ls with a contour of high R2* are present at different MS stages, potentially representing differences in the contribution of iron in MS disease evolution. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroimaging is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108899965&site=ehost-live
350,Impact of Chemotherapy for Childhood Leukemia on Brain Morphology and Function.,Vasiliki Ikonomidou,PLoS ONE,19326203,,Nov-13,8,11,1,9,92668990,10.1371/journal.pone.0078599,Public Library of Science,Article,CANCER chemotherapy; LEUKEMIA in children; BRAIN physiology; CHILDHOOD cancer; LYMPHOBLASTIC leukemia; MAGNETIC resonance imaging; MEMORY; PERFORMANCE evaluation; CANCER risk factors; Diagnostic Imaging Centers,Research Article,"Objective: Using multidisciplinary treatment modalities the majority of children with cancer can be cured but we are increasingly faced with therapy-related toxicities. We studied brain morphology and neurocognitive functions in adolescent and young adult survivors of childhood acute, low and standard risk lymphoblastic leukemia (ALL), which was successfully treated with chemotherapy. We expected that intravenous and intrathecal chemotherapy administered in childhood will affect grey matter structures, including hippocampus and olfactory bulbs, areas where postnatal neurogenesis is ongoing. Methods: We examined 27 ALL-survivors and 27 age-matched healthy controls, ages 15–22 years. ALL-survivors developed disease prior to their 11th birthday without central nervous system involvement, were treated with intrathecal and systemic chemotherapy and received no radiation. Volumes of grey, white matter and olfactory bulbs were measured on T1 and T2 magnetic resonance images manually, using FIRST (FMRIB’s integrated Registration and Segmentation Tool) and voxel-based morphometry (VBM). Memory, executive functions, attention, intelligence and olfaction were assessed. Results: Mean volumes of left hippocampus, amygdala, thalamus and nucleus accumbens were smaller in the ALL group. VBM analysis revealed significantly smaller volumes of the left calcarine gyrus, both lingual gyri and the left precuneus. DTI data analysis provided no evidence for white matter pathology. Lower scores in hippocampus-dependent memory were measured in ALL-subjects, while lower figural memory correlated with smaller hippocampal volumes. Interpretation: Findings demonstrate that childhood ALL, treated with chemotherapy, is associated with smaller grey matter volumes of neocortical and subcortical grey matter and lower hippocampal memory performance in adolescence and adulthood. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92668990&site=ehost-live
351,Lesions by tissue specific imaging characterize multiple sclerosis patients with more advanced disease.,Vasiliki Ikonomidou,Multiple Sclerosis Journal,13524585,,Dec-11,17,12,1424,8,67550193,10.1177/1352458511414601,Sage Publications Inc.,Article,MULTIPLE sclerosis; CEREBROSPINAL fluid; MAGNETIC resonance imaging; MYELIN sheath diseases; VIRUS diseases; Diagnostic Imaging Centers,atrophy; Axonal loss; demyelination; MRI; Multiple sclerosis; T2 lesions,"Background: Cerebrospinal fluid tissue specific imaging (CSF-TSI), a newly implemented magnetic resonance imaging (MRI) technique, allows visualization of a subset of chronic black holes (cBHs) with MRI characteristics suggestive of the presence of CSF-like fluid, and representing lesions with extensive tissue destruction.Objective: To investigate the relationship between lesions in CSF-TSI and disease measures in patients with multiple sclerosis (MS).Methods: Twenty-six patients with MS were imaged at 3.0 T, obtaining T1-weighted (T1-w) and T2-w spin echo (SE), T1 volumetric images and CSF-TSI images. We measured: (i) lesion volume (LV) in T1-w (cBH-LV) and T2-w SE images, and in CSF-TSI; (ii) brain parenchyma fraction (BPF). Differences between patients with and without CSF-TSI lesions were analyzed and association between clinical and MRI metrics were investigated.Results: cBHs were seen in 92% of the patients while lesions in CSF-TSI were seen in 40%. Patients with CSF-TSI lesions were older, with longer disease duration, higher disability scores, larger cBH-LV and T2-LV, and lower BPF than patients without CSF-TSI lesions (≤0.047). Partial correlation analysis correcting for T2-LV, cBH-LV and BPF showed an association (p < 0.0001, r = 0.753) between CSF-TSI LV and disability score.Conclusions: CSF-TSI lesions characterize patients with more advanced disease and probably contribute to the progress of disability. [ABSTRACT FROM AUTHOR] Copyright of Multiple Sclerosis Journal is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=67550193&site=ehost-live
352,Untangling the R2* contrast in multiple sclerosis: A combined MRI-histology study at 7.0 Tesla.,Vasiliki Ikonomidou,PLoS ONE,19326203,,3/21/18,13,3,1,19,128594124,10.1371/journal.pone.0193839,Public Library of Science,Article,MULTIPLE sclerosis diagnosis; MULTIPLE sclerosis; BRAIN imaging; MYELIN proteins; PROTEOLIPIDS; MAGNETIC resonance imaging; Diagnostic Imaging Centers,Anatomical pathology; Anatomy; Autoimmune diseases; Biology and life sciences; Brain; Brain diseases; Central nervous system; Clinical immunology; Clinical medicine; Demyelinating disorders; Diagnostic medicine; Diagnostic radiology; Histology; Imaging techniques; Immunology; Magnetic resonance imaging; Medicine and health sciences; Multiple sclerosis; Nervous system; Neurodegenerative diseases; Neuroimaging; Neurology; Neuroscience; Pathology and laboratory medicine; Radiology and imaging; Research and analysis methods; Research Article; Thalamus,"T2*-weighted multi-echo gradient-echo magnetic resonance imaging and its reciprocal R2* are used in brain imaging due to their sensitivity to iron content. In patients with multiple sclerosis who display pathological alterations in iron and myelin contents, the use of R2* may offer a unique way to untangle mechanisms of disease. Coronal slices from 8 brains of deceased multiple sclerosis patients were imaged using a whole-body 7.0 Tesla MRI scanner. The scanning protocol included three-dimensional (3D) T2*-w multi-echo gradient-echo and 2D T2-w turbo spin echo (TSE) sequences. Histopathological analyses of myelin and iron content were done using Luxol fast blue and proteolipid myelin staining and 3,3′-diaminobenzidine tetrahydrochloride enhanced Turnbull blue staining. Quantification of R2*, myelin and iron intensity were obtained. Variations in R2* were found to be affected differently by myelin and iron content in different regions of multiple sclerosis brains. The data shall inform clinical investigators in addressing the role of T2*/R2* variations as a biomarker of tissue integrity in brains of MS patients, in vivo. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128594124&site=ehost-live
353,Validating Nonlinear Registration to Improve Subtraction Images for Lesion Detection and Quantification in Multiple Sclerosis.,Vasiliki Ikonomidou,Journal of Neuroimaging,10512284,,Jan/Feb2018,28,1,70,9,127107858,10.1111/jon.12479,Wiley-Blackwell,journal article,MULTIPLE sclerosis; IMAGE processing; ALGORITHMS; LINEAR systems; QUANTITATIVE research; One-Hour Photofinishing; Photofinishing Laboratories (except One-Hour),image artifacts; Multiple sclerosis; nonlinear registration; subtraction images,"<bold>Background and Purpose: </bold>To propose and validate nonlinear registration techniques for generating subtraction images because of their ability to reduce artifacts and improve lesion detection and lesion volume quantification.<bold>Methods: </bold>Postcontrast T1 -weighted spin echo and T2 -weighted dual echo images were acquired for 20 patients with relapsing-remitting multiple sclerosis (RRMS) on a monthly basis for a year (14 women, average age 33.6 ± 6.9). The T2 -weighted images from the first scan were used as a baseline for each patient. The images from the last scan were registered to the baseline image. Four different registration algorithms used for evaluation included; linear, halfway linear, nonlinear, and nonlinear halfway. Subtraction images were generated after brain extraction, intensity normalization, and Gaussian blurring. Lesion activity changes along with identified artifacts were scored on all four techniques by two independent observers. Additionally, quantitative analysis of the algorithms was performed by estimating the volume changes of simulated lesions and real lesions. For real lesion volume change analysis, five subjects were selected randomly. Subtraction images were generated between all the 11 time points and the baseline image using linear and nonlinear registration for the five subjects.<bold>Results: </bold>Lesion activity detection resulted in similar performance among the four registration techniques. Lesion volume measurements on subtraction images using nonlinear registration were closer to lesion volume on T2 -weighted images. A statistically significant difference was observed among the four registration techniques while evaluating yin-yang artifacts. Pairwise comparisons showed that nonlinear registration results in the least amount of yin-yang artifacts, which are significantly different.<bold>Conclusions: </bold>Nonlinear registration for generation of subtraction images has been demonstrated to be a promising new technique as it shows improvement in lesion activity change detection. This approach decreases the number of artifacts in subtraction images. With improved lesion volume estimates and reduced artifacts, nonlinear registration may lead to discarding less subject data and an improvement in the statistical power of subtraction imaging studies. [ABSTRACT FROM AUTHOR] Copyright of Journal of Neuroimaging is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127107858&site=ehost-live
354,An Empirical Study of Face-to-Face and Distance Learning Sections of a Core Telecommunication course.,Khondkar Islam,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,11,116024952,,ASEE,Article,FACE-to-face communication; DISTANCE education; LEARNING Management System; EDUCATION software; TELECOMMUNICATION systems; All Other Miscellaneous Schools and Instruction; Administration of Education Programs; Educational Support Services; Satellite Telecommunications,,"We present an empirical study that compared the student learning outcomes of face-to-face and distance learning sections of a Telecommunications course. Student performance was assessed based on the course grade, which included the final exam, quizzes, assignments, and midterm exam scores. Both classes were taught by the same instructor, and had similar content and assessment measures. The study factored in the students' demographics such as gender, work and residency status to assess their impact on student learning. In addition, data stored in the learning management system (LMS), BlackBoard ™, were collected and used to understand student activities within the system, and determine their relation with student performance. The number of times the material was accessed and the time duration spent on assessments are some of the examples of the data that were included in the study. The results show that there is a correlation between students' use of Blackboard and student performance. We found a significant statistical difference between course grades of the face-to-face and distance learning sections. We did not find any evidence for significant difference across a range of demographic factors. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116024952&site=ehost-live
354,An Empirical Study of Face-to-Face and Distance Learning Sections of a Core Telecommunication course.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,11,116024952,,ASEE,Article,FACE-to-face communication; DISTANCE education; LEARNING Management System; EDUCATION software; TELECOMMUNICATION systems; All Other Miscellaneous Schools and Instruction; Administration of Education Programs; Educational Support Services; Satellite Telecommunications,,"We present an empirical study that compared the student learning outcomes of face-to-face and distance learning sections of a Telecommunications course. Student performance was assessed based on the course grade, which included the final exam, quizzes, assignments, and midterm exam scores. Both classes were taught by the same instructor, and had similar content and assessment measures. The study factored in the students' demographics such as gender, work and residency status to assess their impact on student learning. In addition, data stored in the learning management system (LMS), BlackBoard ™, were collected and used to understand student activities within the system, and determine their relation with student performance. The number of times the material was accessed and the time duration spent on assessments are some of the examples of the data that were included in the study. The results show that there is a correlation between students' use of Blackboard and student performance. We found a significant statistical difference between course grades of the face-to-face and distance learning sections. We did not find any evidence for significant difference across a range of demographic factors. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116024952&site=ehost-live
355,Analysis and Modeling of Upstream Throughput in Multihop Packet CDMA Cellular Networks.,Bijan Jabbari,IEEE Transactions on Communications,906778,,Apr-06,54,4,680,13,20720526,10.1109/TCOMM.2006.873076,IEEE,Article,CODE division multiple access; SPREAD spectrum communications; TIME division multiple access; WIRELESS communications; SHADOWING theorem (Mathematics); NETWORK routers; POWER transmission; ELECTRONICS; ELECTRICAL engineering; Engineering Services; Other Electronic and Precision Equipment Repair and Maintenance; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Ad hoc networks; multihop networks; network capacity; network modeling; wireless packet code-division multiple access,"We consider the problem of throughput modeling of wireless multihop packet CDMA networks with cellular overlay using simple forwarding strategies in the upstream. Considering the effect of shadowing and distance-dependent path loss, we approximate the probability density of interference at each base station (BS) and compare numerical and simulation results for different path-loss parameters. We derive the probability density of the received power at each BS due to transmission of one packet from a random node, as well as the probability distribution of the number of packets received at each node per time slot. Subsequently, we use the above results to approximate the probability density of the total received power at each BS based on calculations of moments. We observe that the probability density of intercell interference due to transmissions from terminals and routers may be approximated by normal and log-normal densities, respectively. We quantify the network performance based on throughput, total consumed power, and outage probability for different system parameters. For homogeneous link efficiencies, introducing routers into the network while reducing the transmission power increases the mean and variance of interference to the desired signal, hence higher outage probability. However, there are ample opportunities inherent to multihop structure, applicable to any of the physical, data link, and network layers, which help increase the overall achievable network throughput. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Communications is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20720526&site=ehost-live
356,DRAGON: A Framework for Service Provisioning in Heterogeneous Grid Networks.,Bijan Jabbari,IEEE Communications Magazine,1636804,,Mar-06,44,3,84,7,20300193,10.1109/MCOM.2006.1607870,IEEE,Article,OPTICAL communications; RESOURCE allocation; SONET (Data transmission); DATA transmission systems; GRID computing; COMPUTER systems; COMMUNICATION infrastructure; TELECOMMUNICATION systems; TECHNOLOGY; Satellite Telecommunications; Computer Systems Design Services; Computer systems design and related services (except video game design and development),,"Dynamic Resource Allocation in GMPLS Optical Networks (DRAGON) defines a research and experimental framework for high-performance networks required by Grid computing and e-science applications. The DRAGON project is developing technology and deploying network infrastructure which allows dynamic provisioning of network resources in order to establish deterministic paths in direct response to end-user requests. This includes multidomain provisioning of traffic-engineering paths using a distributed control plane across heterogeneous network technologies while including mechanisms for authentication, authorization, accounting (AAA), and scheduling. A reference implementation of this framework has been instantiated in the Washington, DC area and is being utilized to conduct research and development into the deployment of optical networks technologies toward the satisfaction of very-high-performance science application requirements. [ABSTRACT FROM AUTHOR] Copyright of IEEE Communications Magazine is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20300193&site=ehost-live
357,Data-Driven Optimization of Reward-Risk Ratio Measures.,Ran Ji,INFORMS Journal on Computing,10919856,,Summer2021,33,3,1120,18,152160094,10.1287/ijoc.2020.1002,INFORMS: Institute for Operations Research,Article,NONLINEAR programming; SHARPE ratio; SET functions; MODULAR design; FRACTIONAL programming; ROBUST optimization,data-driven optimization; distributionally robust optimization; fractional programming; reward-risk ratio; Wasserstein metric,"We investigate a class of fractional distributionally robust optimization problems with uncertain probabilities. They consist in the maximization of ambiguous fractional functions representing reward-risk ratios and have a semi-infinite programming epigraphic formulation. We derive a new fully parameterized closed-form to compute a new bound on the size of the Wasserstein ambiguity ball. We design a data-driven reformulation and solution framework. The reformulation phase involves the derivation of the support function of the ambiguity set and the concave conjugate of the ratio function. We design modular bisection algorithms which enjoy the finite convergence property. This class of problems has wide applicability in finance, and we specify new ambiguous portfolio optimization models for the Sharpe and Omega ratios. The computational study shows the applicability and scalability of the framework to solve quickly large, industry-relevant-size problems, which cannot be solved in one day with state-of-the-art mixed-integer nonlinear programming (MINLP) solvers. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152160094&site=ehost-live
358,Distributionally robust portfolio optimization with linearized STARR performance measure.,Ran Ji,Quantitative Finance,14697688,,Jan-22,22,1,113,15,154901822,10.1080/14697688.2021.1993623,Taylor & Francis Ltd,Article,ROBUST optimization; DUALITY theory (Mathematics); LINEAR programming; PORTFOLIO performance; STOCK exchanges; Securities and Commodity Exchanges,Conditional value-at-risk; Distributionally robust optimization; LSTARR performance measure; Wasserstein metric,"We study the distributionally robust linearized stable tail adjusted return ratio (DRLSTARR) portfolio optimization problem, in which the objective is to maximize the worst-case linearized stable tail adjusted return ratio (LSTARR) performance measure under data-driven Wasserstein ambiguity. We consider two types of imperfectly known uncertainties, named uncertain probabilities and continuum of realizations, associated with the losses of assets. We account for two typical combinatorial trading constraints, called buy-in threshold and diversification constraints, to reflect stock market restrictions. Leveraging conic duality theory to tackle the distributionally robust worst-case expectation, the proposed problems are reformulated into mixed-integer linear programming problems. We carry out a series of empirical tests to illustrate the scalability and effectiveness of the proposed solution framework, and to evaluate the performance of the DRLSTARR-constructed portfolios. The cross-validation results obtained using a rolling-horizon procedure show the superior out-of-sample performance of the DRLSTARR portfolios under an uncertain continuum of realizations. [ABSTRACT FROM AUTHOR] Copyright of Quantitative Finance is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154901822&site=ehost-live
359,Analytics and patterns of knowledge creation: Experts at work in an online engineering community.,Aditya Johri,Computers & Education,3601315,,Sep-17,112,,18,19,123504643,10.1016/j.compedu.2017.04.011,Elsevier B.V.,Article,DISTANCE education; ENGINEERING; THEORY of knowledge; METAPHOR; EDUCATORS; Administration of Education Programs; All Other Miscellaneous Schools and Instruction,Engineering education; Knowledge creation; Learning analytics; Learning communities,"Online learning communities have gained popularity amongst engineering learners who seek to build knowledge and share their expertise with others; yet to date, limited research has been devoted to the development of analytics for engineering communities. This is addressed through our study of an online engineering community that serves 31,219 engineering learners who contributed 503,908 messages in 65,209 topics. The guiding theoretical framework is the knowledge creation metaphor, which conceptualizes learning as a collaborative process of developing shared knowledge artifacts for the collective benefit of a community of learners. The aims of this study are twofold: (1) to analyze the state of knowledge creation in the community; and (2) to evaluate the strength of association between proposed analytics and variables indicative of knowledge creation in online environments. Findings suggest that the community is vibrant as a whole but also reveal disparity in participation at the individual level. At the topic-level, knowledge creation activities are strongly associated with Topic Length and moderately associated with Topic Duration. At the individual-level, participation in knowledge creation activities is strongly associated with Individual Total Interactions and weakly associated with Individual Total Membership Period. The implications of the findings are discussed and may provide guidance for educators seeking to adopt learning analytics in online communities. [ABSTRACT FROM AUTHOR] Copyright of Computers & Education is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123504643&site=ehost-live
360,Artificial intelligence and engineering education.,Aditya Johri,Journal of Engineering Education,10694730,,Jul-20,109,3,358,4,144725934,10.1002/jee.20326,Wiley-Blackwell,Article,"ENGINEERING education; ARTIFICIAL intelligence; SOCIAL computing; AUTOMATIC control systems; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers",,"I was recently invited to give a research seminar at an academic institution and so I did what most academics do in such cases: I made a PowerPoint (PPT) presentation. This whole process would have gone unnoticed except that in this instance I was preparing a talk on the potential impact of artificial intelligence (AI) on engineering education. Most AI experts agree that although artificial general intelligence, the kind of AI that is just like humans, is a long way off, artificial narrow intelligence (ANI), which is programmed to perform a single task, is making rapid progress. The recent National AI R&D Plan (Parker, 2018) and NSF investment in AI Institutes are timely reminders that we indeed need to pay attention to AI. [Extracted from the article] Copyright of Journal of Engineering Education is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144725934&site=ehost-live
361,Contextual Constraining of Student Design Practices.,Aditya Johri,Journal of Engineering Education,10694730,,Jul-15,104,3,252,27,108426192,10.1002/jee.20079,Wiley-Blackwell,Article,ENGINEERING design education in universities & colleges; ENGINEERING teachers; ENGINEERING student research; STRUCTURATION theory; EFFECTIVE teaching; Engineering Services,engineering design education; nested structuration theory; qualitative case study,"Background Engineering design is of significant interest to engineering educators. As yet, how the higher education context shapes student outcomes in engineering design courses remains underexplored. Since design courses are the primary way students are taught the critical topic of design, it is important to understand how the institutional and organizational contexts shape student outcomes and how we could improve design projects, given the context. Purpose We sought to answer two questions: What aspects of the design education process are salient, or important, for students? How do these salient aspects affect their design practices? Design/Method We used a qualitative case study approach to address the research questions because of our emphasis on understanding process-related aspects of design work and developing an interpretive understanding from the students' perspective. Results Using a nested structuration framework, we show that the context of design practices shaped students' outcomes by constraining their approach to the project and by providing a framework for their design process. We provide recommendations for design educators to help students overcome impediments to achieving learning objectives for design activities. Our research questions the efficacy of teaching engineering design when a design problem lacks a context beyond the classroom. Conclusions The institutional and organizational contexts influence student design practices. Engineering educators should carefully consider the potential effects of the design projects they implement within a higher education context. [ABSTRACT FROM AUTHOR] Copyright of Journal of Engineering Education is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108426192&site=ehost-live
362,Curating Tweets: A Framework for Using Twitter forWorkplace Learning.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,8314,19,139581986,,ASEE,Article,,,"Cybersecurity is a rapidly evolving field where professionals constantly need to keep up with new technologies and retrain. In this paper, we present a study that analyzed social media data and use the findings to aid professionals and students to learn more effectively using Twitter. We analyzed 23,000 cybersecurity related tweets posted on Twitter across two hashtags #cybersecurity and #infosec. Our analysis created a framework that explains how using descriptive, content, and network analysis can generate information that can help professionals learn. In addition, our analysis provided insights on the tweets and the cybersecurity community that use them. These insights include: Most tweets covered multiple topics and used three or more hashtags. Companies and other organizations had the highest numbers of followers, but individual users, experts in the field, were the most retweeted. Popular users, based on follower counts, were not necessarily the most influential (based on retweets). In terms of content, popular tweets consisted of infographics that packed a lot of information. Tweets were commonly used to announce file dumps of hacks and data leaks. Many highly used hashtags represented current threats and the overall sentiment of cybersecurity tweets are negative. Highly connected users on Twitter served as hubs across the three primary sub communities identified in the data. Insights from his study can assist with improving workforce development by guiding professionals in getting pertinent information and keeping up to date with the latest security threats and news. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139581986&site=ehost-live
363,Developing and Advancing a Cyberinfrastructure to Gain Insights into Research Investments: An Organizing Research Framework.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,9,116025299,,ASEE,Article,CYBERINFRASTRUCTURE; DISTRIBUTED computing; STEM education; ENGINEERING education; NATIONAL Science Foundation (U.S.),,"Although the National Science Foundation (NSF) funds approximately 24% of basic research conducted in U.S. colleges and universities 1, we know little about how NSF funding decisions have shaped the current research terrain. For instance, what new research areas have emerged and how have faculty collaborated over time? The Deep Insights Anytime, Anywhere (DIA2) project was precipitated by the need to better understand these issues and translate them into easy to understand insights for the STEM education community. We were also motivated to understand how research outcomes, particularly of STEM education projects, impact STEM education practice. As part of this project, the DIA2 project team has designed an information and visualization portal (http://www.dia2.org) that allows researchers and scientists to browse and search public data from NSF to understand the research terrain (including information about research on specific topics and researchers active in the area). There are many challenges associated with developing and using such a cyberinfrastructure, but also many potential advantages for practitioners, researchers, and policymakers. In this paper we discuss the research opportunities provided by DIA2 and present the research framework guiding the DIA2 project--a description of the three major themes/areas of research for the study. The paper summarizes the research questions and research activities corresponding to each of the themes, presents next steps, and based on our findings, highlights the value of DIA2 to members of the STEM education community. These concentrated efforts can not only help us better understand the impact and landscape of STEM education research, but this study can also serve as a framework for other large scale cyber-enabled research projects. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116025299&site=ehost-live
364,"Developing Global Engineering Competency Through Participation in ""Engineers Without Borders"".",Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,14,116025309,,ASEE,Article,ENGINEERING; CORE competencies; ENGINEERING education; ENGINEERS Without Borders (Organization),,"With a growing need for globally competent engineers, global engineering educational experiences, such as Engineers Without Borders (EWB), have become an important potential avenue for teaching students global engineering competencies. The purpose of this qualitative case study was to better understand engineering students' learning experiences in a EWB project, looking specifically at how students participating on the project exhibit attributes of global engineering competencies. The case study investigates an EWB project with the mission of designing and implementing a solar-powered electricity system for a school in Uganda. We found that students do exhibit attributes of global engineering competencies, although attributes regarding engineering cultures and ethics were exhibited more strongly than attributes regarding global regulations and standards. We discuss implications of these findings for educational practice and future research. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116025309&site=ehost-live
365,Engineering Time: Learning Analytics Initiative to Understand how Firstyear Engineering Students Spend their Time.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,10506,12,139582129,,ASEE,Article,,,"This Complete Research paper describes a learning analytics (LA) informed initiative to collect a detailed account of how first-year engineering students spend their time. With a plethora of calls to increase the number of engineering graduates, it is imperative to set students up for success during their first year. While there are multiple strategies infused in students' first year of college, and as many focused towards engineering students, there are still gaps in our understanding of what students do with their time outside of the classroom. This paper presents a study that uses a learning analytics initiative to uncover what students are doing outside the classroom and how they spend their time. Specifically, this study addresses one research question: How do first-year engineering students manage their time? Time management is one of the most critical aspects of a student's success in college. Analyzing time management practices of students can provide valuable information about how they work and what helps them succeed. Our research details a pilot study of 14 first-year engineering students across two weeks during the Fall semester of 2017. Students used a shared Google Sheet to keep track of their activities in half-hour increments using a template created by the research team. The template includes six categories for students to fill-in: date, time, location, activity, course, and notes. Results of the study highlight the daily habits of first-year engineering students with sleep (36.94%), leisure (19.22%), other (11.04%), studying short- and long-term (8.93%), and class (7.89%) as the top four categories where students spend their time. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139582129&site=ehost-live
366,Examining Learner-driven Constructs in Co-curricular Engineering Environments: The Role of Student Reflection in Assessment Development.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2017,,,11464,8,125730503,,ASEE,Article,CURRICULUM; ENGINEERING students; ENGINEERING education; FACULTY-college relationship; FACULTY advisors,,"Informal learning experiences are under-utilized in engineering education. Because of the voluntary nature of these experiences, many students may be unaware of their existence or how to access these experiences. Other students may not understand the benefits and, therefore, opt out. As such, students may be missing opportunities to extend their engineering understanding and skills to make them more competitive in the workforce. Therefore, it is important to examine the learning processes and outcomes supported by informal learning experiences. Co-curricular engineering experiences range from unstructured environments, such as social networking, to structured, such as engineering competitions that more closely approximate the workplace. Such experiences situate learning within an environment that may foster integration of knowledge and skills to solve problems (Pierrakos, Borrego, & Lo, 2007). These informal learning environments represent degrees of complexity. Therefore, students application of design or problem solving within such environments may also lead to other desired outcomes, such as increases in innovative thinking, the development of adaptive expertise, or being able to flexibly navigate multiple types of engineering environments (Kusano & Johri, 2014; Pellegrino, DiBello, & Brophy, 2014; Sawyer & Greeno, 2008; Stevens et al., 2008). However, research about how to assess outcomes attained via participation in informal learning environments is nascent. In this paper, we applied situated learning theory as a theoretical framework because our focus was on learning within engineering competitions (Johri & Olds, 2011; Johri, Olds, & O'Connor, 2014). Situated learning theory allowed us to focus on the activities in which students' engaged in order to understand the interactions that engendered different types of learning. Faculty study learning because it allows us to help students to improve, such as for formative purposes, or to certify that students have learned, such as for summative purposes. Thus, faculty make judgments about student learning based on assessment data. Validity is the most important criteria to examine the nature of the judgments and decisions related to test use. Validity does not exist as a property of a test. Rather, validity is about providing evidence that good and appropriate decisions were made based on assessment data (AERA, APA, & NCME, 2014). Therefore, validation processes must begin with the conception of the assessment. Validity is not simply a box to be checked for due diligence. Rather, validity must undergird every decision that is made to develop the assessment, demonstrate that it works for the purposes intended, and to draw any conclusions from assessment data, whether those decisions are about students, faculty teaching, programs, or policy. Response processes are one source of validity evidence. Gathering data about how students respond to assessment tasks or test items allows psychometricians to understand how learners think about, process, and respond to given performance tasks or test items. Messick (1990) stated, that response processes ""probe the ways in which individuals cope with the items or tasks, in an effort to illuminate the processes underlying item response and task performance"" (p. 5). For this paper, we extended the definition of response processes to elucidate the processes in extended performances, such as a competitions, which mirror workplace learning. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125730503&site=ehost-live
367,Informing the Sharing and Access of Engineering Education Research Data through Comparative Analysis.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,16,116025764,,ASEE,Article,ENGINEERING education; ENGINEERING students; COMPARATIVE studies; LEARNING; STUDENT surveys,,"The rapid growth of engineering education as a field of rigorous research has resulted in an explosion of available data and research results. There are numerous research efforts currently underway that gather data on a variety of topics that have the potential to help us better understand how students learn engineering. However, there are currently no easy methods to synthesize research results, share research data, and indeed validate research studies effectively. In general, topics related to data and data sharing are largely treated as taboos in the engineering education research space. Data sharing mechanisms to enable fundamental research in engineering education that has the potential to address systemic problems have not yet been clarified. The research goal of this paper is to identify and understand patterns for data sharing mechanisms in order to inform design requirements for data sharing practices and infrastructure in engineering education. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116025764&site=ehost-live
368,Lifelong and lifewide learning for the perpetual development of expertise in engineering.,Aditya Johri,European Journal of Engineering Education,3043797,,Feb-22,47,1,70,15,154690360,10.1080/03043797.2021.1944064,Taylor & Francis Ltd,Article,LEARNING; SOFTWARE engineers; ENGINEERING education; EXPERTISE; ETHNOLOGY,digital participation; Engineering expertise; lifelong learning; lifewide learning; virtual ethnography,"Increasing digitisation of engineering and social practices has altered the relationship between formal schooling and development of expertise for professional engineering work. What does the development of expertise look like when knowledge is generated and shared at an accelerated pace due to shifts in technology? In this paper, I present case studies of two early career software engineers. Using methodological insights from digital ethnography, I trace their professional journeys over two decades. I empirically demonstrate how the development of engineering expertise is a continuous and perpetual endeavour and engineers learn throughout their lives (lifelong) and across all the different spaces they inhabit at any given time (lifewide). I argue for extending engineering work practices research and research in engineering education more broadly to take larger timescales of learning into account to build a comprehensive understanding of engineering expertise development. [ABSTRACT FROM AUTHOR] Copyright of European Journal of Engineering Education is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154690360&site=ehost-live
369,"Live, Love, Juul: User and Content Analysis of Twitter Posts about Juul.",Aditya Johri,American Journal of Health Behavior,10873244,,Mar/Apr2019,43,2,326,11,134748482,10.5993/AJHB.43.2.9,PNG Publications,Article,SMOKING prevention; AGE distribution; CHI-squared test; COMMUNICATION; CONTENT analysis; HEALTH education; HEALTH promotion; MARKETING; PRESS; PUBLIC opinion; RESEARCH funding; STATISTICAL sampling; STATISTICS; ELECTRONIC cigarettes; Marketing Consulting Services; News Syndicates; Marketing Research and Public Opinion Polling; Tobacco product manufacturing; Tobacco Manufacturing; TWITTER (Web resource),content analysis; electronic cigarettes; JUUL; smoking; social media; Twitter,"Objectives: In this study, we identified patterns of communication around Juul use and users on Twitter. Methods: Public tweets were collected from April 27, 2018 until June 27, 2018. We categorized 1008 randomly selected tweets on 4 dimensions: user type, sentiment, genre, and theme. Results: Most tweets were through personal accounts followed by ones of the tobacco industry. Participation by anti-tobacco campaigners, educational, and governmental entities was limited. Posts were mostly about first-hand use, use intentions, and personal opinions. Tweets advocating Juul were most common; meanwhile a handful of tweets discouraged Juul use. Young women, young men, and the tobacco industry expressed positive sentiments about Juul. Conclusions: Twitter data are a rich source of public communication to complement surveillance of emerging tobacco products. Youth actively and positively communicate about Juul on Twitter. Educational content and strategies must be examined for curtailing dissemination of positive sentiments and advocacy that normalize and promote Juul use among youth and non-smokers. We observed limited evidence supporting a claim for Juul to be a smoking cessation adjunct. [ABSTRACT FROM AUTHOR] Copyright of American Journal of Health Behavior is the property of PNG Publications and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134748482&site=ehost-live
370,Needle in a haystack: Identifying learner posts that require urgent response in MOOC discussion forums.,Aditya Johri,Computers & Education,3601315,,Mar-18,118,,1,9,126946178,10.1016/j.compedu.2017.11.002,Elsevier B.V.,Article,ONLINE education; COLLABORATIVE learning; EDUCATIONAL cooperation; DATA mining; BIG data,Computer-mediated communication; Improving classroom teaching; MOOC; Navigation,"Although massive open online courses or MOOCs have been successful in attracting a large number of learners, they have not been equally successful in retaining the learners to the point of course completion. One critical point of failure in many courses, especially those that use discussion forums as a means of collaborative learning, is the large number of messages exchanged on the forums. The extensive exchange of messages often creates chaos from the instructors' perspective and several questions remain unanswered. Lack of attention and response to urgent messages – those that are critical from the learners’ perspective to move forward – becomes a major challenge in this environment. This paper proposes a model to identify “urgent” posts that need immediate attention from instructors. In our analysis, we investigate different feature sets and different data mining techniques, and report the best set of features and classification techniques for addressing the problem of identifying messages that need urgent attention. The results demonstrate the ability to use a limited number of linguistic features with select metadata to build a moderate to substantially reliable classification model that can identify urgent posts in MOOC forums regardless of the course content. The work has potential application across a range of platforms that provide large scale courses and can help instructors efficiently navigate the discussion forums and prioritize the responses so that timely intervention can support learning and may reduce dropout rates. [ABSTRACT FROM AUTHOR] Copyright of Computers & Education is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126946178&site=ehost-live
370,Needle in a haystack: Identifying learner posts that require urgent response in MOOC discussion forums.,Huzefa Rangwala,Computers & Education,3601315,,Mar-18,118,,1,9,126946178,10.1016/j.compedu.2017.11.002,Elsevier B.V.,Article,ONLINE education; COLLABORATIVE learning; EDUCATIONAL cooperation; DATA mining; BIG data,Computer-mediated communication; Improving classroom teaching; MOOC; Navigation,"Although massive open online courses or MOOCs have been successful in attracting a large number of learners, they have not been equally successful in retaining the learners to the point of course completion. One critical point of failure in many courses, especially those that use discussion forums as a means of collaborative learning, is the large number of messages exchanged on the forums. The extensive exchange of messages often creates chaos from the instructors' perspective and several questions remain unanswered. Lack of attention and response to urgent messages – those that are critical from the learners’ perspective to move forward – becomes a major challenge in this environment. This paper proposes a model to identify “urgent” posts that need immediate attention from instructors. In our analysis, we investigate different feature sets and different data mining techniques, and report the best set of features and classification techniques for addressing the problem of identifying messages that need urgent attention. The results demonstrate the ability to use a limited number of linguistic features with select metadata to build a moderate to substantially reliable classification model that can identify urgent posts in MOOC forums regardless of the course content. The work has potential application across a range of platforms that provide large scale courses and can help instructors efficiently navigate the discussion forums and prioritize the responses so that timely intervention can support learning and may reduce dropout rates. [ABSTRACT FROM AUTHOR] Copyright of Computers & Education is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126946178&site=ehost-live
371,Retention and Persistence among STEM Students: A Comparison of Direct Admit and Transfer Students across Engineering and Science.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2017,,,1,11,125729975,,ASEE,Article,STEM education; COLLEGE dropouts; TRANSFER students; ENGINEERING; SCIENCE,,"Improving student retention in particular science, technology, engineering and mathematics majors has focused on identifying strategies, and practices that will encourage students to complete a degree in STEM major. In this paper, we present findings from a study of retention and migration among STEM students, comparing rates across both engineering and science students. We look at all students admitted between 2009- 2014, both direct admits and transfer, at a large public university. Transfer students are often neglected in studies of retention and persistence especially in engineering. We found that engineering students are more persistent than science students with retention rates over 60% for engineering students compared to 40% in math. Persistence rates for firsttime students are less than transfer students in the engineering enrollments. Also, as in previous studies, most migration out of discipline occurs in the first two years of enrollment. We also found that among enrolled students, a large number of engineering students (almost 20%) have not declared a major some until later in their studies. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125729975&site=ehost-live
371,Retention and Persistence among STEM Students: A Comparison of Direct Admit and Transfer Students across Engineering and Science.,Huzefa Rangwala,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2017,,,1,11,125729975,,ASEE,Article,STEM education; COLLEGE dropouts; TRANSFER students; ENGINEERING; SCIENCE,,"Improving student retention in particular science, technology, engineering and mathematics majors has focused on identifying strategies, and practices that will encourage students to complete a degree in STEM major. In this paper, we present findings from a study of retention and migration among STEM students, comparing rates across both engineering and science students. We look at all students admitted between 2009- 2014, both direct admits and transfer, at a large public university. Transfer students are often neglected in studies of retention and persistence especially in engineering. We found that engineering students are more persistent than science students with retention rates over 60% for engineering students compared to 40% in math. Persistence rates for firsttime students are less than transfer students in the engineering enrollments. Also, as in previous studies, most migration out of discipline occurs in the first two years of enrollment. We also found that among enrolled students, a large number of engineering students (almost 20%) have not declared a major some until later in their studies. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125729975&site=ehost-live
372,SeeMore: A kinetic parallel computer sculpture for educating broad audiences on parallel computation.,Aditya Johri,Journal of Parallel & Distributed Computing,7437315,,Jul-17,105,,183,17,122841763,10.1016/j.jpdc.2017.01.017,Academic Press Inc.,Article,PARALLEL algorithms; PARALLEL computers; RASPBERRY Pi; SERVOMECHANISMS; COMPUTATIONAL complexity,Computer science education; Kinetic art; Parallel and distributed computing,"We discuss the design, implementation, and evaluation of a 256-node Raspberry-Pi cluster with kinetic properties. Each compute node is attached to a servo mechanism such that movement results from local computation. The result is SeeMore, a kinetic parallel computer sculpture designed to enable visualization of parallel algorithms in an effort to educate broad audiences as to the beauty, complexity, and importance of parallel computation. The algorithms and interfaces were implemented by students from various related courses at VA Tech. We describe these designs in sufficient detail to enable others to build their own kinetic computing sculptures to augment their experiential learning programs. Our evaluations at exhibitions indicate 63% and 84% of visitors enjoyed interacting with SeeMore while 69% and 87% believed SeeMore has educational value. [ABSTRACT FROM AUTHOR] Copyright of Journal of Parallel & Distributed Computing is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122841763&site=ehost-live
373,SeeMore: An Interactive Kinetic Sculpture Designed to Teach Parallel Computational Thinking.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,14,116026163,,ASEE,Article,COMPUTER systems; KINETIC sculpture; ABSTRACT sculpture; PARALLEL programs (Computer programs); SCIENTISTS; EDUCATION; All Other Miscellaneous Schools and Instruction; Educational Support Services; Administration of Education Programs; Computer systems design and related services (except video game design and development); Computer Systems Design Services,,"Parallel computing is generally perceived to be difficult topic to understand and learn. This paper presents a design case study that was conceptualized and implemented to introduce non-computational savvy audience (e.g. students, K-12, senators, elderly, etc.) to the concepts of parallel computational thinking. In order to visualize and better comprehend the data transmission mechanism and algorithmic patterns of parallel computing, a kinetic computing sculpture comprising of a functional cluster of Raspberry Pi computers has been built by an interdisciplinary group of researchers. To evaluate users' learning experience, a focus group interview with high school students was conducted. Data from the study reveal that students found the sculpture an engaging and effective visual artifact that illustrated parallel computing patterns. After interacting with the sculpture, the participants were able to explain parallel computing to other participants and correctly answer all assessment related questions. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116026163&site=ehost-live
374,Student Autonomy: Implications of Design-Based Informal Learning Experiences in Engineering.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2014,,,1,12,115955926,,ASEE,Article,NONFORMAL education research; ENGINEERING student research; INSTRUCTIONAL innovations; EXPERIMENTAL methods in education; ENGINEERING design education in universities & colleges; Engineering Services,,"As part of their college-based undergraduate degree experience, a large portion of engineering students are involved in different informal learning experiences, such as co-curricular design teams, student organizations, and undergraduate research. The purpose of this qualitative study was to better understand engineering students' learning experiences in informal learning sites, particularly their sense of autonomy, which emerged as a major theme in initial data analysis. Specifically, this study investigates a hands-on design and manufacturing laboratory for engineering students in a large research and state institution, which is home to student engineering design teams, such as a Formula design team. We found that these experiences enhanced students' self-directed autonomy and allowed them to take control of their learning trajectory. We discuss implications for future research and educational practices. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=115955926&site=ehost-live
375,Student Experiences In An Interdisciplinary Studio-Based Design Course: The Role Of Peer Scaffolding.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2014,,,1,14,115955922,,ASEE,Article,TEACHING methods research; INDUSTRIAL design education; ARCHITECTURAL design education; HIGHER education; TECHNICAL education; ARCHITECTURAL studios; ENGINEERING student research,,The article discusses a study which examined the role of peer scaffoldng in the investigation of engineering students' perception of an interdisciplinary studio-based architecture and industrial design course. The operationalized peer scaffolding attributes used in the study are presented including shared understanding and fading support. Information is also presented on student studio orientation and students' studio design experience.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=115955922&site=ehost-live
376,The Cambridge Handbook of Engineering Education Research and Reflections on the Future of the Field.,Aditya Johri,Journal of Engineering Education,10694730,,Jul-14,103,3,363,6,96924099,10.1002/jee.20047,Wiley-Blackwell,Editorial,"ENGINEERING education; NONFICTION; CAMBRIDGE Handbook of Engineering Education Research (Book); JOHRI, Aditya; OLDS, Barbara M.",,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96924099&site=ehost-live
377,Uses and Gratifications of Pokémon Go: Why do People Play Mobile Location-Based Augmented Reality Games?,Aditya Johri,International Journal of Human-Computer Interaction,10447318,,2019,35,9,804,16,135932593,10.1080/10447318.2018.1497115,Taylor & Francis Ltd,Article,POKEMON Go; AUGMENTED reality; MOBILE games; MOBILE apps; HUMAN-computer interaction; Software Publishers,Augmented reality; freemium; gamification; location-based games; Uses and Gratifications,"In recent years, augmented reality games (ARGs) such as Pokémon Go have become increasingly popular. These games not only afford a novel gaming experience but also have the potential to alter how players view their physical realities. In addition to the common experiences and gratifications people derive from games, (location-based) ARGs can afford, for example outdoor adventures, communal activities, and health benefits, but also create problems stemming from, for example privacy concerns and poor usability. This raises some important research questions as to what drives people to use these new applications, and why they may be willing to spend money on the content sold within them. In this study, we investigate the various gratifications people derive from ARGs (Pokémon Go) and the relationship of these gratifications with the players' intentions to continue playing and spending money on them. We employ data drawn from players of Pokémon Go (N = 1190) gathered through an online survey. The results indicate that game enjoyment, outdoor activity, ease of use, challenge, and nostalgia are positively associated with intentions to reuse (ITR), meanwhile outdoor activity, challenge, competition, socializing, nostalgia and ITR are associated with in-app purchase intentions (IPI). In contrast with our expectations, privacy concerns or trendiness were not associated with reuse intentions or IPI. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Human-Computer Interaction is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135932593&site=ehost-live
378,Cryptography on a Speck of Dust.,Jens-Peter Kaps,Computer (00189162),189162,,Feb-07,40,2,38,7,24104773,10.1109/MC.2007.52,IEEE,Article,"RADIO frequency identification systems; WIRELESS communications security; COMPUTER programming; ALGORITHMS; COMPUTER security software; DATA encryption; DATA transmission system security measures; PERSONAL communication service systems; CRYPTOGRAPHY; COMPUTER software; SECURITY systems; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); Wireless Telecommunications Carriers (except Satellite); Custom Computer Programming Services; Computer systems design and related services (except video game design and development); Other Computer Related Services; Electronic components, navigational and communications equipment and supplies merchant wholesalers; Security Systems Services (except Locksmiths); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing",,"The article discusses the security problems with wireless sensor networks (WSNs) and radio frequency identification (RFID) devices whose applications range from supply-chain management to home automation and health care. Since these devices are extremely small and have limited power, the task of applying security requires cryptographic services that can perform on a minuscule sized area. The authors give an overview of current cryptographic algorithms and then apply those ideas on a extremely small level. The article continues by giving design recommendations for new algorithms. INSET: Ultralow-Power Application Domain.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24104773&site=ehost-live
379,Implementation of efficient SR-Latch PUF on FPGA and SoC devices.,Jens-Peter Kaps,Microprocessors & Microsystems,1419331,,Aug-17,53,,92,14,124935137,10.1016/j.micpro.2017.07.006,Elsevier B.V.,Article,FIELD programmable gate arrays; SYSTEMS on a chip; CRYPTOGRAPHY; OSCILLATIONS; RELIABILITY in engineering; Semiconductor and Related Device Manufacturing,Key generation; Metastability; PUF; SR-Latch; Xilinx FPGAs; Zynq SoC,"In this paper we present a reliable and efficient SR-Latch based PUF design, with two times improvement in area over the state of the art, thus making it very attractive for low-area designs. This PUF is able to reliably generate a cryptographic key. The PUF response is generated by quantifying the number of oscillations during the metastability state for preselected latches. The derived design has been verified on 25 Xilinx Spartan-6 FPGAs (XC6SLX16) and 10 Xilinx Zynq SoC (XC7Z010) devices. The design exhibited ∼49% uniqueness figures when tested on both types of FPGAs. The reliability figures were >94% for temperature variation (0–85 °C) and ±5% of core voltage variation. [ABSTRACT FROM AUTHOR] Copyright of Microprocessors & Microsystems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124935137&site=ehost-live
380,DEVELOPING KNOWLEDGE-BASED SYSTEMS: REORGANIZING THE SYSTEM DEVELOPMENT LIFE CYCLE.,Larry Kerschberg,Communications of the ACM,10782,,Apr-89,32,4,482,7,5247997,10.1145/63334.63340,Association for Computing Machinery,Article,"COMPUTER systems; TRANSACTION systems (Computer systems); DECISION support systems; EXPERT systems; DATA mining; ARTIFICIAL intelligence; Data Processing, Hosting, and Related Services; Computer systems design and related services (except video game design and development); Computer Systems Design Services",Software development,"This article describes a Knowledge-Based-System Developmental Life Cycle that shows what must be changed and retained from conventional Synchronous Data Link Controls. Building computer-based information systems involves some basic tasks such as, problem detection, identification, and definition, solution definition, system analysis, logical and physical system design, procedure and program design, procedure and program writing, program testing, integrated testing, conversion and installation, and operation. The organization of these tasks may change, but the tasks still must be performed. Transaction processing systems (TPS), decision support systems and knowledge-based systems offer different development challenges. TPSs perform routine data processing, are designed around forms, procedures, inputs, and outputs, and often address well-structured problems. System development life cycles (SDLCs) originated when most systems were TPSs. Many organizations execute SDLC phases sequentially, with a sign-off after each phase, an approach that is suitable for many TPSs.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=5247997&site=ehost-live
381,The Role of Context in Social Semantic Search and Decision Making.,Larry Kerschberg,International Journal on Artificial Intelligence Tools,2182130,,Dec-14,23,6,-1,4,100100344,10.1142/S0218213014600227,World Scientific Publishing Company,Article,SEMANTICS; SEARCH engines; DECISION making; COMPUTER users; ONTOLOGIES (Information retrieval); RECOMMENDER systems,Context; search,"This paper is based on my Keynote Address at the Tools with AI conference on November 4, 2013. The talk focussed on some factors used to determine a user's context - those attributes, both tacit and explicit, which help to ascertain a user's intensions for a search request in order to make a decision. I also explored how social semantic search can assist in guiding the decision process, especially when the decision is of a personal nature, for example, in decisions involving health care, where there may be a number of avenues to pursue. I focussed on our patented-technology exemplified by the Knowledge Sifter system, and an application called Personal Health Explorer (PHE), a semantic recommender system that allows an individual to perform semantic search and discovery related to conditions and diseases contained in his personal health record (PHR). The PHE system consults authoritative ontologies and reputable information sources to provide semantically enhanced authoritative recommendations that can be stored in the individual's (PHR) for further research and consultation. The PHE is implemented using the Knowledge Sifter agent-based framework and Microsoft's HealthVault. [ABSTRACT FROM AUTHOR] Copyright of International Journal on Artificial Intelligence Tools is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100100344&site=ehost-live
382,Programmers Are Users Too: Human-Centered Methods for Improving Programming Tools.,Thomas LaToza,Computer (00189162),189162,,Jul-16,49,7,44,9,116697858,10.1109/MC.2016.200,IEEE,Article,COMPUTER programming; HUMAN-computer interaction; COMPUTER programmers; SOFTWARE engineering; STREAMING video & television; Computer systems design and related services (except video game design and development); Other Computer Related Services; Custom Computer Programming Services; Internet Publishing and Broadcasting and Web Search Portals,A/B testing; contextual inquiry; data mining; end-user software engineering; evaluation studies; exploratory lab studies; HCI; Human computer interaction; human-centered computing; human-computer interaction; log analysis; Natural language processing; natural-programming elicitation; Programming; rapid prototyping; software development; Software engineering; software psychology; studies of program constructs; think-aloud usability evaluation; user interfaces,"Human-centered methods can help researchers better understand and meet programmers' needs. Because programming is a human activity, many of these methods can be used without change. However, some programmer needs require new methods, which can also be applied to domains other than software engineering. This article features five Web extras. The video at https://youtu.be/4PH9-qi-yTQ demonstrates Azurite, an Eclipse plug-in with a selective undo feature that lets programmers more easily backtrack their code. The video at https://youtu.be/gOSlR62-rd8 describes Graphite, an Eclipse plug-in offering active code completion, a simple but powerful technique that integrates useful code-generation tools directly into the editor. The video at https://youtu.be/zyrqcYxqDtI describes HANDS, a new programming system that emphasizes usability by building on children's and beginning programmers' natural problem-solving tendencies. The video extra at https://youtu.be/80EctbI7PFc describes Whyline, a debugging tool that lets developers ask questions about their program's output and behavior. The video at https://youtu.be/3L4MK2dG_6k demonstrates the prototype for Whyline, a debugging tool that lets developers pose questions about their program's output. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116697858&site=ehost-live
384,Delay of routine health care during the COVID-19 pandemic: A theoretical model of individuals' risk assessment and decision making.,Myeong Lee,Social Science & Medicine,2779536,,Aug-22,307,,N.PAG,1,158369630,10.1016/j.socscimed.2022.115164,Elsevier B.V.,Article,COMMUNICABLE diseases; MATHEMATICAL models; RESEARCH methodology; MEDICAL care; INTERVIEWING; UNCERTAINTY; RISK assessment; DECISION making; THEORY; COVID-19 pandemic; DISEASE management; UNITED States,COVID-19; Decision making; Delayed health care; Risk assessment,"Delaying routine health care has been prevalent during the COIVD-19 pandemic. Macro-level data from this period reveals that U.S. patients under-utilized routine health care services such as primary care visits, preventative tests, screenings, routine optometry care, dental appointments, and visits for chronic disease management. Yet, there is a gap in research on how and why patients understand risks associated with seeking or delaying routing health care during an infectious disease pandemic. Our research addresses this gap based on semi-structured interviews with 40 participants living in regions across the United States. By building upon Unger-Saldaña and Infante-Castañeda's model of delayed health care, we extend this model by articulating how health care delays happen during an infectious disease pandemic. Specifically, we show how perceptions of uncertainty and subjective risk assessments shape people's decisions to delay routine health care while they operate at two levels, internal and external to one's social bubble, interacting with each other. • During COVID-19, people delayed their routine health care due to various risks. • Interplays between internal and external factors lead to risk assessment. • One's perception and assessment of uncertainty and risk lead to health care delay. • The availability of alternative health care may moderate one's risk assessment. • We propose a new model of decision-making to delay health care. [ABSTRACT FROM AUTHOR] Copyright of Social Science & Medicine is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158369630&site=ehost-live
385,Effective Course-of-Action Determination to Achieve Desired Effects.,Alexander Levis,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,Nov-07,37,6,1140,11,27590174,10.1109/TSMCA.2007.904771,IEEE,Article,ALGORITHMS; BAYESIAN analysis; BAYESIAN field theory; HEURISTIC programming; ERGONOMICS; SYSTEMS engineering; HUMAN-machine systems; MATHEMATICAL optimization; FREE probability theory,Course of action (COA); Dynamic Bayesian networks (DBNs); effects-based operations; evolutionary algorithms (EAs); optimization; timed influence nets (TINs),"An evolutionary algorithm-based approach to identify effective courses of action (COAs) in dynamic uncertain situations is presented. The uncertain situation is modeled using timed influence nets, an instance of dynamic Bayesian networks. The approach makes significant enhancements to the current trial-and-error-based manual technique, which is not only labor intensive but also not capable of modeling constraints among actionable events. The proposed approach is an attempt to overcome these limitations. It automates the process of COA identification. It also allows a system analyst to capture certain types of constraints among actionable events. Because of its parallel search nature, the approach produces multiple COAs that have a similar fitness value. This feature not only gives more flexibility to a decision maker during mission planning, but it can also be used to generalize the COAs if there exists a pattern among them. This paper also discusses a heuristic that further enhances the performance of the approach. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=27590174&site=ehost-live
386,Evaluation of Service Oriented Architecture-based federated architectures.,Alexander Levis,Systems Engineering,10981241,,Spring2011,14,1,56,17,57480660,10.1002/sys.20162,Wiley-Blackwell,Article,SERVICE-oriented architecture (Computer science); SYSTEMS engineering; PERFORMANCE evaluation; INFORMATION technology; ARCHITECTURE & technology; ARCHITECTURAL models,enterprise service bus; federated information domains; service oriented architecture,"This paper discusses the specifications, methods, and constructs to implement end-to-end Service Oriented Architecture (SOA)-based systems engineering across a federation of information domains. It addresses the necessity and benefits of a repeatable service design framework and its ability to consistently yield quantifiable results for SOA performance evaluation. An illustrative example of the approach is presented. © 2010 Wiley Periodicals, Inc. Syst Eng 14: 56-72, 2011 [ABSTRACT FROM AUTHOR] Copyright of Systems Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=57480660&site=ehost-live
387,Identification of best sets of actions in Influence Nets.,Alexander Levis,International Journal of Hybrid Intelligent Systems,14485869,,Mar-08,5,1,19,11,31802259,10.3233/HIS-2008-5102,IOS Press,Article,NONLINEAR programming; MATHEMATICAL programming; INTEGER programming; ALGORITHMS; VECTOR analysis,Bayesian Networks; combinatorial optimization; Influence Nets; mixed integer nonlinear programming; probabilistic reasoning; sensitivity analysis,"This paper presents a heuristic approach to solve the problem of best set of actions determination in Influence Nets. Influence Nets are a special instance of Bayesian Networks that model uncertain situations by connecting a set of desired effects to a set of actionable events through chains of probabilistic cause and effect relationships. Once an Influence Net is specified, a system analyst is often interested in identifying the set of action which has the highest probability of achieving a desired effect. The existing techniques to solve this problem, such as sensitivity analysis and exhaustive search, have limitations of their own. The proposed algorithm, named SAF, attempts to overcome these limitations. The paper also shows that the problem of best set of actions determination can be formulated as an instance of Mixed Integer Non Linear Programming (MINLP). An empirical study is presented that compares the performance of sensitivity analysis, SAF, and MINLP. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Hybrid Intelligent Systems is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=31802259&site=ehost-live
388,Modeling time-varying uncertain situations using Dynamic Influence Nets,Alexander Levis,International Journal of Approximate Reasoning,0888613X,,Oct-08,49,2,488,15,34533073,10.1016/j.ijar.2008.04.007,Elsevier B.V.,Article,PROBABILITY theory; MATHEMATICAL combinations; MATHEMATICS; BAYESIAN analysis,Dynamic Bayesian Networks; Dynamic Influence Nets; Probabilistic reasoning; Timed Influence Nets,"Abstract: This paper enhances the Timed Influence Nets (TIN) based formalism to model uncertainty in dynamic situations. The enhancements enable a system modeler to specify persistence and time-varying influences in a dynamic situation that the existing TIN fails to capture. The new class of models is named Dynamic Influence Nets (DIN). Both TIN and DIN provide an alternative easy-to-read and compact representation to several time-based probabilistic reasoning paradigms including Dynamic Bayesian Networks. The Influence Net (IN) based approach has its origin in the Discrete Event Systems modeling. The time delays on arcs and nodes represent the communication and processing delays, respectively, while the changes in the probability of an event at different time instants capture the uncertainty associated with the occurrence of the event over a period of time. [Copyright &y& Elsevier] Copyright of International Journal of Approximate Reasoning is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34533073&site=ehost-live
389,Rescuing Prometheus: Four Monumental Projects That Changed the Modern World.,Alexander Levis,Defense Acquisition Research Journal: A Publication of the Defense Acquisition University,21568391,,Oct-16,23,4,417,3,118642453,,Defense Acquisition University,Book Review,"HISTORY of technological innovations; NONFICTION; RESCUING Prometheus: Four Monumental Projects That Changed the Modern World (Book); HUGHES, Thomas Parke, 1923-2014",,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=118642453&site=ehost-live
390,A comprehensive study of an online packet scheduling algorithm.,Fei Li,Theoretical Computer Science,3043975,,Jul-13,497,,31,8,89510870,10.1016/j.tcs.2012.06.002,Elsevier B.V.,Article,"DATA packeting; COMPUTER algorithms; COMPUTER scheduling; DISCRETE systems; BUFFER storage (Computer science); COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Buffer management; Competitive analysis; Online algorithm; Packet scheduling,"Abstract: We study the bounded-delay model for Qualify-of-Service buffer management. Time is discrete. There is a buffer. Unit-length jobs (also called packets) arrive at the buffer over time. Each packet has an integer release time, an integer deadline, and a positive real value. A packet’s characteristics are not known to an online algorithm until the packet actually arrives. In each time step, at most one packet can be sent out of the buffer. The objective is to maximize the total value of the packets sent by their respective deadlines in an online manner. An online algorithm’s performance is usually measured in terms of competitive ratio, when this online algorithm is compared with a clairvoyant algorithm achieving the maximum total value. In this paper, we study a simple and intuitive online algorithm. We analyze its performance in terms of competitive ratio for the general model and a few important variants. [Copyright &y& Elsevier] Copyright of Theoretical Computer Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89510870&site=ehost-live
391,A near-optimal memoryless online algorithm for FIFO buffering two packet classes.,Fei Li,Theoretical Computer Science,3043975,,Jul-13,497,,164,9,89510881,10.1016/j.tcs.2011.11.039,Elsevier B.V.,Article,"ONLINE algorithms; FIRST in, first out (Queuing theory); COMPUTER scheduling; DATA packeting; COMBINATORIAL optimization; COMPUTER networks; Computer Systems Design Services",Buffer management; Competitive analysis; Online algorithm; Packet scheduling,"Abstract: We consider scheduling packets with values in a capacity-bounded buffer in an online setting. In this model, there is a buffer with limited capacity . At any time, the buffer cannot accommodate more than packets. Packets arrive over time. Each packet has a non-negative value. Packets leave the buffer only because they are either sent or dropped. Those packets that have left the buffer will not be reconsidered for delivery any more. In each time step, at most one packet in the buffer can be sent. The order in which the packets are sent should comply with the order of their arrival time. The objective is to maximize the total value of the packets sent in an online manner. In this paper, we study a variant of this FIFO buffering model in which a packet’s value is either 1 or . We present a deterministic memoryless 1.304-competitive algorithm. This algorithm has the same competitive ratio as the one presented in Lotker and Patt-Shamir [Z. Lotker, B. Patt-Shamir, Nearly optimal FIFO buffer management for DiffServ, in: Proceedings of the 21st Annual ACM Symposium on Principles of Distributed Computing, PODC, 2002, pp. 134–142; Z. Lotker, B. Patt-Shamir, Nearly optimal FIFO buffer management for DiffServ, Computer Networks 17 (1) (2003) 77–89]. However, our algorithm is simpler and does not employ any marking bits. The idea used in our algorithm is novel and different from all previous approaches that have been applied for the general model and its variants. We do not proactively preempt one packet when a new packet arrives. Instead, we may preempt more than one 1-value packet at the time when the buffer contains sufficiently many -value packets. [Copyright &y& Elsevier] Copyright of Theoretical Computer Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89510881&site=ehost-live
392,Online packet scheduling with bounded delay and lookahead.,Fei Li,Theoretical Computer Science,3043975,,Jul-19,776,,95,19,136444306,10.1016/j.tcs.2019.01.013,Elsevier B.V.,Article,ONLINE algorithms; INTEGERS,Buffer management; Lookahead; Online algorithm; Online scheduling,"We study the online bounded-delay packet scheduling problem (PacketScheduling) , where packets of unit size arrive at a router over time and need to be transmitted over a network link. Each packet has two attributes: a non-negative weight and a deadline for its transmission. The objective is to maximize the total weight of the transmitted packets. This problem has been well studied in the literature; yet currently the best published upper bound is 1.828 [8] , still quite far from the best lower bound of ϕ ≈ 1.618 [11,2,6]. In the variant of PacketScheduling with s-bounded instances , each packet can be scheduled in at most s consecutive slots, starting at its release time. The lower bound of ϕ applies even to the special case of 2-bounded instances, and a ϕ -competitive algorithm for 3-bounded instances was given in [5]. Improving that result, and addressing a question posed by Goldwasser [9] , we present a ϕ -competitive algorithm for 4 -bounded instances. We also study a variant of PacketScheduling where an online algorithm has the additional power of 1-lookahead , knowing at time t which packets will arrive at time t + 1. For PacketScheduling with 1-lookahead restricted to 2-bounded instances, we present an online algorithm with competitive ratio 1 2 (13 − 1) ≈ 1.303 and we prove a nearly tight lower bound of 1 4 (1 + 17) ≈ 1.281. In fact, our lower bound result is more general: using only 2-bounded instances, for any integer ℓ ≥ 0 we prove a lower bound of 1 2 (ℓ + 1) (1 + 5 + 8 ℓ + 4 ℓ 2 ) for online algorithms with ℓ -lookahead, i.e., algorithms that at time t can see all packets arriving by time t + ℓ. Finally, for non-restricted instances we show a lower bound of 1.25 for randomized algorithms with ℓ -lookahead, for any ℓ ≥ 0. [ABSTRACT FROM AUTHOR] Copyright of Theoretical Computer Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136444306&site=ehost-live
393,Anisotropic thermoelectric behavior in armchair and zigzag mono- and fewlayer MoS2 in thermoelectric generator applications.,Qiliang Li,Scientific Reports,20452322,,9/4/15,,,13706,1,109303311,10.1038/srep13706,Springer Nature,Article,ARMCHAIRS; THERMOELECTRIC generators; DENSITY functional theory; GREEN'S functions; NONEQUILIBRIUM flow; PHONON spectra,,"In this work, we have studied thermoelectric properties of monolayer and fewlayer MoS2 in both armchair and zigzag orientations. Density functional theory (DFT) using non-equilibrium Green's function (NEGF) method has been implemented to calculate the transmission spectra of mono- and fewlayer MoS2 in armchair and zigzag directions. Phonon transmission spectra are calculated based on parameterization of Stillinger-Weber potential. Thermoelectric figure of merit, ZT, is calculated using these electronic and phonon transmission spectra. In general, a thermoelectric generator is composed of thermocouples made of both n-type and p-type legs. Based on our calculations, monolayer MoS2 in armchair orientation is found to have the highest ZT value for both p-type and n-type legs compared to all other armchair and zigzag structures. We have proposed a thermoelectric generator based on monolayer MoS2 in armchair orientation. Moreover, we have studied the effect of various dopant species on thermoelectric current of our proposed generator. Further, we have compared output current of our proposed generator with those of Silicon thin films. Results indicate that thermoelectric current of MoS2 armchair monolayer is several orders of magnitude higher than that of Silicon thin films. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109303311&site=ehost-live
394,Autonomous Visual Perception for Unmanned Surface Vehicle Navigation in an Unknown Environment.,Qiliang Li,Sensors (14248220),14248220,,May-19,19,10,2216,1,136675160,10.3390/s19102216,MDPI,Article,AUTONOMOUS vehicles; COMPUTER vision; PATTERN recognition systems; IMAGE processing; ROBUST control; One-Hour Photofinishing; Photofinishing Laboratories (except One-Hour),deep-learning; recognition; unmanned surface vehicles; vision; water region,"Robust detection and recognition of water surfaces are critical for autonomous navigation of unmanned surface vehicles (USVs), since any none-water region is likely an obstacle posing a potential danger to the sailing vehicle. A novel water region visual detection method is proposed in this paper. First, the input image pixels are clustered into different regions and each pixel is assigned a label tag and a confidence value by adaptive multistage segmentation algorithm. Then the resulting label map and associated confidence map are fed into a convolutional neural network (CNN) as training samples to train the network online. Finally, the online trained CNN is used to segment the input image again but with greater precision and stronger robustness. Compared with other deep-learning image segmentation algorithms, the proposed method has two advantages. Firstly, it dispenses with the need of manual labeling training samples which is a costly and painful task. Secondly, it allows real-time online training for CNN, making the network adaptive to the navigational environment. Another contribution of this work relates to the training process of neuro network. An effective network training method is designed to learn from the imperfect training data. We present the experiments in the lake with a various scene and demonstrate that our proposed method could be applied to recognize the water region in the unknown navigation environment automatically. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136675160&site=ehost-live
395,Enhanced energy storage performance and thermal stability in relaxor ferroelectric (1‐x)BiFeO3‐x(0.85BaTiO3‐0.15Bi(Sn0.5Zn0.5)O3) ceramics.,Qiliang Li,Journal of the American Ceramic Society,27820,,Jun-21,104,6,2646,9,149598200,10.1111/jace.17705,Wiley-Blackwell,Article,HEAT storage; RELAXOR ferroelectrics; THERMAL stability; ENERGY storage; ELECTRIC discharges; ELECTRIC breakdown,BiFeO3; energy‐storage density; relaxor ferroelectric; solid phase sintering,"Lead‐free (1‐x)BiFeO3‐x(0.85BaTiO3‐0.15Bi(Sn0.5Zn0.5)O3) [(1‐x)BF‐x(BT‐BSZ), x=0.45‐0.7] ceramic samples were prepared by solid phase sintering. It is revealed that the pure single‐phase perovskite structure can be obtained in samples with x ≥ 0.6. With increasing x, the measured ferroelectric hysteresis loop becomes gradually slimmed in accompanying with reduced remnant polarization, and a clear ferroelectric‐relaxor transition at x = 0.65 is identified. Furthermore, the measured electric breakdown strength can be significantly enhanced with increasing x, and the optimal energy storage performance is achieved at x = 0.65, characterized by the recoverable energy storage density up to ≈3.06 J/cm3 and energy storage efficiency as high as ≈92 %. Excellent temperature stability (25°C–110°C) and fatigue endurance (>105 cycles) for energy storage are demonstrated. Our results suggest that the BF‐based relaxor ceramics can be tailored for promising applications in high energy storage devices. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Ceramic Society is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149598200&site=ehost-live
396,High energy storage performances of Bi1−xSmxFe0.95Sc0.05O3 lead-free ceramics synthesized by rapid hot press sintering.,Qiliang Li,Journal of the European Ceramic Society,9552219,,Jul-19,39,7,2331,8,135228187,10.1016/j.jeurceramsoc.2019.02.009,Elsevier B.V.,Article,LEAD-free ceramics; ENERGY storage; BISMUTH compounds; HOT pressing; SINTERING; CHEMICAL synthesis; Iron Ore Mining,BiFeO3; Energy-storage density; Hot press sintering; Lead-free ceramics,"Abstract Lead-free Bi 1−x Sm x Fe 0.95 Sc 0.05 O 3 (x = 0.15–0.19) ceramics were fabricated by rapid hot press sintering, and their structure, ferroelectric and energy storage properties were comprehensively investigated. All the samples are in the mixed phases with R3c rhombohedral and Pbnm orthorhombic structures. With increasing x , the ferroelectric polarization decreases gradually, while the polarization loop becomes gradually slimed too. An high recoverable energy density (˜2.21 J/cm3) and a large efficiency (˜76%) with good thermal stability (20 °C–120 °C) are obtained under electric field (230 kV/cm) for the optimized sample x = 0.17. Moreover, transmission electron microscopy and piezo-response force microscopy measurements reveal that the presence of two-phase coexistence favors the formation of polar nano-regions, leading to the linear-towards polarization behaviors and the enhanced dielectric breakdown field, which is responsible for the superior energy storage performance of Bi 1−x Sm x Fe 0.95 Sc 0.05 O 3 ceramics. These results indicate a significant step to tailor lead-free BiFeO 3 -based ceramics towards high dielectric energy storage applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of the European Ceramic Society is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135228187&site=ehost-live
397,High-Performance Nonequilibrium InSb PIN Infrared Photodetectors.,Qiliang Li,IEEE Transactions on Electron Devices,189383,,Mar-19,66,3,1361,7,136509782,10.1109/TED.2019.2895032,IEEE,Article,PIN photodiodes; PHOTODETECTORS; CURRENT density (Electromagnetism); DARK currents (Electric); COOLING,Auger suppression; Cooling; Current density; Dark current; Detectors; Doping; infrared (IR) detectors; InSb; Photodetectors; Pins,"The cooling requirement is still a great burden of sensitive infrared (IR) detectors. To overcome this challenge, a detailed investigation of InSb P-intrinsic-N diodes was conducted. Its dark current was comprehensively studied for the design and optimization of high-performance nonequilibrium IR detectors. The doping concentration, exclusion junction, and layer geometry were engineered to lower the Auger suppression trigger current density ${J}_{\text {onset}}$ , leading to a low dark saturation current ${J}_{\text {sat}}$. The result indicated that the Auger suppression can be triggered at ${J}_{\text {onset}}={15}$ A/cm2 and ${V}_{\text {onset}} = {0.8}$ V, while ${J}_{\text {sat}}$ can be kept as low as 10 A/cm2. The nature of Auger suppression has been studied by calculating the carrier concentration profile in the diode with different bias voltages. It is proven that the exclusion diode has the most significant effect on the Auger suppression. The specific detectivity of optimized nonequilibrium InSb detectors at different temperatures has been compared favorably with the results reported by other groups. Such a nonequilibrium InSb IR detector has a high potential in room temperature and above application. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Electron Devices is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136509782&site=ehost-live
398,High-performance room-temperature TiO2-functionalized GaN nanowire gas sensors.,Qiliang Li,Applied Physics Letters,36951,,9/16/19,115,12,N.PAG,5,138756850,10.1063/1.5116677,American Institute of Physics,Article,GALLIUM nitride; N-type semiconductors; DETECTORS; NANOFABRICATION; GASES; TRANSDUCERS; NANOWIRE devices; NANOWIRES; Semiconductor and other electronic component manufacturing; Other Electronic Component Manufacturing,,"Hybrid gas sensors based on TiO2 functionalized gallium nitride nanowires have been prepared by nanofabrication and comprehensively studied for high-responsivity applications. The devices exhibited a high responsivity (25%) to 500 ppm NO2 assisted with ultraviolet illumination at room temperature. The thickness and doping concentration of TiO2 were engineered to improve the transducer function. The result indicated that an excellent n-type response can be stably obtained for a doping range from 1 × 1017 cm−3 to 1 × 1019 cm−3. The TiO2 thickness and doping concentration can be further fine-tuned to achieve optimal performance. In addition, a comprehensive device simulation was carried out to understand the device operation and gain insight for optimizing the device performance. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138756850&site=ehost-live
400,Methods to Characterize the Electrical and Mechanical Properties of Si Nanowires.,Qiliang Li,AIP Conference Proceedings,0094243X,,9/26/07,931,1,457,5,26887919,10.1063/1.2799417,American Institute of Physics,Article,"MECHANICAL behavior of materials; NANOTUBES; NANOWIRES; METROLOGY; ELECTROMECHANICAL devices; SEMICONDUCTORS; Semiconductor and other electronic component manufacturing; Totalizing Fluid Meter and Counting Device Manufacturing; Automatic Environmental Control Manufacturing for Residential, Commercial, and Appliance Use; Other Electronic Component Manufacturing; Motor and Generator Manufacturing; Other Communications Equipment Manufacturing; Semiconductor and Related Device Manufacturing",Nanoelectromechanical System; Nanowire; Non-volatile memory; Transfer Length Method,"We report metrology methods to characterize nanowires. In this work, representative devices and test structures, including nanoelectromechanical switches, non-volatile nanowire memory devices with SONOS structure, and both transfer-length-method and Kelvin test structures, have been developed to investigate the electrical and mechanical properties of the silicon nanowires. These methods and test structures can be readily applied to other (non-Si) semiconductor nanowires/nanotubes. [ABSTRACT FROM AUTHOR] Copyright of AIP Conference Proceedings is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26887919&site=ehost-live
401,New families of large band gap 2D topological insulators in ethynyl-derivative functionalized compounds.,Qiliang Li,Applied Surface Science,1694332,,Aug-19,484,,1208,6,139234583,10.1016/j.apsusc.2019.04.071,Elsevier B.V.,Article,TOPOLOGICAL insulators; HIGH temperatures; TOPOLOGICAL property; BISMUTH telluride; LOW temperatures; FAMILIES,Edge states; Functionalized; Spin orbital coupling (SOC); Topological insulators; Two-dimensional materials,"The search for large band gap systems with dissipationless edge states is essential to developing materials that function under a wide range of temperatures. Two-dimensional (2D) topological insulators (TIs) have recently attracted significant attention due to their dissipationless transport, robust properties and excellent compatibility with device integration. However, a major barrier of 2D TIs is their small bulk band gap, which allows for applications only in extremely low temperatures. In this work, first principle calculations were used to analyze the geometric, electronic, and topological properties of PbC 2 X and BiC 2 X (X = H, Cl, F, Br, I) compounds. The band gap values are remarkably large, ranging from 0.79 eV to 0.99 eV. The nanoribbons of these compounds exhibited nontrivial topological order in the simulation, thus proving ethynyl-derivative functionalized Pb and Bi films to be new classes of giant band gap 2D TIs. In addition, these findings indicate that chemical functionalization with ethynyl-derivatives is an effective method to tune the band gap and preserve the nontrivial topological order. These novel materials that are applicable at both room temperature and high temperatures open the door to a new generation of electronics. (a) Calculated zigzag nanoribbon band structure of a 2D BiC 2 I monolayer. (b) The zigzag nanoribbon geometric structure that was used to calculate the edge states and current vs. bias characteristics. Unlabelled Image [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139234583&site=ehost-live
402,Nonvolatile memory based on redox-active ruthenium molecular monolayers.,Qiliang Li,Applied Physics Letters,36951,,10/14/19,115,16,N.PAG,5,139218620,10.1063/1.5108675,American Institute of Physics,Article,NONVOLATILE memory; RUTHENIUM; COMPUTER storage devices; X-ray photoelectron spectroscopy; SILICON oxide; RANDOM access memory; MONOMOLECULAR films; Computer and peripheral equipment manufacturing; Computer Storage Device Manufacturing,,"A monolayer of diruthenium molecules was self-assembled onto the silicon oxide surface in a semiconductor capacitor structure with a ""click"" reaction for nonvolatile memory applications. The attachment of the active molecular monolayer was verified by x-ray photoelectron spectroscopy. The prototypical capacitor memory devices in this work employed a metal/oxide/molecule/oxide/Si structure. With the intrinsic redox-active charge-storage properties of diruthenium molecules, these capacitor memory devices exhibited fast Program and Erase speed, excellent endurance performance with negligible degradation of the memory window after 105 program/erase cycles, and very good 10-year memory retention. These experimental results indicate that the redox-active ruthenium molecular memory is very promising for use in nonvolatile memory applications. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139218620&site=ehost-live
403,Observation and control of the anomalous Aharonov-Bohm oscillation in enhanced-mode topological insulator nanowire field-effect transistors.,Qiliang Li,Applied Physics Letters,36951,,8/12/19,115,7,N.PAG,5,138145238,10.1063/1.5111180,American Institute of Physics,Article,TOPOLOGICAL insulators; FIELD-effect transistors; OSCILLATIONS; QUANTUM interference; SILICON nanowires; SEMICONDUCTOR nanowires; WAVE functions,,"Aharonov-Bohm (AB) oscillation is a quantum mechanical phenomenon which reveals the coupling of electromagnetic potentials with the electron wave function, affecting the phase of the wave function. Such a quantum interference effect can be demonstrated through the magnetotransport measurement focusing on low-dimensional electronic states. Here, we report the experimental observation of anomalous AB oscillation in an enhanced-mode topological insulator Bi2Se3 nanowire field-effect transistor (FET) under strong surface disorder, which is different from the reported AB oscillation in topological insulator nanostructures. The surrounding gate of the nanowire FET gives rise to tunability of the chemical potential and introduces strong disorder on the surface states, leading to primary oscillation with an anomalous h/e period. Furthermore, the oscillation exhibits a significant dependence on the gate voltage which has been preliminary explained with the quantization of the surface conduction channel. The experimental demonstration can be very attractive for further exploration of quantum phase interference through electrical approaches, enabling applications in future information and electromagnetic sensing technology. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138145238&site=ehost-live
404,Precise gas discrimination with cross-reactive graphene and metal oxide sensor arrays.,Qiliang Li,Applied Physics Letters,36951,,11/26/18,113,22,N.PAG,4,133317637,10.1063/1.5063375,American Institute of Physics,Article,"METAL oxide semiconductors; GAS detectors; GRAPHENE; DETECTORS; INTERNET of things; Professional machinery, equipment and supplies merchant wholesalers",,"Discriminating similar molecules remains a very challenging problem for semiconductor gas sensors. Here, we report a method to achieve precise gas discrimination of similar chemical vapors (mesitylene, o-xylene, and toluene) by using cross-reactive arrays consisting of metal oxide semiconductor and graphene sensors. It is difficult to identify these three chemicals as they have very similar responses to these sensors. Through cross-reactive Principal Component Analysis of the sensor response features, however, the discrimination accuracy improved from about 70% with a single gas sensor to almost 100% with the cross-reactive sensor array. Such a precise discrimination and the low-cost planar process make this approach a very attractive candidate for smart gas sensing and for future Internet of Things applications. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133317637&site=ehost-live
405,"Recent Advances in Electrochemical Sensors for Detecting Toxic Gases: NO2, SO2 and H2S.",Qiliang Li,Sensors (14248220),14248220,,Feb-19,19,4,905,1,135038022,10.3390/s19040905,MDPI,Article,NITRIC oxide; GAS mixtures; ELECTROCHEMICAL sensors; GRAPHENE; SENSITIVITY analysis,density-functional theory (DFT); gas sensor; hydrogen sulfide (H2S); Internet of Things (IoT); nitrogen dioxide (NO2); response/recovery time; sensitivity; sulphur dioxide (SO2),"Toxic gases, such as NOx, SOx, H2S and other S-containing gases, cause numerous harmful effects on human health even at very low gas concentrations. Reliable detection of various gases in low concentration is mandatory in the fields such as industrial plants, environmental monitoring, air quality assurance, automotive technologies and so on. In this paper, the recent advances in electrochemical sensors for toxic gas detections were reviewed and summarized with a focus on NO2, SO2 and H2S gas sensors. The recent progress of the detection of each of these toxic gases was categorized by the highly explored sensing materials over the past few decades. The important sensing performance parameters like sensitivity/response, response and recovery times at certain gas concentration and operating temperature for different sensor materials and structures have been summarized and tabulated to provide a thorough performance comparison. A novel metric, sensitivity per ppm/response time ratio has been calculated for each sensor in order to compare the overall sensing performance on the same reference. It is found that hybrid materials-based sensors exhibit the highest average ratio for NO2 gas sensing, whereas GaN and metal-oxide based sensors possess the highest ratio for SO2 and H2S gas sensing, respectively. Recently, significant research efforts have been made exploring new sensor materials, such as graphene and its derivatives, transition metal dichalcogenides (TMDs), GaN, metal-metal oxide nanostructures, solid electrolytes and organic materials to detect the above-mentioned toxic gases. In addition, the contemporary progress in SO2 gas sensors based on zeolite and paper and H2S gas sensors based on colorimetric and metal-organic framework (MOF) structures have also been reviewed. Finally, this work reviewed the recent first principle studies on the interaction between gas molecules and novel promising materials like arsenene, borophene, blue phosphorene, GeSe monolayer and germanene. The goal is to understand the surface interaction mechanism. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135038022&site=ehost-live
406,Recent Progress in Smart Electronic Nose Technologies Enabled with Machine Learning Methods.,Qiliang Li,Sensors (14248220),14248220,,Nov-21,21,22,7620,1,153873885,10.3390/s21227620,MDPI,Article,"MACHINE learning; GAS detectors; ELECTRONIC noses; FEATURE extraction; DIAGNOSIS; ROBOTICS; Professional machinery, equipment and supplies merchant wholesalers",electronic nose; gas sensor array; machine learning; neural networks; review,"Machine learning methods enable the electronic nose (E-Nose) for precise odor identification with both qualitative and quantitative analysis. Advanced machine learning methods are crucial for the E-Nose to gain high performance and strengthen its capability in many applications, including robotics, food engineering, environment monitoring, and medical diagnosis. Recently, many machine learning techniques have been studied, developed, and integrated into feature extraction, modeling, and gas sensor drift compensation. The purpose of feature extraction is to keep robust pattern information in raw signals while removing redundancy and noise. With the extracted feature, a proper modeling method can effectively use the information for prediction. In addition, drift compensation is adopted to relieve the model accuracy degradation due to the gas sensor drifting. These recent advances have significantly promoted the prediction accuracy and stability of the E-Nose. This review is engaged to provide a summary of recent progress in advanced machine learning methods in E-Nose technologies and give an insight into new research directions in feature extraction, modeling, and sensor drift compensation. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153873885&site=ehost-live
407,Self-aligned multi-channel silicon nanowire field-effect transistors,Qiliang Li,Solid-State Electronics,381101,,Dec-12,78,,92,5,80032482,10.1016/j.sse.2012.05.058,Elsevier B.V.,Article,SILICON nanowires; FIELD-effect transistors; GATE array circuits; PHOTOLITHOGRAPHY; PERFORMANCE evaluation; ELECTRON mobility; SCHOTTKY barrier,Multi-channel nanowire FET; Nanowire field-effect transistor; Self-alignment; Voltage tolerance,"Abstract: Si nanowire field effect transistors (SiNW FETs) with multiple nanowire channels and different gate lengths have been fabricated by using a directed assembly approach combined with a standard photolithographic process. The electrical characteristics of SiNW FETs containing different numbers of nanowire channels were measured and compared. The multi-channel SiNW FETs show excellent performance: small subthreshold slope (≈75mV/dec), large ON/OFF ratio (≈108), good break-down voltage (>30V) and good carrier mobility (μ p ≈100cm2 V−1s−1). These excellent device properties were achieved by using a clean self-alignment process and an improved device structure with Schottky barriers at the source and drain contacts. Such high-performance multi-nanowire FETs are attractive for logic, memory, and sensor applications. [Copyright &y& Elsevier] Copyright of Solid-State Electronics is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=80032482&site=ehost-live
408,Self-assembled nanowire array capacitors: capacitance and interface state profile.,Qiliang Li,Nanotechnology,9574484,,4/4/14,25,13,135201,6,94772196,10.1088/0957-4484/25/13/135201,IOP Publishing,Article,"CAPACITORS; ELECTRIC capacity; SEMICONDUCTORS; ELECTRIC conductivity; SILICON nanowires; TRANSISTORS; Other Electronic Parts and Equipment Merchant Wholesalers; Capacitor, Resistor, Coil, Transformer, and Other Inductor Manufacturing; All Other Miscellaneous Electrical Equipment and Component Manufacturing; Electronic components, navigational and communications equipment and supplies merchant wholesalers; Semiconductor and Related Device Manufacturing; Semiconductor and other electronic component manufacturing",,"Direct characterization of the capacitance and interface states is very important for understanding the electronic properties of a nanowire transistor. However, the capacitance of a single nanowire is too small to precisely measure. In this work we have fabricated metal–oxide–semiconductor capacitors based on a large array of self-assembled Si nanowires. The capacitance and conductance of the nanowire array capacitors are directly measured and the interface state profile is determined by using the conductance method. We demonstrate that the nanowire array capacitor is an effective platform for studying the electronic properties of nanoscale interfaces. This approach provides a useful and efficient metrology for the study of the physics and device properties of nanoscale metal–oxide–semiconductor structures. [ABSTRACT FROM AUTHOR] Copyright of Nanotechnology is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94772196&site=ehost-live
409,Silicon nanowire NVM with high-k gate dielectric stack,Qiliang Li,Microelectronic Engineering,1679317,,Jul-09,86,9-Jul,1957,4,40631529,10.1016/j.mee.2009.03.095,Elsevier B.V.,Article,FLASH memory; COMPUTER storage devices; DIELECTRIC devices; MICROFABRICATION; WIRE; NANOSILICON; ELECTRIC properties of metals; SEMICONDUCTOR junctions; Computer Storage Device Manufacturing; Computer and peripheral equipment manufacturing; Metal Service Centers and Other Metal Merchant Wholesalers,Flash memory; NVM; Silicon nanowire; SiNW,"Abstract: Three flash memory cell structures with silicon nanowire channels and high-k dielectric stacks were fabricated with a “self-aligning” process and their characteristics are reported and compared in this paper: a Metal/SiO2/HfO2/SiO2/Si (MOHOS) cell with a SiO2 blocking layer and two Metal/Al2O3/HfO2/SiO2/Si (MAHOS) cells with Al2O3, all with HfO2 as the charge trapping layer. Compared to (control) planar cells, all three operate at higher speeds, attributed to the enhanced electric field across the tunneling oxide surrounding the channel. The MAHOS cells (Al2O3 blocking layer) outperform the MOHOS cells (SiO2 blocking layer) and both have large memory window, fast operation speed, good endurance and retention. [Copyright &y& Elsevier] Copyright of Microelectronic Engineering is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=40631529&site=ehost-live
410,Simultaneously enhanced energy storage density and efficiency in novel BiFeO3-based lead-free ceramic capacitors.,Qiliang Li,Journal of the European Ceramic Society,9552219,,Jan-21,41,1,387,7,146614794,10.1016/j.jeurceramsoc.2020.08.032,Elsevier B.V.,Article,LEAD-free ceramics; CERAMIC capacitors; ENERGY storage; ENERGY density; SPECIFIC gravity; ELECTRIC breakdown,BiFeO3; Energy-storage density; Relaxor ferroelectric; Solid phase sintering,"In this work, a series of novel lead-free (1- x)Bi 0.83 Sm 0.17 Fe 0.95 Sc 0.05 O 3 - x (0.85BaTiO 3 -0.15Bi(Mg 0.5 Zr 0.5)O 3) [(1- x)BSFS- x (BT-BMZ), x = 0.45−0.85] relaxor ceramics were prepared by solid phase sintering, and their dielectric properties and energy storage performances were explored. It was revealed that all the samples have a dense structure with pure pseudo-cubic phase. With the increase of x , the ferroelectric hysteresis loop is gradually slimmed accompanied by a decreasing polarization, indicating an enhanced relaxor behavior. Moreover, the electric breakdown strength increases linearly with x due to the fine grain size and enhanced relative density. Interestingly, a large recoverable energy density (∼3.2 J/cm3) with an outstanding efficiency (∼92 %) is achieved under an electric field ∼206 kV/cm for the optimized component x = 0.75, which is superior to other reported lead-free ceramic systems. Moreover, the optimized ceramics of 0.25BSFS-0.75(BT-BMZ) show good thermal stability (25−100 °C) and excellent fatigue endurance (cycle number: > 105) in energy storage performances. This work opens up a new route to tailor lead-free dielectric ceramics with high energy storage properties. [ABSTRACT FROM AUTHOR] Copyright of Journal of the European Ceramic Society is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146614794&site=ehost-live
411,SOI FED-SRAM Cell: Structure and Operation.,Qiliang Li,IEEE Transactions on Electron Devices,189383,,Sep-15,62,9,2865,6,109065842,10.1109/TED.2015.2450693,IEEE,Article,FIELD-effect devices; SEMICONDUCTOR diodes; THYRISTORS; BISTABLE devices; COMPUTER storage devices; Computer Storage Device Manufacturing; Computer and peripheral equipment manufacturing; Semiconductor and other electronic component manufacturing; Semiconductor and Related Device Manufacturing,Anodes; Charge carrier density; Field-effect diode (FED); Logic gates; MOSFET; Random access memory; SOI; SRAM; thin-capacitively coupled thyristor (TCCT); thyristor; Thyristors; Timing,"A static memory cell (SRAM) based on the field-effect diode (FED) is presented, and its operation is explained with the help of numerical device simulations. Although this new cell resembles the thin-capacitively coupled-thyristor (TCCT) SRAM cell in concept and operation, it is nevertheless characterized by significant advantages. These advantages derive from the fact that the thyristorlike mode of operation of the FED is gate induced, whereas the TCCT is an actual built-in thyristor. The operation of the cell is explained with the help of suitable timing diagrams, and the mechanisms of storing 1 and 0 are analyzed with detailed numerical simulations. In one operation scheme (where the cell could better be termed quasi-SRAM), a sequence of restore pulses is periodically applied after the cell is put on Hold, which ensures that the stored data remain valid for as long as the cell is powered ON. High read 0/1 current margin, fast write/read time, and densely packed cells are among the cell advantages obtained. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Electron Devices is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109065842&site=ehost-live
412,Study of interfacial strain at the α-Al2O3/monolayer MoS2 interface by first principle calculations.,Qiliang Li,Applied Surface Science,1694332,,Jan-18,428,,593,5,126185471,10.1016/j.apsusc.2017.09.203,Elsevier B.V.,Article,CHALCOGENIDES; METAL oxide semiconductor field-effect transistors; DIELECTRICS; CHARGE transfer; NANOELECTRONICS,2D semiconductors; Dielectric engineering; Strain effect; Temperature effect,"With the advances in two-dimensional (2D) transition metal dichalcogenides (TMDCs) based metal–oxide–semiconductor field-effect transistor (MOSFET), the interface between the semiconductor channel and gate dielectrics has received considerable attention due to its significant impacts on the morphology and charge transport of the devices. In this study, first principle calculations were utilized to investigate the strain effect induced by the interface between crystalline α-Al 2 O 3 (0001)/h-MoS 2 monolayer. The results indicate that the 1.3 nm Al 2 O 3 can induce a 0.3% tensile strain on the MoS 2 monolayer. The strain monotonically increases with thicker dielectric layers, inducing more significant impact on the properties of MoS 2 . In addition, the study on temperature effect indicates that the increasing temperature induces monotonic lattice expansion. This study clearly indicates that the dielectric engineering can effectively tune the properties of 2D TMDCs, which is very attractive for nanoelectronics. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126185471&site=ehost-live
413,Detection of Deep-Levels in Doped Silicon Nanowires Using Low-Frequency Noise Spectroscopy.,Qiliang Li,IEEE Transactions on Electron Devices,189383,,Dec-13,60,12,4206,7,92520639,10.1109/TED.2013.2285154,IEEE,Article,ELECTRIC properties of silicon nanowires; SPECTRUM analysis; LORENTZIAN function; IONIZATION energy; ELECTRON capture; ELECTRIC current measurement,Current measurement; Deep-levels; field-effect transistor (FET); generation-recombination (G-R) noise; Gold; low-frequency noise (LFN); Nickel; Noise; Silicon; silicon nanowire (SiNW); Spectroscopy; Temperature measurement,"We report detailed characterization of electrically-active deep-levels in doped Si nanowires (SiNWs) grown using catalyst-assisted vapor–liquid-solid (VLS) technique. Temperature-dependent low-frequency noise (LFN) spectroscopy was used to reveal the presence of generation-recombination related Lorentzian-type peaks along with 1/f-type noise in these NWs. In Ni-catalyzed SiNWs, the correlated LFN spectroscopy detected electrically active deep-levels with ionization energies of 0.42 eV for the n-type and 0.22 eV for the p-type SiNWs, respectively. In Au-catalyzed n- and p-type SiNWs, the energies of the deep-levels were estimated to be 0.44 and 0.38 eV, respectively. These values are in good agreement with the known ionization energies of deep-levels introduced by Ni and Au in Si. Associated trap concentrations and hole and electron capture cross sections were also estimated. This paper clearly indicated the presence of electrically active deep-levels associated with unintentional incorporation of catalyst atoms in the VLS-grown SiNWs. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Electron Devices is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92520639&site=ehost-live
414,Domain structure and multiferroic properties of epitaxial hexagonal ErMnO3 films.,Qiliang Li,Journal of Alloys & Compounds,9258388,,Apr-20,821,,N.PAG,1,141632063,10.1016/j.jallcom.2019.153529,Elsevier B.V.,Article,EPITAXY; PULSED laser deposition; MAGNETIC measurements; THIN films; RAMAN spectroscopy,Ferroelectric domains; Hexagonal ErMnO3 film; Multiferroicity; Raman spectrum,"Epitaxial ErMnO 3 thin films were grown on Pt-coated Al 2 O 3 substrate by pulsed laser deposition. Their structure, multiferroicity, and ferroelectric domain properties have been comprehensively characterized and studied. The XRD measurement indicated an excellent epitaxy of out-of-plane ErMnO 3 (0001)//Pt(111)//Al 2 O 3 (0001) and in-plane ErMnO 3 [1000]//Pt[11 2 ‾ ]//Al 2 O 3 [11 2 ‾ 0] structures. The as-deposited ErMnO 3 films exhibited spontaneous ferroelectric domains with reversible polarization. A significant remnant polarization of 1.3 μC/cm2 and an active peak at 662 cm−1 in Raman spectra were found, further showing a high quality of the ErMnO 3 thin films. Moreover, the magnetic measurements indicated that the thin film has an excellent anisotropic magnetic property with a Neel temperature at ≈53 K. • The films were grown with highly hexagonal epitaxy with out-of-plane ErMnO 3 (0001)//Pt(111)//Al 2 O 3 (0001) and in-plane ErMnO 3 [1000]//Pt[11 2 ]//Al 2 O 3 [11 2 0]. • The as-deposited ErMnO 3 films clearly exhibited spontaneous ferroelectric domains, which can be polarized reversibly with excellent retention performance. • A remanent polarization value of 1.3 μC/cm2 and a significant Raman-active peak at 662 cm−1 were observed. • The films exhibited a anisotropic magnetic properties with a Neel temperature of 53 K. [ABSTRACT FROM AUTHOR] Copyright of Journal of Alloys & Compounds is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141632063&site=ehost-live
415,Electrical transport and low-frequency noise in chemical vapor deposited single-layer MoS2 devices.,Qiliang Li,Nanotechnology,9574484,,4/18/14,25,15,155702,7,95012273,10.1088/0957-4484/25/15/155702,IOP Publishing,Article,MOLYBDENUM disulfide; CHEMICAL vapor deposition; ELECTRONIC noise; CHARGE exchange; FIELD-effect transistors; PASSIVATION,,"We have studied temperature-dependent (77–300 K) electrical characteristics and low-frequency noise (LFN) in chemical vapor deposited (CVD) single-layer molybdenum disulfide (MoS2) based back-gated field-effect transistors (FETs). Electrical characterization and LFN measurements were conducted on MoS2 FETs with Al2O3 top-surface passivation. We also studied the effect of top-surface passivation etching on the electrical characteristics of the device. Significant decrease in channel current and transconductance was observed in these devices after the Al2O3 passivation etching. For passivated devices, the two-terminal resistance variation with temperature showed a good fit to the activation energy model, whereas for the etched devices the trend indicated a hopping transport mechanism. A significant increase in the normalized drain current noise power spectral density (PSD) was observed after the etching of the top passivation layer. The observed channel current noise was explained using a standard unified model incorporating carrier number fluctuation and correlated surface mobility fluctuation mechanisms. Detailed analysis of the gate-referred noise voltage PSD indicated the presence of different trapping states in passivated devices when compared to the etched devices. Etched devices showed weak temperature dependence of the channel current noise, whereas passivated devices exhibited near-linear temperature dependence. [ABSTRACT FROM AUTHOR] Copyright of Nanotechnology is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95012273&site=ehost-live
416,Gate assisted Kelvin test structure to measure the electron and hole flows at the same nanowire contacts.,Qiliang Li,Applied Physics Letters,36951,,9/29/14,105,13,1,4,98710878,10.1063/1.4897008,American Institute of Physics,Article,SILICON nanowires; LOGIC circuits; ELECTRONS; FIELD-effect transistors; MICROFABRICATION; NANOELECTRONICS,,"A gate assisted Kelvin test structure based on Si nanowire field effect transistors has been designed and fabricated for the characterization of the transistor source/drain contacts. Because the Si nanowire field effect transistors exhibit ambipolar characteristics with electron current slightly lower than the hole current, we can select the type of carriers (electrons or holes) flowing through the same contacts and adjust the current by the applied gate voltage. In this way, we are able to measure the characteristics of the same contact with either pure electron or hole flow. In addition, we found that the nanowire contacts behave very differently depending on the current flow directions. This indicates that the source and drain contact resistance can be dramatically different. Such a gate assisted Kelvin Test structure will lead to future metrology and applications in nanoelectronics. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98710878&site=ehost-live
417,Hole doping induced half-metallic itinerant ferromagnetism and giant magnetoresistance in CrI3 monolayer.,Qiliang Li,Applied Surface Science,1694332,,Jan-21,535,,N.PAG,1,146612864,10.1016/j.apsusc.2020.147693,Elsevier B.V.,Article,MONTE Carlo method; GIANT magnetoresistance; FERROMAGNETISM; CURIE temperature; MAGNETIC storage; MONOMOLECULAR films,Carrier doping; Giant magnetoresistance; Half metal; Itinerant ferromagnetism; Two dimensional monolayer,"• The magnetoresistance over 106% is achieved via hole doping in 1L-CrI 3 by NEGF. • Hole doping renders 1L-CrI 3 half-metallic and nearly 100% spin-polarization. • Hole doping significantly enhances ferromagnetic stability and Curie temperature. The exploit of magnetic devices with high magnetoresistance is vital for the development of magnetic sensing and data storage technologies. Here, using density functional calculations combined with Monte Carlo simulations, we explore the magnetic properties and spin-dependent transport of CrI 3 monolayer under an electrostatic hole doping. Extraordinarily, the magnetoresistance can be controlled over 106% within a certain doping density range. The hole doping can render CrI 3 monolayer half-metallic and nearly 100% spin-polarization at Fermi energy level can be achieved. Moreover, the hole doping can significantly enhance the stability of itinerant ferromagnetism. The Heisenberg exchange parameters can be significantly improved and meanwhile, the Curie temperature can be boosted to room temperature via a doping density of 8.49 × 1014 cm−2. This study reveals that the carrier doping engineering can enable two-dimensional CrI 3 as a remarkable material for developing practical and high-performance spintronic nanodevices. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146612864&site=ehost-live
418,Maritime vessel emission monitoring by an UAV gas sensor system.,Qiliang Li,Ocean Engineering,298018,,Dec-20,218,,N.PAG,1,147604796,10.1016/j.oceaneng.2020.108206,Elsevier B.V.,Article,"NAVIGATION; ATMOSPHERIC turbulence; TRACKING algorithms; DRONE aircraft; MOBILE robots; DETECTORS; Search, Detection, Navigation, Guidance, Aeronautical, and Nautical System and Instrument Manufacturing",Gas detection; Joint tracking; Maritime monitoring; UAV (Unmanned aerial vehicle); Vessel emission,"Monitoring the gas emission of maritime vessels is very challenging as the fields are almost inaccessible and the emission is very susceptible to the surrounding environment. In ultra-large-scale maritime scenes, mobile robots capable of gas detection and tracking will most likely fall into atmospheric turbulence. In this work, an enhanced tracking algorithm using our unmanned aerial vehicle (UAV) gas sensor system is proposed for maritime vessel emission monitoring. A global prediction is acquired by modelling the emission of individual vessel, while another local gradient direction can be calculated from the real-time onboard sensor measurements. By a probabilistic framework, the global prediction and the gradient detection are fused to generate a more reliable navigation for the UAV. In order to verify the proposed algorithm, real monitoring experiments using the developed UAV gas sensor system are presented and SO 2 , NO x contents in vessel emissions are conducted. A comparison between the UAV-based detections and the off-line sampled data is preformed, and the results indicate that the proposed algorithm has the advantage in exactly guiding the UAV towards the vessel emission. • Aim to utilizing UAV for monitoring the gas emissions from maritime vessels.. • A vector tracking algorithm for UAV to detect and track the vessel emissions.. • A series of comprehensive on-the-spot experiments have been performed.. • The proposed UAV system and algorithm is applicable well to maritime situation.. [ABSTRACT FROM AUTHOR] Copyright of Ocean Engineering is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147604796&site=ehost-live
419,Non-volatile memory with self-assembled ferrocene charge trapping layer.,Qiliang Li,Applied Physics Letters,36951,,7/29/13,103,5,53102,4,89546936,10.1063/1.4817009,American Institute of Physics,Article,"NONVOLATILE memory; FERROCENE; ELECTRIC charge; CAPACITORS; X-ray photoelectron spectroscopy; METALLIC oxides; Electronic components, navigational and communications equipment and supplies merchant wholesalers; All Other Miscellaneous Electrical Equipment and Component Manufacturing; Capacitor, Resistor, Coil, Transformer, and Other Inductor Manufacturing; Other Electronic Parts and Equipment Merchant Wholesalers",,"A metal/oxide/molecule/oxide/Si capacitor structure containing redox-active ferrocene molecules has been fabricated for non-volatile memory application. Cyclic voltammetry and X-ray photoelectron spectroscopy were used to measure the molecules in the structure, showing that the molecules attach on SiO2/Si and the molecules are functional after device fabrication. These solid-state molecular memory devices have fast charge-storage speed and can endure more than 109 program/erase cycles. This excellent performance is derived from the intrinsic properties of the redox-active molecules and the hybrid Si-molecular device structure. These molecular devices are very attractive for future high-level non-volatile memory applications. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89546936&site=ehost-live
420,Novel Te doping in Y2O3–Al2O3 system phosphor.,Qiliang Li,Journal of Alloys & Compounds,9258388,,Apr-20,821,,N.PAG,1,141632021,10.1016/j.jallcom.2019.153474,Elsevier B.V.,Article,YTTRIUM aluminum garnet; PHOSPHORS; DENSITY of states; OPTICAL properties; GALLIUM antimonide; YAMS; TELLURIUM; POWDERS; Potato Farming,First-principles; Luminescence; Te doping; Y3Al5O12; Y4Al2O9,"Te doped Yttrium-aluminum (Y 2 O 3 –Al 2 O 3) system powders were synthesized by solid-state method. The structure and optical properties were investigated by XRD, SEM, XPS and PL. XRD analysis indicated that the samples transform from YAM, YAP to YAG phase as the sintering temperature rises. The samples sintered below 1200 °C have a broad emission centered at 604 nm with a lifetime up to 13.33 μs under room temperature. While in the mix phase of YAP and YAG, a sharp and intense emission peak centered at 715 nm was monitored with a lifetime up to approximate 2.76 ms. Moreover, the band structure and density of states for Te doped YAGwere investigated by first-principle calculations. [ABSTRACT FROM AUTHOR] Copyright of Journal of Alloys & Compounds is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141632021&site=ehost-live
421,Novel Two-Dimensional Mechano-Electric Generators and Sensors Based on Transition Metal Dichalcogenides.,Qiliang Li,Scientific Reports,20452322,,8/7/15,,,12854,1,108797757,10.1038/srep12854,Springer Nature,Article,"ELECTROMECHANICAL devices; ELECTRIC generators; TRANSITION metals; CHALCOGENIDES; BAND gaps; All other building equipment contractors; Other Building Equipment Contractors; Motor and Generator Manufacturing; Electrical Apparatus and Equipment, Wiring Supplies, and Related Equipment Merchant Wholesalers; Electrical wiring and construction supplies merchant wholesalers; Totalizing Fluid Meter and Counting Device Manufacturing; Automatic Environmental Control Manufacturing for Residential, Commercial, and Appliance Use; Other Electronic Component Manufacturing; Other Communications Equipment Manufacturing; Semiconductor and other electronic component manufacturing",,"Transition metal dichalcogenides (TMDCs), such as MoS2 and WSe2, provide two-dimensional atomic crystals with semiconductor band gap. In this work, we present a design of new mechano-electric generators and sensors based on transition metal dichalcogenide nanoribbon PN junctions and heterojunctions. The mechano-electric conversion was simulated by using a first-principle calculation. The output voltage of MoS2 nanoribbon PN junction increases with strain, reaching 0.036 V at 1% strain and 0.31 V at 8% strain, much larger than the reported results. Our study indicates that the length, width and layer number of TMDC nanoribbon PN junctions have an interesting but different impact on the voltage output. Also, the results indicate that doping position and concentration only cause a small fluctuation in the output voltage. These results have been compared with the mechano-electric conversion of TMDC heterojunctions. Such novel mechano-electric generators and sensors are very attractive for applications in future self-powered, wearable electronics and systems. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108797757&site=ehost-live
422,"Phase transition, effective mass and carrier mobility of MoS2 monolayer under tensile strain.",Qiliang Li,Applied Surface Science,1694332,,Jan-15,325,,27,6,100234033,10.1016/j.apsusc.2014.11.079,Elsevier B.V.,Article,PHASE transitions; MOLYBDENUM compounds; MONOMOLECULAR films; TENSILE strength; COMPUTATIONAL chemistry; ELECTRON mobility; DEFORMATION potential,Mobility enhancement; MoS 2 monolayer; Phase transition; Strain effect; Two-dimensional materials,"We report a computational study on the impact of tensile strain on MoS 2 monolayer. The transition between direct and indirect bandgap structure and the transition between semiconductor and metal phases in the monolayer have been investigated with tensile strain along all direction configurations with both x -axis and y -axis components ɛ xy ( ɛ x and ɛ y ). Electron effective mass and the hole effective mass are isotropic for biaxial strain ɛ xy = ɛ x = ɛ y and anisotropic for ɛ xy with ɛ x ≠ ɛ y . The carrier effective mass behaves differently along different directions in response to the tensile strain. In addition, the impact of strain on carrier mobility has been studied by using the deformation potential theory. The electron mobility increases over 10 times with the biaxial strain: ɛ x = ɛ y = 9.5%. Also, the mobility decreases monotonically with the increasing temperature as μ ∼ T −1 . These results are very important for future nanotechnology based on two-dimensional materials. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100234033&site=ehost-live
423,Polarization tunability in multiferroic DyMn2O5: Influence of Y and Eu co-doping and 3d-4f exchange.,Qiliang Li,Solid State Communications,381098,,Feb-20,307,,N.PAG,1,142250119,10.1016/j.ssc.2019.113809,Elsevier B.V.,Article,SPECIFIC heat; DATA warehousing; MANGANITE; MAGNETIC fields; FERROELECTRICITY; RARE earth metals; All Other Metal Ore Mining,DyMn2O5; Ferrielectricity; Manganite; Multiferroicity,"Coupling effects among spin, charge, and lattice in a strongly correlated system are critical for next generation spintronic and data storage devices. However, the complex effects are elusive and difficult to distinguish their contributions to polarization modulation. Here we tailored the polarization by co-doping of non-magnetic Y and Eu at A-sites in DyMn 2 O 5. The structure, specific heat, magnetism, and ferroelectricity of the polycrystalline Dy 1-x (Eu 0.24 Y 0.76) x Mn 2 O 5 ceramics were comprehensively explored. Interestingly, the co-doping does not cause lattice distortion of DyMn 2 O 5 , and all the ceramics are orthorhombic structures, while the independent Dy3+ spin order and the Dy3+-Mn3+ coupling can be suppressed. With increasing the co-doping content x , the spins related properties associated with the Dy3+-Mn4+-Dy3+ sub-lattice are progressively inhibited, while they keep less disturbance in the Mn3+-Mn4+-Mn3+ block. Moreover, the spin coupling of Dy3+-Mn3+ ions is stronger again the magnetic field than that of Dy3+-Mn3+. Our results enhance the understanding of ferrielectricity in DyMn 2 O 5 , and provide a method for controlling the polarization in the multiferroic manganite coexisting 3 d and 4 f elements. [ABSTRACT FROM AUTHOR] Copyright of Solid State Communications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142250119&site=ehost-live
424,Polarization tunable and enhanced photovoltaic properties in tetragonal-like BiFeO3 epitaxial films with graphene top electrode.,Qiliang Li,Journal of Alloys & Compounds,9258388,,Nov-19,811,,N.PAG,1,138815563,10.1016/j.jallcom.2019.152013,Elsevier B.V.,Article,EPITAXY; OPEN-circuit voltage; PULSED laser deposition; PHOTOVOLTAIC effect; ENERGY conversion; FERROELECTRIC materials; SCHOTTKY barrier,Graphene; Photovoltaic effect; Polarization; Tetragonal-like BiFeO3,"Ferroelectric photovoltaic materials have attracted intensive interest due to their intriguing above-bandgap photovoltage, while the low photocurrent limits their further device applications. In this work, both enhanced and tunable photovoltaic effect (PVE) are demonstrated in sandwiched structure of Graphene/tetragonal-like BiFeO 3 /Ca 0.96 Ce 0.04 MnO 3 (Graphene/T(-like) BFO/CCMO). The epitaxial BFO film is grown on the CCMO buffered LaAlO 3 substrate by pulsed laser deposition and the mechanically exfoliated graphene is transferred directly onto the BFO film as top electrode. The optimized open circuit voltage (V oc) and short circuit current density (J sc) are measured to be −0.88 V and 2.56 mA/cm2, respectively. Moreover, the photovoltaic response can be modulated by controllable ferroelectric polarization, whereby both the V oc and J sc show piezoresponse-like hysteresis behaviors against voltage. The notably enhanced PVE is likely a result of the combination effects of large polarization in the T(-like) BFO film, partially unscreened depolarization field, high transmittance and conductivity of the graphene top electrode. This work clearly indicates the potential of Graphene/ferroelectric photovoltaic devices for memory and energy conversion applications. • New nanocapacitor consists tetragonal-like BFO and exfoliated ultrathin graphene. • Open circuit voltage and short circuit current density are −0.88 V and 2.56 mA/cm2. • The photovoltaic response can be well modulated by ferroelectric polarization. • The mechanism relies on the polarization modulated Schottky barrier change. • The ultrathin graphene further enhances the photovoltaic response. [ABSTRACT FROM AUTHOR] Copyright of Journal of Alloys & Compounds is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138815563&site=ehost-live
425,Ultraviolet/ozone treatment to reduce metal-graphene contact resistance.,Qiliang Li,Applied Physics Letters,36951,,5/6/13,102,18,183110,5,87545140,10.1063/1.4804643,American Institute of Physics,Article,ELECTRIC properties of graphene; METAL fabrication; CHEMICAL vapor deposition; PHOTOLITHOGRAPHY; METHYL methacrylate; PHOTORESIST materials; RAMAN spectroscopy; Other plate work and fabricated structural product manufacturing,,"We report reduced contact resistance of single-layer graphene devices by using ultraviolet ozone treatment to modify the metal/graphene contact interface. The devices were fabricated from mechanically transferred, chemical vapor deposition grown single layer graphene. Ultraviolet ozone treatment of graphene in the contact regions as defined by photolithography and prior to metal deposition was found to reduce interface contamination originating from incomplete removal of poly(methyl-methacrylate) and photoresist. Our control experiment shows that exposure times up to 10 min did not introduce significant disorder in the graphene as characterized by Raman spectroscopy. By using the described approach, contact resistance of less than 200 Ω μm was achieved for 25 min ultraviolet ozone treatment, while not significantly altering the electrical properties of the graphene channel region of devices. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=87545140&site=ehost-live
426,An agent-based system with temporal data mining for monitoring financial stability on insurance markets.,Jessica Lin,Expert Systems with Applications,9574174,,Jun-19,123,,270,13,134739097,10.1016/j.eswa.2019.01.049,Elsevier B.V.,Article,"DATA mining; MULTIAGENT systems; INSURANCE companies; EXPERT systems; ECONOMIC forecasting; Other Direct Insurance (except Life, Health, and Medical) Carriers; Direct Health and Medical Insurance Carriers; Direct group life, health and medical insurance carriers; Insurance Agencies and Brokerages",Agents; Anomaly; Automobile insurance; Crisis; Cycle; Motif,"Highlights • Knowledge discovery on agent-based simulations predicts market instability. • Cycles on insurance markets can be forecast. • Aggressive price-undercutting by some insurers creates a winners curse. • Regulators should monitor pricing patterns of insurers. Abstract We describe an expert system to monitor the stability of insurance markets. It consists of two components: an agent-based simulation component and a temporal data mining component. Like other financial markets, insurance markets experience destabilizing cycles and suffer episodic crises. The expert system assists market regulators by monitoring the financial position of individual insurers and of the overall market, and by forecasting cycles and impending insolvencies. The agent-based simulation component runs a forward simulation allowing for interaction among insurers in a competitive market, and between insurers and customers. The temporal data mining component extracts useful information for market regulators from the simulations. A prototype of the system is applied to the automobile insurance market. We show how the system may be used to forecast cycles, investigate stability, and analyze insurers' herding behavior on the market. A practical policy conclusion is that regulators should monitor individual insurers' pricing pattern because aggressive price undercutting creates a ""winner's curse"", with subsequent losses and market instability. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems with Applications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134739097&site=ehost-live
427,CPM: A general feature dependency pattern mining framework for contrast multivariate time series.,Jessica Lin,Pattern Recognition,313203,,Apr-21,112,,N.PAG,1,148407299,10.1016/j.patcog.2020.107711,Elsevier B.V.,Article,TIME series analysis; DISTRIBUTION (Probability theory); SEQUENTIAL pattern mining; PROCESS optimization; SCALABILITY; INTERVENTION (Federal government); GAUSSIAN distribution,Contrast pattern; Controlled experiment; Driving behavior; Feature dependency; Multivariate time series,"• Unsupervised framework to mine contrast patterns in controlled experiment. • Customizable regularization techniques. • Efficient optimization algorithm easily adapt to various models under the framework. • Highly interpretable results in real world controlled experiments. With recent advances in sensor technology, multivariate time series data are becoming extremely large with sophisticated but insightful inter-variable dependency patterns. Mining contrast dependency patterns in controlled experiments can help quantify the differences between control and experimental time series, however, overwhelms practitioners' capability. Existing methods suffer from determining whether the differences are caused by the intervention or by different states. We propose a novel Contrast Pattern Mining (CPM) framework to find the intervention-related differences by jointly determining and characterizing the dynamic states in both time series via multivariate Gaussian distributions. Under the CPM framework, we not only propose a new covariance-based contrast pattern model, but also integrate our previous proposed partial correlation-based model as a special case. An efficient generic algorithm is developed to optimize various CPM models by adjusting one of the sub-routines. Comprehensive experiments are conducted to analyze the effectiveness, scalability, utility, and interpretability of the proposed framework. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148407299&site=ehost-live
428,Efficient Discovery of Unusual Patterns in Time Series.,Jessica Lin,New Generation Computing,2883635,,2007,25,1,61,33,23971598,,Springer Nature,Article,"TIME Series Processor (Computer program language); ANOMALY detection (Computer security); MARKOV processes; DATABASE management; COMPUTER algorithms; RESEARCH & development; Data Processing, Hosting, and Related Services; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",Anomaly Detection; Markov Model Feature Extraction; Novelty Detection; Suffix Tree; Time Series,"The problem of finding a specified pattern in a time series database (i.e., query by content) has received much attention and is now a relatively mature field. In contrast, the important problem of enumerating all surprising or interesting patterns has received far less attention. This problem requires a meaningful definition of ‘surprise’, and an efficient search technique. All previous attempts at finding surprising patterns in time series use a very limited notion of surprise, and/or do not scale to massive datasets. To overcome these limitations we propose a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance, given some previously seen data. This notion has the advantage of not requiring the user to explicitly define what is a surprising pattern, which may be hard, or perhaps impossible, to elicit from a domain expert. Instead, the user gives the algorithm a collection of previously observed ‘normal’ data. Our algorithm uses a suffix tree to efficiently encode the frequency of all observed patterns and allows a Markov model to predict the expected frequency of previously unobserved patterns. Once the suffix tree has been constructed, a measure of surprise for all the patterns in a new database can be determined in time and space linear in the size of the database. We demonstrate the utility of our approach with an extensive experimental evaluation. [ABSTRACT FROM AUTHOR] Copyright of New Generation Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23971598&site=ehost-live
429,Teaching as a Design Process: A Framework for Design-based Research in Engineering Education.,Craig Lorie,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,11,116026269,,ASEE,Article,TEACHING; ENGINEERING education; HUMAN ecology; DESIGN education; ENGINEERING models,,"The article provides information on teaching aspects related to design process and design based developmental research in engineering education. Topics discussed include characterization of human environment under constraints, methodologies of design research, and activities to demonstrate teaching practice.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116026269&site=ehost-live
431,Assessment of Level-3 Gridded Global Precipitation Mission (GPM) Products Over Oceans.,Viviana Maggioni,Remote Sensing,20724292,,Feb-19,11,3,255,1,134843738,10.3390/rs11030255,MDPI,Article,METEOROLOGICAL precipitation measurement; METEOROLOGICAL satellites; ATMOSPHERIC effects on remote sensing; RADAR meteorology; SATELLITE-based remote sensing; STANDARD deviations; ERROR analysis in mathematics; GLOBAL Precipitation Climatology Project,error analysis; precipitation; satellite remote sensing; triple collocation,"The performance of Level-3 gridded Global Precipitation Mission (GPM)-based precipitation products (IMERG, Integrated Multi-satellite Retrievals for GPM) is assessed against two references over oceans: the OceanRAIN dataset, derived from oceanic shipboard disdrometers, and a satellite-based radar product (the Level-3 Dual-frequency Precipitation Radar, 3DPRD). Daily IMERG products (early, late, final) and microwave-only (MW) and Infrared-only (IR) precipitation components are evaluated at four different spatial resolutions (0.5°, 1°, 2°, and 3°) during a 3-year study period (March 2014–February 2017). Their performance is assessed based on both categorical and continuous performance metrics, including correlation coefficient, probability of detection, success ratio, bias, and root mean square error (RMSE). A triple collocation analysis (TCA) is also presented to further investigate the performance of these satellite-based products. Overall, the IMERG products show an underestimation with respect to OceanRAIN. Rain events in OceanRAIN are correctly detected by all IMERG products ~80% of the times. IR estimates show relatively large errors and low correlations with OceanRAIN compared to the other products. On the other hand, the MW component performs better than other products in terms of both categorical and continuous statistics. TCA reveals that 3DPRD performs consistently better than OceanRAIN in terms of RMSE and coefficient of determination at all spatial resolutions. This work is part of a larger effort to validate GPM products over nontraditional regions such as oceans. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134843738&site=ehost-live
433,Building an Online Learning Module for Satellite Remote Sensing Applications in Hydrologic Science.,Viviana Maggioni,Remote Sensing,20724292,,Sep-20,12,18,3009,1,146537753,10.3390/rs12183009,MDPI,Letter,REMOTE sensing; LEARNING modules; ONLINE education; REMOTE-sensing images; HYDROLOGIC cycle; ENVIRONMENTAL sciences; MULTISPECTRAL imaging,active learning; authentic task; constructive alignment; e-learning; higher education; remote sensing,"This article presents an online teaching tool that introduces students to basic concepts of remote sensing and its applications in hydrology. The learning module is intended for junior/senior undergraduate students or junior graduate students with no (or little) prior experience in remote sensing, but with some basic background of environmental science, hydrology, statistics, and programming. This e-learning environment offers background content on the fundamentals of remote sensing, but also integrates a set of existing online tools for visualization and analysis of satellite observations. Specifically, students are introduced to a variety of satellite products and techniques that can be used to monitor and analyze changes in the hydrological cycle. At completion of the module, students are able to visualize remote sensing data (both in terms of time series and spatial maps), detect temporal trends, interpret satellite images, and assess errors and uncertainties in a remote sensing product. Students are given the opportunity to check their understanding as they progress through the module and also tackle complex real-life problems using remote sensing observations that professionals and scientists commonly use in practice. The learning tool is implemented in HydroLearn, an open-source, online platform for instructors to find and share learning modules and collaborate on developing teaching resources in hydrology and water resources. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146537753&site=ehost-live
434,Centroidal Voronoi tessellation based methods for optimal rain gauge location prediction.,Viviana Maggioni,Journal of Hydrology,221694,,May-20,584,,N.PAG,1,142766578,10.1016/j.jhydrol.2020.124651,Elsevier B.V.,Article,CENTROIDAL Voronoi tessellations; PRECIPITATION gauges; RAIN gauges; FORECASTING; PRECIPITATION variability; PATTERNMAKING; GAUGE field theory; NORTHERN Italy; OKLAHOMA; Other Measuring and Controlling Device Manufacturing,CVT; Decorrelation; Optimal placement; Rain gauges,"• An automated strategy to find optimal precipitation gauge locations. • Allow users customizing the level of confidence when placing gauges. • Potentially flexible to any precipitation dataset. With more satellite and model precipitation data becoming available, new analytical methods are needed that can take advantage of emerging data patterns to make well informed predictions in many hydrological applications. We propose a new strategy where we extract precipitation variability patterns and use correlation map to build the resulting density map that serves as an input to centroidal Voronoi tessellation construction that optimizes placement of precipitation gauges. We provide results of numerical experiments based on the data from the Alto-Adige region in Northern Italy and Oklahoma and compare them against actual gauge locations. This method provides an automated way for choosing new gauge locations and can be generalized to include physical constraints and to tackle other types of resource allocation problems. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142766578&site=ehost-live
435,Characteristics and Diurnal Cycle of GPM Rainfall Estimates over the Central Amazon Region.,Viviana Maggioni,Remote Sensing,20724292,,Jul-16,8,7,544,20,117069858,10.3390/rs8070544,MDPI,Article,DIURNAL variations of rainfall; ARTIFICIAL satellites; RAINFALL; HAZARD mitigation; METEOROLOGICAL precipitation measurement; RADAR meteorology; Space Research and Technology,GoAmazon; GPM; GPROF; IMERG; radar rainfall estimates; satellite rainfall estimates; uncertainty quantification,"Studies that investigate and evaluate the quality, limitations and uncertainties of satellite rainfall estimates are fundamental to assure the correct and successful use of these products in applications, such as climate studies, hydrological modeling and natural hazard monitoring. Over regions of the globe that lack in situ observations, such studies are only possible through intensive field measurement campaigns, which provide a range of high quality ground measurements, e.g., CHUVA (Cloud processes of tHe main precipitation systems in Brazil: A contribUtion to cloud resolVing modeling and to the GlobAl Precipitation Measurement) and GoAmazon (Observations and Modeling of the Green Ocean Amazon) over the Brazilian Amazon during 2014/2015. This study aims to assess the characteristics of Global Precipitation Measurement (GPM) satellite-based precipitation estimates in representing the diurnal cycle over the Brazilian Amazon. The Integrated Multi-satellitE Retrievals for Global Precipitation Measurement (IMERG) and the Goddard Profiling Algorithm--Version 2014 (GPROF2014) algorithms are evaluated against ground-based radar observations. Specifically, the S-band weather radar from the Amazon Protection National System (SIPAM), is first validated against the X-band CHUVA radar and then used as a reference to evaluate GPM precipitation. Results showed satisfactory agreement between S-band SIPAM radar and both IMERG and GPROF2014 algorithms. However, during the wet season, IMERG, which uses the GPROF2014 rainfall retrieval from the GPM Microwave Imager (GMI) sensor, significantly overestimates the frequency of heavy rainfall volumes around 00:00-04:00 UTC and 15:00-18:00 UTC. This overestimation is particularly evident over the Negro, Solimões and Amazon rivers due to the poorly-calibrated algorithm over water surfaces. On the other hand, during the dry season, the IMERG product underestimates mean precipitation in comparison to the S-band SIPAM radar, mainly due to the fact that isolated convective rain cells in the afternoon are not detected by the satellite precipitation algorithm. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117069858&site=ehost-live
436,Comparing global passive microwave freeze/thaw records: Investigating differences between Ka- and L-band products.,Viviana Maggioni,Remote Sensing of Environment,344257,,Sep-20,247,,N.PAG,1,144459988,10.1016/j.rse.2020.111936,Elsevier B.V.,Article,MICROWAVE radiometry; MICROWAVES; ARID regions; THAWING; SHRUBLANDS; SOIL moisture measurement; COLD regions,Climate; Cryosphere; Freeze-thaw earth system data record (FT-ESDR); Freeze/thaw; Microwave radiometry; Passive microwave; Remote sensing; Satellite applications; Soil moisture active passive (SMAP),"The NASA L-Band Soil Moisture Active Passive (SMAP) satellite mission launched in 2015 has produced soil moisture and freeze thaw (FT) products at a global scale. While the use of L-band (1.41 GHz) passive microwave radiometry (P-MW) has proven useful in detecting changes in the surface FT state, these classifications have not been comprehensively assessed against similar existing FT products, such as the global FT record from the Special Sensor Microwave/Imager (SSM/I, Ka-band, 37.0 GHz) as part of the FT Earth System Data Record (FT-ESDR). In order to fill in this gap, this study investigates regions in which FT classifications diverge and identifies potential sources of classification variability. The SMAP and SSM/I FT records are compared over an extended period covering multiple seasonal cycles from April 2015 through December 2017. The spatially and temporally varying relationship between these products is examined in relation to climate (Köppen-Geiger climate classes and air temperature), MODIS (MoDerate Resolution Imaging Spectrometer) land cover, and topography (using Global Multi-resolution Terrain Elevation Data). SMAP and SSM/I FT product agreement proportion (Ap) was corrected for seasonality and then separated by land cover classes and compared to the global Ap mean. The agreement between these products vary most notably during freeze and thaw onset and in areas near abundant surface water, snow and ice, and wetlands. Relative to other vegetation types, reduced agreement between FT products is also observed over grasslands, sparsely vegetated lands, as well as mixed and evergreen forests. Distinct seasonal differences in FT classification agreement were also detected between products over cold arid regions and between continental and temperate classes. Similarly, as topographic complexity increases, a decreasing trend in agreement between L- and Ka-band FT products is observed. While reiterating challenges in FT classifications identified by prior studies, this work also contributes new insights by providing detailed geospatial and seasonal analyses into the factors contributing to FT product divergence. • We compare ~3 years of global microwave derived freeze/thaw (FT) records. • Variations in defined frozen extent are observed between L- and Ka-bands. • FT records from FT-ESDR and SMAP had classification agreement of 83.5%. • Product agreement is substantially diminished in shoulder seasons. • Distinct variability identified across climate, topography, and landcover. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing of Environment is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144459988&site=ehost-live
437,Complementing near-real time satellite rainfall products with satellite soil moisture-derived rainfall through a Bayesian Inversion approach.,Viviana Maggioni,Journal of Hydrology,221694,,Jun-19,573,,341,11,139236837,10.1016/j.jhydrol.2019.03.038,Elsevier B.V.,Article,SOIL moisture; RAINFALL; FLOOD forecasting; ARTIFICIAL satellites; PRECIPITATION probabilities; WATER levels; ITALY; Space Research and Technology,Predictive uncertainty; Rainfall; Soil moisture; Water resource management,"• A Bayesian approach has been used for merging multiple satellite rainfall products. • We created a superior product that can be efficiently run in near-real time. • Soil moisture can provide useful information for improving satellite rainfall. This work investigates the potential of using the Bayesian-based Model Conditional Processor (MCP) for complementing satellite precipitation products with a rainfall dataset derived from satellite soil moisture observations. MCP – which is a Bayesian Inversion approach – was originally developed for predictive uncertainty estimates of water level and discharge to support real-time flood forecasting. It is applied here for the first time to precipitation to provide its probability distribution conditional on multiple satellite precipitation estimates derived from TRMM Multi-Satellite Precipitation Analysis real-time product v.7.0 (3B42RT) and the soil moisture-based rainfall product SM2RAIN-CCI. In MCP, 3B42RT and SM2RAIN-CCI represent a priori information (predictors) about the ""true"" precipitation (predictand) and are used to provide its real-time a posteriori probabilistic estimate by means of the Bayes theorem. MCP is tested across Italy during a 6-year period (2010–2015) at daily/0.25 deg temporal/spatial scale. Results demonstrate that the proposed methodology provides rainfall estimates that are superior to both 3B42RT (as well as its successor IMERG-early run) and SM2RAIN-CCI in terms of both median bias, random errors and categorical scores. The study confirms that satellite soil moisture-derived rainfall can provide valuable information for improving state-of-the-art satellite precipitation products, thus making them more attractive for water resource management and large scale flood forecasting applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139236837&site=ehost-live
438,Development and Evaluation of Ensemble Consensus Precipitation Estimates over High Mountain Asia.,Viviana Maggioni,Journal of Hydrometeorology,1525755X,,Sep-22,23,9,1469,18,159348296,10.1175/JHM-D-21-0196.1,American Meteorological Society,Article,ATMOSPHERIC circulation; HYDROLOGIC cycle; COMMUNITIES; RAINFALL; ASIA,Asia; Climate variability; Climatology; Ensembles; Forcing; Hydrologic cycle; Precipitation; Rainfall; Snowfall,"Precipitation estimates are highly uncertain in complex regions such as High Mountain Asia (HMA), where ground measurements are very difficult to obtain and atmospheric dynamics poorly understood. Though gridded products derived from satellite-based observations and/or reanalysis can provide temporally and spatially distributed estimates of precipitation, there are significant inconsistencies in these products. As such, to date, there is little agreement in the community on the best and most accurate gridded precipitation product in HMA, which is likely area dependent because of HMA's strong heterogeneities and complex orography. Targeting these gaps, this article presents the development of a consensus ensemble precipitation product using three gridded precipitation datasets [the Integrated Multi-satellitE Retrievals for Global Precipitation Measurement (IMERG), the Climate Hazards Group Infrared Precipitation with Station data (CHIRPS), and the ECMWF reanalysis ERA5] with a localized probability matched mean (LPM) approach. We evaluate the performance of the LPM estimate along with a simple ensemble mean (EM) estimate to overcome the differences and disparities of the three selected constituent products on long-term averages and trends in HMA. Our analysis demonstrates that LPM reduces the high biases embedded in the ensemble members and provides more realistic spatial patterns compared to EM. LPM is also a good alternative for merging data products with different spatiotemporal resolutions. By filtering disparities among the individual ensemble members, LPM overcomes the problem of a certain product performing well only in a particular area and provides a consensus estimate with plausible temporal trends. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrometeorology is the property of American Meteorological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159348296&site=ehost-live
439,Investigating the Error Propagation from Satellite-Based Input Precipitation to Output Water Quality Indicators Simulated by a Hydrologic Model.,Viviana Maggioni,Remote Sensing,20724292,,Nov-20,12,22,3728,1,147276947,10.3390/rs12223728,MDPI,Article,WATER quality; TOTAL suspended solids; RAIN gauges; HYDROLOGIC models; WATER temperature; HYDROLOGY; WASHINGTON (D.C.); Other Measuring and Controlling Device Manufacturing,CMORPH; HSPF; modeling; PERSIANN; satellite-based precipitation products; TMPA; water quality,"This study investigated the propagation of errors in input satellite-based precipitation products (SPPs) on streamflow and water quality indicators simulated by a hydrological model in the Occoquan Watershed, located in the suburban Washington, D.C. area. A dense rain gauge network was used as reference to evaluate three SPPs which are based on different retrieval algorithms. A Hydrologic Simulation Program-FORTRAN (HSPF) hydrology and water quality model was forced with the three SPPs to simulate output of streamflow (Q), total suspended solids (TSS), stream temperature (TW), and dissolved oxygen (DO). Results indicate that the HSPF model may have a dampening effect on the precipitation-to-streamflow error. The bias error propagation of all three SPPs showed a positive dependency on basin scale for streamflow and TSS, but not for TW and DO. On a seasonal basis, bias error propagation varied by product, with larger values generally found in fall and winter. This study demonstrated that the spatiotemporal variability of SPPs, along with their algorithms to estimate precipitation, have an influence on water quality simulations in a hydrologic model. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147276947&site=ehost-live
440,"Modeling Satellite Precipitation Errors Over Mountainous Terrain: The Influence of Gauge Density, Seasonality, and Temporal Resolution.",Viviana Maggioni,IEEE Transactions on Geoscience & Remote Sensing,1962892,,Jul-17,55,7,4130,11,124146582,10.1109/TGRS.2017.2688998,IEEE,Article,METEOROLOGICAL precipitation; NATURAL satellites; HYDROLOGY; METEOROLOGICAL observations; RAINFALL intensity duration frequencies,Algorithm design and analysis; Complex terrain; Correlation; Estimation; precipitation; Rain; satellite observations; Satellites; Spatial resolution,"This paper contributes to the predictive understanding of satellite precipitation estimation errors over complex terrain, which is fundamental to the development of error models for improving hydrological applications. This paper focuses on the Trentino-Alto Adige region of the eastern Italian Alps. Rainfall observations over a 10-year period (2000–2009) from a dense rain gauge network in the region are used as reference precipitation. A number of satellite precipitation error properties (probability of detection, false alarm rates, missed events, spatial correlation of the error, and hit biases) are investigated in terms of seasonality, satellite algorithm, rainfall intensity, gauge density, and temporal resolution dependencies. These error parameters are typically used in error models (e.g., SREM2D) and provide the basis for enhancing error scheme development. Three widely used satellite-based precipitation products are employed: 1) the Climate Prediction Center morphing product; 2) the precipitation estimation from remotely sensed imagery using artificial neural networks; and 3) the Tropical Rainfall Measuring Mission multisatellite precipitation analysis 3B42 near-real-time product. The three products show similar performances, with larger errors during the warm season, characterized by convective storms, and less variability in the cold season, characterized by more organized stratiform systems. Lower biases are depicted at the daily scale with respect to the 3-hourly resolution. The SREM2D error model has the ability to correct the satellite precipitation products, even though attention is needed for potential systematic errors when applying the calibrated model to independent periods or regions. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Geoscience & Remote Sensing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124146582&site=ehost-live
441,On the performance of satellite precipitation products in riverine flood modeling: A review.,Viviana Maggioni,Journal of Hydrology,221694,,Mar-18,558,,214,11,128126433,10.1016/j.jhydrol.2018.01.039,Elsevier B.V.,Article,FLOODS; NATURAL satellites; METEOROLOGICAL precipitation; RAINFALL; HYDROLOGIC models; FLOOD forecasting; MATHEMATICAL models,Floods; Hydrologic modeling; Satellite precipitation,"This work is meant to summarize lessons learned on using satellite precipitation products for riverine flood modeling and to propose future directions in this field of research. Firstly, the most common satellite precipitation products (SPPs) during the Tropical Rainfall Measuring Mission (TRMM) and Global Precipitation Mission (GPM) eras are reviewed. Secondly, we discuss the main errors and uncertainty sources in these datasets that have the potential to affect streamflow and runoff model simulations. Thirdly, past studies that focused on using SPPs for predicting streamflow and runoff are analyzed. As the impact of floods depends not only on the characteristics of the flood itself, but also on the characteristics of the region (population density, land use, geophysical and climatic factors), a regional analysis is required to assess the performance of hydrologic models in monitoring and predicting floods. The performance of SPP-forced hydrological models was shown to largely depend on several factors, including precipitation type, seasonality, hydrological model formulation, topography. Across several basins around the world, the bias in SPPs was recognized as a major issue and bias correction methods of different complexity were shown to significantly reduce streamflow errors. Model re-calibration was also raised as a viable option to improve SPP-forced streamflow simulations, but caution is necessary when recalibrating models with SPP, which may result in unrealistic parameter values. From a general standpoint, there is significant potential for using satellite observations in flood forecasting, but the performance of SPP in hydrological modeling is still inadequate for operational purposes. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128126433&site=ehost-live
442,Propagation of satellite precipitation uncertainties through a distributed hydrologic model: A case study in the Tocantins–Araguaia basin in Brazil.,Viviana Maggioni,Journal of Hydrology,221694,,Aug-15,527,,943,15,108322177,10.1016/j.jhydrol.2015.05.042,Elsevier B.V.,Article,METEOROLOGICAL precipitation; HYDROLOGIC models; STREAMFLOW; RAINFALL; ARAGUAIA River (Brazil),Satellite rainfall; Streamflow ensemble; Tropical basin; Uncertainties precipitation,"Summary This study investigates the applicability of error corrections to satellite-based precipitation products in streamflow simulations. A three-year time series (2008–2011) is considered across 19 sub-basins of the Tocantins–Araguaia basin (764,000 km 2 ), located in the center-north region of Brazil. A raingauge network (24 h accumulation) of approximately 300 collection points (∼1 gauge every 2500 km 2 ) is used as reference for evaluating the following four satellite rainfall products: the Tropical Rainfall Measuring Mission real-time 3B42 product (3B42RT), the Climate Prediction Center morphing technique (CMORPH), the Global Satellite Mapping of Precipitation (GSMaP), and the NOAA Hydroestimator (HYDRO-E). Ensemble streamflow simulations, for both dry and rainy seasons, are obtained by forcing the Distributed Hydrological Model developed by the Brazilian National Institute for Space Research (MHD–INPE) with the satellite rainfall products, corrected using a two-dimensional stochastic satellite rainfall error model (SREM2D). The ensemble simulations are evaluated using streamflow output derived by forcing the model with reference rainfall gauge data. SREM2D is able to correct for errors in the satellite precipitation data by pushing the modeled streamflow ensemble closer to the reference river discharge, when compared to the simulations forced with uncorrected rainfall input. Ensemble streamflow error statistics (MAE and RMSE) show a decreasing trend as a function of the catchment area for all satellite products, but the rainfall-to-streamflow error propagation does not show any dependence on the basin size. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108322177&site=ehost-live
443,The Global Satellite Precipitation Constellation: Current Status and Future Requirements.,Viviana Maggioni,Bulletin of the American Meteorological Society,30007,,Oct-21,102,10,E1844,18,153454292,10.1175/BAMS-D-20-0299.1,American Meteorological Society,Article,SPATIAL resolution; PRECIPITATION variability; METEOROLOGICAL satellites; CONSTELLATIONS; RADIOMETERS,Instrumentation/sensors; Microwave observations; Precipitation; Rainfall; Satellite observations; Snowfall,"To address the need to map precipitation on a global scale, a collection of satellites carrying passive microwave (PMW) radiometers has grown over the last 20 years to form a constellation of about 10–12 sensors at any one time. Over the same period, a broad range of science and user communities has become increasingly dependent on the precipitation products provided by these sensors. The constellation presently consists of both conical and cross-track-scanning precipitation-capable multichannel instruments, many of which are beyond their operational and design lifetime but continue to operate through the cooperation of the responsible agencies. The Group on Earth Observations and the Coordinating Group for Meteorological Satellites (CGMS), among other groups, have raised the issue of how a robust, future precipitation constellation should be constructed. The key issues of current and future requirements for the mapping of global precipitation from satellite sensors can be summarized as providing 1) sufficiently fine spatial resolutions to capture precipitation-scale systems and reduce the beam-filling effects of the observations; 2) a wide channel diversity for each sensor to cover the range of precipitation types, characteristics, and intensities observed across the globe; 3) an observation interval that provides temporal sampling commensurate with the variability of precipitation; and 4) precipitation radars and radiometers in low-inclination orbit to provide a consistent calibration source, as demonstrated by the first two spaceborne radar–radiometer combinations on the Tropical Rainfall Measuring Mission (TRMM) and Global Precipitation Measurement (GPM) mission Core Observatory. These issues are critical in determining the direction of future constellation requirements while preserving the continuity of the existing constellation necessary for long-term climate-scale studies. [ABSTRACT FROM AUTHOR] Copyright of Bulletin of the American Meteorological Society is the property of American Meteorological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153454292&site=ehost-live
444,The impact of weather condition and social activity on COVID-19 transmission in the United States.,Viviana Maggioni,Journal of Environmental Management,3014797,,Jan2022:Part A,302,,N.PAG,1,153848748,10.1016/j.jenvman.2021.114085,Academic Press Inc.,Article,COVID-19; COVID-19 pandemic; WEATHER; SOCIAL history; HUMIDITY; UNITED States,COVID-19 transmission; Machine learning; Random forest regression model; Social activity factor; Weather condition,"The coronavirus disease 2019 (COVID-19) has been first reported in December 2019 and rapidly spread worldwide. As other severe acute respiratory syndromes, it is a widely discussed topic whether seasonality affects the COVID-19 infection spreading. This study presents two different approaches to analyse the impact of social activity factors and weather variables on daily COVID-19 cases at county level over the Continental U.S. (CONUS). The first one is a traditional statistical method, i.e., Pearson correlation coefficient, whereas the second one is a machine learning algorithm, i.e., random forest regression model. The Pearson correlation is analysed to roughly test the relationship between COVID-19 cases and the weather variables or the social activity factor (i.e. social distance index). The random forest regression model investigates the feasibility of estimating the number of county-level daily confirmed COVID-19 cases by using different combinations of eight factors (county population, county population density, county social distance index, air temperature, specific humidity, shortwave radiation, precipitation, and wind speed). Results show that the number of daily confirmed COVID-19 cases is weakly correlated with the social distance index, air temperature and specific humidity through the Pearson correlation method. The random forest model shows that the estimation of COVID-19 cases is more accurate with adding weather variables as input data. Specifically, the most important factors for estimating daily COVID-19 cases are the population and population density, followed by the social distance index and the five weather variables, with temperature and specific humidity being more critical than shortwave radiation, wind speed, and precipitation. The validation process shows that the general values of correlation coefficients between the daily COVID-19 cases estimated by the random forest model and the observed ones are around 0.85. • This study investigates the impact of social activity factors and weather variables on COVID-19 transmission in the U.S. • The random forest model shows the estimation of COVID-19 cases is more accurate with adding weather variables as input data. • The random forest model estimated number of COVID-19 cases are highly correlated with the observation data. [ABSTRACT FROM AUTHOR] Copyright of Journal of Environmental Management is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153848748&site=ehost-live
445,The impact of weather condition and social activity on COVID-19 transmission in the United States.,Viviana Maggioni,Journal of Environmental Management,3014797,,Jan2022:Part B,302,,N.PAG,1,153848842,10.1016/j.jenvman.2021.114085,Academic Press Inc.,Article,COVID-19; COVID-19 pandemic; WEATHER; SOCIAL history; HUMIDITY; UNITED States,COVID-19 transmission; Machine learning; Random forest regression model; Social activity factor; Weather condition,"The coronavirus disease 2019 (COVID-19) has been first reported in December 2019 and rapidly spread worldwide. As other severe acute respiratory syndromes, it is a widely discussed topic whether seasonality affects the COVID-19 infection spreading. This study presents two different approaches to analyse the impact of social activity factors and weather variables on daily COVID-19 cases at county level over the Continental U.S. (CONUS). The first one is a traditional statistical method, i.e., Pearson correlation coefficient, whereas the second one is a machine learning algorithm, i.e., random forest regression model. The Pearson correlation is analysed to roughly test the relationship between COVID-19 cases and the weather variables or the social activity factor (i.e. social distance index). The random forest regression model investigates the feasibility of estimating the number of county-level daily confirmed COVID-19 cases by using different combinations of eight factors (county population, county population density, county social distance index, air temperature, specific humidity, shortwave radiation, precipitation, and wind speed). Results show that the number of daily confirmed COVID-19 cases is weakly correlated with the social distance index, air temperature and specific humidity through the Pearson correlation method. The random forest model shows that the estimation of COVID-19 cases is more accurate with adding weather variables as input data. Specifically, the most important factors for estimating daily COVID-19 cases are the population and population density, followed by the social distance index and the five weather variables, with temperature and specific humidity being more critical than shortwave radiation, wind speed, and precipitation. The validation process shows that the general values of correlation coefficients between the daily COVID-19 cases estimated by the random forest model and the observed ones are around 0.85. • This study investigates the impact of social activity factors and weather variables on COVID-19 transmission in the U.S. • The random forest model shows the estimation of COVID-19 cases is more accurate with adding weather variables as input data. • The random forest model estimated number of COVID-19 cases are highly correlated with the observation data. [ABSTRACT FROM AUTHOR] Copyright of Journal of Environmental Management is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153848842&site=ehost-live
446,The Joint Assimilation of Remotely Sensed Leaf Area Index and Surface Soil Moisture into a Land Surface Model.,Viviana Maggioni,Remote Sensing,20724292,,Feb-22,14,3,437,1,155266389,10.3390/rs14030437,MDPI,Article,LEAF area index; SOIL moisture; SURFACE area; STANDARD deviations,data assimilation; Ensemble Kalman Filter; evapotranspiration; GLASS; net ecosystem exchange; SMAP,"This work tests the hypothesis that jointly assimilating satellite observations of leaf area index and surface soil moisture into a land surface model improves the estimation of land vegetation and water variables. An Ensemble Kalman Filter is used to test this hypothesis across the Contiguous United States from April 2015 to December 2018. The performance of the proposed methodology is assessed for several modeled vegetation and water variables (evapotranspiration, net ecosystem exchange, and soil moisture) in terms of random errors and anomaly correlation coefficients against a set of independent validation datasets (i.e., Global Land Evaporation Amsterdam Model, FLUXCOM, and International Soil Moisture Network). The results show that the assimilation of the leaf area index mostly improves the estimation of evapotranspiration and net ecosystem exchange, whereas the assimilation of surface soil moisture alone improves surface soil moisture content, especially in the western US, in terms of both root mean squared error and anomaly correlation coefficient. The joint assimilation of vegetation and soil moisture information combines the results of individual vegetation and soil moisture assimilations and reduces errors (and increases correlations with the reference datasets) in evapotranspiration, net ecosystem exchange, and surface soil moisture simulated by the land surface model. However, because soil moisture satellite observations only provide information on the water content in the top 5 cm of the soil column, the impact of the proposed data assimilation technique on root zone soil moisture is limited. This work moves one step forward in the direction of improving our estimation and understanding of land surface interactions using a multivariate data assimilation approach, which can be particularly useful in regions of the world where ground observations are sparse or missing altogether. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155266389&site=ehost-live
447,Towards hyper-resolution land-surface modeling of surface and root zone soil moisture.,Viviana Maggioni,Journal of Hydrology,221694,,Mar-21,594,,N.PAG,1,148984348,10.1016/j.jhydrol.2020.125945,Elsevier B.V.,Article,SOIL moisture; METEOROLOGICAL precipitation; WIND speed; WATER management; ATMOSPHERIC temperature; WATER supply; OKLAHOMA; Water Supply and Irrigation Systems,Hyper-resolution; Land surface modeling; Soil moisture,"• A new framework for modeling hyper-resolution soil moisture is developed. • Finer resolution forcing data improve modeled surface and root-zone soil moisture. • The resolution of input precipitation plays a critical role in improving soil moisture. The goal of this work is to estimate surface and root zone soil moisture at resolutions that are useful for decision making and water resources management. A 500-m atmospheric forcing dataset is developed from the 12.5-km NLDAS-2 (North America Land Data Assimilation System) products across Oklahoma, where high-quality observations are available for validation purposes. A land surface model is then forced with three combinations of input variables to simulate surface and root zone soil moisture: 1) NLDAS-2 atmospheric forcings at their original resolution; 2) downscaled NLDAS-2 atmospheric variables (i.e., near-surface air temperature and humidity, wind speed and direction, incident longwave and shortwave radiation, pressure) and original resolution NLDAS-2 precipitation; and 3) downscaled NLDAS-2 atmospheric variables and precipitation. Results show that the third simulation is able to bring modeled standard-normal deviates of both surface and root zone soil moisture closer to in-situ observations, whereas the second simulation only shows slight improvements with respect to one forced with original resolution NLDAS-2 data. This is particularly evident for negative values of standard-normal deviates, which correspond to drier than usual cases, due to the improved ability of the downscaled precipitation to detect missed events and no-rain cases. In summary, finer resolution forcings have the potential to improve simulations of soil moisture and the resolution of precipitation plays a critical role in improving time series of soil moisture standard-normal deviates. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148984348&site=ehost-live
448,"Using remote sensing and modeling techniques to investigate the annual parasite incidence of malaria in Loreto, Peru.",Viviana Maggioni,Advances in Water Resources,3091708,,Oct-17,108,,423,16,125374606,10.1016/j.advwatres.2016.11.009,Elsevier B.V.,Article,MALARIA; PLASMODIUM; DISEASE incidence; REMOTE sensing; LORETO (Peru : Dept.),Climate; Environment; Malaria; Modeling; Peru; Remote sensing,"Between 2001 and 2010 significant progress was made towards reducing the number of malaria cases in Peru; however, the country saw an increase between 2011 and 2015. This work attempts to uncover the associations among various climatic and environmental variables and the annual malaria parasite incidence in the Peruvian region of Loreto. A Multilevel Mixed-effects Poisson Regression model is employed, focusing on the 2009–2013 period, when trends in malaria incidence shifted from decreasing to increasing. The results indicate that variations in elevation (β = 0.78; 95% confidence interval (CI), 0.75–0.81), soil moisture (β = 0.0021; 95% CI, 0.0019–0.0022), rainfall (β = 0.59; 95% CI, 0.56–0.61), and normalized difference vegetation index (β = 2.13; 95% CI, 1.83–2.43) is associated with higher annual parasite incidence, whereas an increase in temperature (β = -0.0043; 95% CI, − 0.0044-− 0.0041) is associated with a lower annual parasite incidence. The results from this study are particularly useful for healthcare workers in Loreto and have the potential of being integrated within malaria elimination plans. [ABSTRACT FROM AUTHOR] Copyright of Advances in Water Resources is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125374606&site=ehost-live
449,Using Satellite Error Modeling to Improve GPM-Level 3 Rainfall Estimates over the Central Amazon Region.,Viviana Maggioni,Remote Sensing,20724292,,Feb-18,10,2,336,14,128347569,10.3390/rs10020336,MDPI,Article,MEASUREMENT errors; METEOROLOGICAL precipitation; REMOTE sensing; RAINFALL; SPATIAL distribution (Quantum optics),Amazon; error model; global precipitation measurement; IMERG; PUSH; validation,"This study aims to assess the characteristics and uncertainty of Integrated Multisatellite Retrievals for Global Precipitation Measurement (GPM) (IMERG) Level 3 rainfall estimates and to improve those estimates using an error model over the central Amazon region. The S-band Amazon Protection National System (SIPAM) radar is used as reference and the Precipitation Uncertainties for Satellite Hydrology (PUSH) framework is adopted to characterize uncertainties associated with the satellite precipitation product. PUSH is calibrated and validated for the study region and takes into account factors like seasonality and surface type (i.e., land and river). Results demonstrated that the PUSH model is suitable for characterizing errors in the IMERG algorithm when compared with S-band SIPAM radar estimates. PUSH could efficiently predict the satellite rainfall error distribution in terms of spatial and intensity distribution. However, an underestimation (overestimation) of light satellite rain rates was observed during the dry (wet) period, mainly over rivers. Although the estimated error showed a lower standard deviation than the observed error, the correlation between satellite and radar rainfall was high and the systematic error was well captured along the Negro, Solimões, and Amazon rivers, especially during the wet season. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128347569&site=ehost-live
450,A Derived Optimal Linear Interpolation approach for merging multiple satellite soil moisture-based rainfall products with IMERG early run.,Viviana Maggioni,Geophysical Research Abstracts,10297006,,2019,21,,1,1,140488939,,Copernicus Gesellschaft mbH,Article,"SOIL moisture; EARTH system science; SOIL moisture measurement; MOISTURE measurement; WEATHER forecasting; RAINFALL; HYDROLOGIC cycle; SEAWATER salinity; All Other Professional, Scientific, and Technical Services",,"As a natural feature of the Earth's weather system, rainfall is the main driver of the hydrological cycle. Rainfall plays an essential role in many applications including climate monitoring, extreme weather prediction and weather forecasting. On a global scale, ground-monitoring networks do not provide sufficient coverage and satellite rainfall products are often the only source of rainfall that guarantee a continuous temporal coverage. However, the indirect and the instantaneous nature of the measurement makes satellite rainfall products prone to errors (Kucera et al., 2013). Thanks to the strong connection between soil moisture and precipitation, capable to track accumulated precipitation estimates (rather than instantaneous), soil moisture can be successfully used to enhance the quality of satellite rainfall observations (Crow et al., 2011; Pellarin et al., 2013; Brocca et al. 2014). The SMOS+rainfall project of the European Space Agency (ESA), started in 2015 and concluded in 2017, has demonstrated the capability of the SMOS soil moisture product to enhance satellite rainfall information over land and has raised many interesting research questions related to the potential improvement that can be obtained by a combination of different soil moisture sensors. Here, we propose the use of a new near real time purely observational rainfall dataset derived from the combination of the Integrated Multi-Satellite Retrievals for GPM (IMERG early run) with multiple satellite rainfall products obtained from the inversion of the soil moisture retrievals derived from: 1) the Soil Moisture Active and Passive (SMAP) mission, 2) the Advanced Scatterometer (ASCAT) and 3) the Soil Moisture and Ocean Salinity (SMOS) mission via SM2RAIN (Brocca et al. 2014).The weighting method (Hobeichi et al. 2018) is based on a technique that provides an analytically optimal linear combination of rainfall products and accounts for both the performance differences and error covariance between the participating products. We examine the performance of the weighting approach in India, United States, Australia and Europe showing that the simultaneous use of soil moisture products is able to increase the quality of IMERG early run product and its performance for hydrological applications.Brocca et al., 2014. Soil as a natural rain gauge: estimating global rainfall from satellite soil moisture data. J. Geophy. Res. 119 (9), 5128–5141.Crow et al., 2011. Correcting rainfall using satellite-based surface soil moisture retrievals: the soil moisture analysis rainfall tool (SMART). Water Resour. Res. 47, W08521.Kucera et al. 2013. Precipitation from space: advancing earth system science. Bull. Am. Meteorol. Soc. 94, 365–375.Pellarin et al. 2013. A simple and effective method for correcting soil moisture and precipitation estimates using AMSR-E measurements. Remote Sens. Environ. 136, 28–36.Hobeichi et al. 2018. Derived Optimal Linear Combination Evapotranspiration (DOLCE): a global gridded synthesis ET estimate, Hydrol. Earth Syst. Sci., 22, 1317-1336. [ABSTRACT FROM AUTHOR] Copyright of Geophysical Research Abstracts is the property of Copernicus Gesellschaft mbH and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140488939&site=ehost-live
451,Applying a precipitation error model to numerical weather predictions for probabilistic flood forecasts.,Viviana Maggioni,Journal of Hydrology,221694,,Jul-21,598,,N.PAG,1,150933051,10.1016/j.jhydrol.2021.126374,Elsevier B.V.,Article,"NUMERICAL weather forecasting; FLOOD forecasting; WEATHER forecasting; STOCHASTIC models; BRAZIL; All Other Professional, Scientific, and Technical Services",Ensemble flood forecasting; Ensemble prediction system; Numerical weather prediction; Stochastic error model,"• A stochastic error model was applied to numerical weather predictions. • Rainfall ensemble forecasting fields were generated using the error model. • The error model proved to be efficient to remove forecasts biases. • Error model flood forecasts performed similarly to expensive consecrated techniques. This work investigates the use of a stochastic error model (the 2-Dimensional Satellite Rainfall Error Model-SREM2D) to generate an ensemble of rainfall fields, based on the forecasts from the Eta regional weather forecast model. To evaluate the usefulness of this approach against traditional techniques, streamflow probabilistic forecasts from a distributed hydrological model forced with two sources of rainfall data are compared in the Tocantins-Araguaia basin in Brazil. The first dataset is an empirical rainfall ensemble produced by the SREM2D model applied to the Eta model, and the second is a state-of-the-art rainfall ensemble produced by the ECMWF model. Results show the potential of the stochastic error model to generate precipitation ensemble fields from a regional numerical weather forecasting model removing around 60% and 12% of the systematic and random error, respectively. Moreover, SREM2D is proven to be an efficient technique that involves a low computational cost when compared to the more sophisticated ensemble techniques used by the ECMWF model. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150933051&site=ehost-live
452,Data Assimilation of Terrestrial Water Storage Observations to Estimate Precipitation Fluxes: A Synthetic Experiment.,Viviana Maggioni,Remote Sensing,20724292,,Mar-21,13,6,1223,1,149574591,10.3390/rs13061223,MDPI,Article,WATER storage; SOIL moisture; FLUX (Energy); STREAMFLOW; RUNOFF,data assimilation; GRACE; precipitation; TWS,"The Gravity Recovery and Climate Experiment (GRACE) mission and its Follow-On (GRACE-FO) mission provide unprecedented observations of terrestrial water storage (TWS) dynamics at basin to continental scales. Established GRACE data assimilation techniques directly adjust the simulated water storage components to improve the estimation of groundwater, streamflow, and snow water equivalent. Such techniques artificially add/subtract water to/from prognostic variables, thus upsetting the simulated water balance. To overcome this limitation, we propose and test an alternative assimilation scheme in which precipitation fluxes are adjusted to achieve the desired changes in simulated TWS. Using a synthetic data assimilation experiment, we show that the scheme improves performance skill in precipitation estimates in general, but that it is more robust for snowfall than for rainfall, and it fails in certain regions with strong horizontal gradients in precipitation. The results demonstrate that assimilation of TWS observations can help correct (adjust) the model's precipitation forcing and, in turn, enhance model estimates of TWS, snow mass, soil moisture, runoff, and evaporation. A key limitation of the approach is the assumption that all errors in TWS originate from errors in precipitation. Nevertheless, the proposed approach produces more consistent improvements in simulated runoff than the established GRACE data assimilation techniques. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149574591&site=ehost-live
453,Estimating uncertainties associated with quasi-global satellite infrared-based retrievals over land.,Viviana Maggioni,Geophysical Research Abstracts,10297006,,2019,21,,1,1,140485959,,Copernicus Gesellschaft mbH,Article,PRECIPITATION variability; HYDROLOGIC cycle; CLIMATE change; FARM management; ORBITS of artificial satellites; ARTIFICIAL satellites; PRECIPITATION (Chemistry); Space Research and Technology; Support activities for crop production; Farm Management Services,,"An accurate characterization of the global hydrologic cycle is essential not only to study and forecast climate variations, but also for extreme event mitigation and agricultural planning. Since precipitation is the major driving force of the hydrological cycle, current and future satellite missions with a focus on precipitation are critical to estimate hydrological variables globally. Error estimates associated with satellite precipitation retrievals are crucial to allow inferences about the reliability of such products in their operational applications. However, evaluating satellite precipitation error characteristics is challenging because of the inherent temporal and spatial variability of precipitation, measurement errors, and sampling uncertainties, especially at fine temporal and spatial resolutions.This study proposes to use a stochastic error model – PUSH (Probability Uncertainty in Satellite Hydrology) – for estimating uncertainties associated with fine resolution satellite precipitation products. The framework is tested on the daily IMERG (Integrated Multi-satellitE Retrievals for GPM) infrared-only (IR) precipitation component using a satellite-based radar product (the Level-3 Dual-frequency Precipitation Radar, 3DPRD) as reference. PUSH decomposes the error into four components and employs different modeling approaches for each case: correct no-precipitation detection; missed precipitation; false alarm; hit bias. PUSH is calibrated globally over land for different climatological regions. The calibrated parameters are validated using an independent period to verify whether they can be applied to estimate uncertainties associated with future IR retrievals without degrading the model performance. The four error components are then investigated as a function of climate region to study their spatial variability. [ABSTRACT FROM AUTHOR] Copyright of Geophysical Research Abstracts is the property of Copernicus Gesellschaft mbH and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140485959&site=ehost-live
454,Investigating the GPM Dual‐frequency Precipitation Radar signatures of low‐level precipitation enhancement.,Viviana Maggioni,Quarterly Journal of the Royal Meteorological Society,359009,,Oct-19,145,724,3161,14,139587645,10.1002/qj.3611,"John Wiley & Sons, Inc.",Article,RADAR meteorology; METEOROLOGICAL precipitation; RADAR; LANDSLIDES; NATURAL disasters; MICROPHYSICS; RAINFALL,collision‐coalescence; DPR; GPM; microphysics; precipitation enhancement; radar,"High‐intensity precipitation represents a threat for several regions of the world because of the related risk of natural disasters (e.g. floods and landslides). This work focuses on low‐level precipitation enhancement that occurs in the cloud warm layer and has been observed in relation to collision‐coalescence (CC) leading to flash floods and extreme rainfall events in tropical and temperate latitudes. Specifically, signatures of precipitation enhancement (referred to as CC‐dominant precipitation) are investigated in the observations from the Global Precipitation Measurement (GPM) core mission Dual‐frequency Precipitation Radar (DPR) over the central/eastern Contiguous United States (CONUS) during June 2014–May 2018. A classification scheme for CC‐dominant precipitation, developed for dual‐polarization S‐band radar measurements and applied in a previous work to X‐band radar observations in complex terrain, is used as a benchmark. The scheme is here applied to the GPM ground validation dataset that matches ground‐based radar observations across CONUS to space‐borne DPR retrievals. The occurrence of CC‐dominant precipitation is documented and the corresponding signatures of CC‐dominant precipitation at Ku‐ and Ka‐band are studied. CC‐dominant profiles show distinguishing features when compared to profiles not dominated by CC, e.g. characteristic vertical slopes of reflectivity at Ku‐ and Ka‐band in the liquid layer, lower freezing‐level height, and shallower ice layer, which are linked to environmental conditions driving the peculiar CC microphysics. This work aims at improving satellite quantitative precipitation estimation, particularly GPM retrievals, by targeting CC development in precipitation columns. [ABSTRACT FROM AUTHOR] Copyright of Quarterly Journal of the Royal Meteorological Society is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139587645&site=ehost-live
455,A Recursive Algorithm for Wideband Temporal Spectrum Sensing.,Brian Mark,IEEE Transactions on Communications,906778,,Jan-18,66,1,26,13,127333278,10.1109/TCOMM.2017.2749578,IEEE,Article,COGNITIVE radio; MARKOV processes; ALGORITHMS; TIME division multiple access; SPECTRAL energy distribution,Cognitive radio; Detectors; Heuristic algorithms; hidden Markov model; Hidden Markov models; OFDM; Signal to noise ratio; spectrum sensing; wideband,"Wideband spectrum sensing techniques determine which portions of a given spectrum band are occupied or idle in the frequency domain. The idle portions represent spectrum holes that can potentially be exploited by secondary or unlicensed users. Existing methods for wideband sensing, however, do not take into account the temporal activity of the primary or licensed users within the spectrum band. We propose an algorithm that identifies primary user activity over a wide spectrum band and provides a statistical characterization of the primary user signals in the band. The algorithm applies hidden Markov modeling to a hierarchically partitioned representation of the spectrum band, together with a recursive tree search. Different from existing wideband sensing algorithms, the proposed wideband temporal sensing method is able to accurately detect spectrum holes even in the presence of bursting primary user signals. Moreover, the hidden Markov modeling of the primary user signals enables the accurate detection and the prediction of primary user activity over time. Numerical results demonstrate the significant performance gain of the proposed algorithm over existing wideband spectrum sensing algorithms, particularly in the presence of low duty-cycle primary user signals. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Communications is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127333278&site=ehost-live
456,Design and Analysis of a Denial-of-Service-Resistant Quality-of-Service Signaling Protocol for MANETs.,Brian Mark,IEEE Transactions on Vehicular Technology,189545,,May-06,55,3,743,9,21158820,10.1109/TVT.2006.873834,IEEE,Article,COMPUTER network protocols; COMPUTER networks; NETWORK performance; QUALITY of service; SIMULATION methods & models; MOBILE communication systems; DIGITAL communications; DATA transmission systems; UNITED States; Computer Systems Design Services; Wireless Telecommunications Carriers (except Satellite),Cross-layer design; denial-of-service (DoS); mobile ad hoc networks (MANETs); quality-of-service (QoS) signaling,"Quality-of-service (QoS) signaling protocols for mobile ad hoc networks (MANETs) are highly vulnerable to attacks. In particular, a class of denial-of-service (DOS) attacks can severely cripple network performance with relatively little effort expended by the attacker. A distributed QoS signaling protocol that is resistant to a class of DoS attacks on signaling is proposed. The signaling protocol provides QoS for real-time traffic and employs mechanisms at the medium access control (MAC) layer, which serve to avoid potential attacks on network resource usage. The key MAC layer mechanisms that provide support for the QoS signaling scheme include sensing of available bandwidth, traffic policing, and rate monitoring, all of which are performed in a distributed manner by the mobile nodes. The proposed signaling scheme achieves a compromise between signaling protocols that require the maintenance of per-flow state and those that are completely stateless. The signaling scheme scales gracefully in terms of the number of nodes and/or traffic flows in the MANET. The authors analyze the security properties of the protocol and present simulation results to demonstrate its resistance to DoS attacks. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Vehicular Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=21158820&site=ehost-live
457,Discrete-Time Level-Crossing Analysis of Soft Handoff Performance in Cellular Networks.,Brian Mark,IEEE Transactions on Information Theory,189448,,Jul-06,52,7,3283,8,23313529,10.1109/TIT.2006.876357,IEEE,Article,DISCRETE-time systems; TELECOMMUNICATION systems; CODE division multiple access; WIRELESS communications; TIME division multiple access; MULTIUSER computer systems; GAUSSIAN processes; RANDOM noise theory; INFORMATION theory; Satellite Telecommunications; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,Cellular networks; code-division multiple access (CDM); level-crossing; macrodiversity; network dimensioning; performance analysis; recursive computation; soft handoff,"In this correspondence, we develop a formal method for solving a class of level-crossing problems in discrete-time with application to the analysis of soft handoff performance in cellular networks such as code-division multiple-access (CDMA) systems. In such networks, proper dimensioning of soft handoff parameters is critical to overcoming propagation impairments and providing a transparent radio access service for multiple user profiles. We obtain exact expressions for the cell assignment and active set update probabilities of a mobile station traveling along an arbitrary straight-line trajectory. We develop recursive algorithms to compute performance measures such as outage probability, macrodiversity gain, and signaling load. The discrete-time level-crossing analysis yields an accurate and efficient computational tool for designing and dimensioning high performance soft handoff algorithms while avoiding the need for approximations or time-consuming computer simulations. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Information Theory is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23313529&site=ehost-live
458,Hermes: A quantitative trust establishment framework for reliable data packet delivery in MANETs.,Brian Mark,Journal of Computer Security,0926227X,,2007,15,1,3,36,23463749,10.3233/JCS-2007-15102,IOS Press,Article,MOBILE computing; COMPUTER network architectures; ROUTING (Computer network management); COMPUTER network management; BAYESIAN analysis; NETWORK routers,Mobile ad hoc networks; routing; trust establishment,"In mobile ad hoc networks (MANETs), a source node must rely on other nodes to forward its packets on multi-hop routes to the destination. Secure and reliable handling of packets by the intermediate nodes is difficult to ensure in an ad hoc environment. We propose a trust establishment scheme for MANETs, which aims to improve the reliability of packet forwarding over multi-hop routes in the presence of potentially malicious nodes. Using a Bayesian framework, each node assigns a “trustworthiness” value to each of its neighbor nodes based on direct observations of packet forwarding behavior. More generally, each node forms an “opinion” about each of the other nodes in the network, based on the set of trustworthiness values computed in the network. The opinion metric can be incorporated into ad hoc routing protocols to achieve reliable packet delivery even when a portion of the network exhibits malicious behavior. We present numerical results, which demonstrate the effectiveness of the proposed trust establishment scheme. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computer Security is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23463749&site=ehost-live
459,Joint Spatial–Temporal Spectrum Sensing for Cognitive Radio Networks.,Brian Mark,IEEE Transactions on Vehicular Technology,189545,,9/1/10,59,7,3480,11,77779178,10.1109/TVT.2010.2050610,IEEE,Article,WIRELESS communications; RADIO (Medium); REMOTE sensing; DETECTORS; ELECTRONIC feedback; Radio Networks; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Cognitive radio; Detectors; dynamic spectrum access; hypothesis testing; Interference; Joints; Noise; Radio transmitters; spectrum sensing,"In a wireless system with opportunistic spectrum sharing, secondary users equipped with cognitive radios attempt to access a radio spectrum that is not being used by the primary licensed users. On a given frequency channel, a secondary user can perform spectrum sensing to determine spatial or temporal opportunities for spectrum reuse. Whereas most prior works address either spatial or temporal sensing in isolation, we propose a joint spatial–temporal spectrum-sensing scheme that exploits information from spatial sensing to improve the performance of temporal sensing. We quantify the performance benefit of the joint spatial–temporal scheme over pure spatial sensing and pure temporal sensing based on counting-rule and linear quadratic (LQ) detectors. Finally, we analyze a multilevel quantization feedback scheme that can improve the performance of temporal sensing based on counting-rule detectors. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Vehicular Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77779178&site=ehost-live
460,Reducing ASE Effect in Coherent Detection by Employing Double-Pass Fiber Preamplifier and Time-Domain Filter.,Brian Mark,IEEE Journal of Quantum Electronics,189197,,Sep/Oct2009,45,10-Sep,1289,8,44676452,10.1109/JQE.2009.2024773,IEEE,Article,ELECTRIC filters; PHOTON emission; PHOTOEMISSION; BANDPASS filters; SIGNAL-to-noise ratio; Other Electronic Component Manufacturing,Coherent detection; detection; double-pass fiber preamplifier; sensitivity; time-domain filter (TDF),"We report on a novel fiber based coherent detection system employing a double-pass fiber preamplifier, a spectral bandpass filter, and a time-domain filter. The time-domain filter, a synchronous time gate, reduces the in-band amplified spontaneous emission (ASE) beat noise, which cannot be achieved by the spectral bandpass filter alone. The double-pass fiber amplifier further reduces the out-band ASE by about 20 dB with a fiber Bragg grating (FBG) at the end of the fiber amplifier. In preliminary experiments with a 100 GHz bandpass filter, no degradation is observed from the optically preamplified coherent detection compared to pure coherent detection. With a 10 ns pulse width, 500 kHz repetition rate, and 10 pW average input power, about 2 dB and 1 dB signal-to-noise ratio (SNR) improvements are achieved when 5% and 50% time gating duty cycles are used, respectively. [ABSTRACT FROM AUTHOR] Copyright of IEEE Journal of Quantum Electronics is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=44676452&site=ehost-live
461,What probability distribution describes search?,Brian Mark,Optical Engineering,913286,,Sep-19,58,9,093103-1,15,138999169,10.1117/1.OE.58.9.093103,SPIE - International Society of Optical Engineering,Article,TARGET acquisition; COMPUTER network performance evaluation; QUEUING theory; SENSOR networks; COMPUTER networks; COMPUTER systems; Computer Systems Design Services; Computer systems design and related services (except video game design and development),detection; probability theory; reconnaissance; search; surveillance; target acquisition,"Typically, search research papers assume that target acquisition is described by an exponential distribution. We investigate when this assumption is valid. It is obvious that two people are more effective than one person at finding a target, but how can that be quantified? The network imaging sensor (NIS) and timedependent search parameter (TDSP) models quantify how much more effective multiple observers are at finding a target than a single individual for a wide variety of scenarios. We reference and summarize evidence supporting the NIS and TDSP models and demonstrate how NIS model results can be expressed in terms of a reduced hyperexponential distribution for scenarios where observer and target are stationary. Target acquisition probabilities are determined by analysis and confirmed by computer simulations and perception experiments. Search by multiple stationary observers looking for a stationary target is described by the hyperexponential distribution. Stationary scenarios with multiple observers are more accurately modeled by hyperexponential rather than exponential distributions. Hyperexponential distributions are an example of phase-type distributions used in queuing and in the performance evaluation of computer networks and systems. The observation that search, queuing, and computer networks share phase-type distributions facilitates cross fertilization between these fields. [ABSTRACT FROM AUTHOR] Copyright of Optical Engineering is the property of SPIE - International Society of Optical Engineering and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138999169&site=ehost-live
462,A Method for Design and Performance Modeling of Client/Server Systems.,Daniel Menascé,IEEE Transactions on Software Engineering,985589,,Nov-00,26,11,1066,20,3842920,,IEEE,Article,"COMPUTER network architectures; DISTRIBUTED computing; COMPUTER software developers; PROGRAMMING languages; DATABASE management; SQL; Data Processing, Hosting, and Related Services; Software publishers (except video game publishers); Software Publishers; Computer systems design and related services (except video game design and development)",,"Designing complex distributed client/server applications that meet performance requirements may prove extremely difficult in practice if software developers are not willing or do not have the time to help software performance analysts. This paper advocates the need to integrate both design and performance modeling activities so that one can help the other. We present a method developed and used by the authors in the design of a fairly large and complex client/server application. The method is based on a software performance engineering language developed by one of the authors. Use cases were developed and mapped to a performance modeling specification using the language. A compiler for the language generates an analytic performance model for the system. Service demand parameters at servers, storage boxes, and networks are derived by the compiler from the system specification. A detailed model of DBMS query optimizers allows the compiler to estimate the number of I/Os and CPU time for SQL statements. The paper concludes with some results of the application that prompted the development of the method and language. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=3842920&site=ehost-live
463,On a Unified Framework for the Evaluation of Distributed Quorum Attainment Protocols.,Daniel Menascé,IEEE Transactions on Software Engineering,985589,,Nov-94,20,11,868,17,14315976,10.1109/32.368122,IEEE,Article,"SOFTWARE engineering; ALGORITHMS; FAULT tolerance (Engineering); RELIABILITY in engineering; ELECTRONIC data processing; COMPUTER software; Computer, computer peripheral and pre-packaged software merchant wholesalers; Software publishers (except video game publishers); Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Data Processing, Hosting, and Related Services",delay analysis; distributed systems; fault-tolerance; majority consensus; Mutual exclusion; performability; performance analysis; primary site protocol; tree-based mutual exclusion protocols,"Quorum attainment protocols are an important part of many mutual exclusion algorithms. Assessing the performance of such protocols in terms of number of messages, as is usually done, may be less significant than being able to compute the delay in attaining the quorum. Some protocols achieve higher reliability at the expense of increased message cost or delay. A unified analytical model which takes into account the network delay and its effect on the time needed to obtain a quorum is presented. A combined performability metric, which takes into account both availability and delay, is defined in this paper, and expressions to calculate its value are derived for two different reliable quorum attainment protocols: Agrawal and El Abbadi's and Majority Consensus algorithms. Expressions for the Primary Site approach are also given as upper bound on performability and lower bound on delay. A parallel version of the Agrawal and El Abbadi protocol is introduced and evaluated. This new algorithm is shown to exhibit lower delay at the expense of a negligible increase in the number of messages exchanged. Numerical results derived from the model are discussed [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14315976&site=ehost-live
464,Student Perceptions of Engineering Entrepreneurship: An Exploratory Study.,Daniel Menascé,Journal of Engineering Education,10694730,,Apr-06,95,2,153,12,20590679,10.1002/j.2168-9830.2006.tb00886.x,Wiley-Blackwell,Article,ACTIVITY programs in education; ENGINEERING students; ENGINEERING education; TECHNOLOGY education; ENGINEERING schools; TECHNICAL institutes; ACTIVE learning,engineering entrepreneurship; pedagogy; student retention,"This study examines students' overall perceptions of the engineering profession in a first-year course in engineering, and the effect of a pedagogical approach aimed at exposing students to engineering entrepreneurship and their perceptions of engineering entrepreneurship. The approach featured a market game that engaged a pilot group of 20 students in forming IT companies and competing for the best design of a travel agent system. The rest of the students in the course completed the traditional class project, which involved designing and building a land sailer. A pre-post Likert-type survey designed to measure students' perceptions of the engineering profession was administered to all students enrolled in this course. In addition, a short answer questionnaire seeking students' pedagogical perceptions of the market game and the land sailer project was administered at the end of the course. Results indicated that students' overall perceptions of the engineering profession significantly improved by the end of the course. More importantly, the results indicated that students who participated in the market game had significantly better perceptions of engineering entrepreneurship, specifically professional skills, than students who participated in the land sailer project. These findings are of considerable interest to engineering schools that want to increase student retention and are looking for novel approaches to assist freshmen in choosing their majors. [ABSTRACT FROM AUTHOR] Copyright of Journal of Engineering Education is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20590679&site=ehost-live
465,An optimal stopping approach to managing travel-time uncertainty for time-sensitive customer pickup.,Elise Miller-Hooks,Transportation Research: Part B,1912615,,Aug-17,102,,22,16,123894813,10.1016/j.trb.2017.04.017,Elsevier B.V.,Article,RIDESHARING services; OPTIMAL stopping (Mathematical statistics); VEHICLE routing problem; TAXICABS; STOCHASTIC models; Taxi Service; All Other Support Activities for Transportation,Dial-a-ride; On-line routing; Optimal stopping; Stochastic travel times,"In dynamic vehicle routing, it is common to respond to real-time information with immediate updates to routes and fleet management. However, even if routes are updated continuously, in practice, some decisions once made are difficult to reverse. At times, it may thus be valuable to wait for additional information before acting on a decision. We use the theory of optimal stopping to determine the optimal timing of a recourse action when vehicles are likely to miss customer deadlines due to travel-time stochasticities and backup services are available. The factors involved in making this decision – that is, the likelihood that the primary vehicle will arrive late, the location of the backup vehicle, and value of waiting for additional travel-time information – each change dynamically over time. We develop a recourse model that accounts for this complexity. We formulate the optimal recourse policy as a stochastic dynamic program. Properties of the optimal policy are derived analytically, and its solution is approximated with a binomial lattice method used in the pricing of American options. Finally, we develop a two-stage stochastic optimization approach to show how the opportunity to take recourse dynamically might be integrated into a priori scheduling and routing. The framework is demonstrated for a stochastic dial-a-ride application in which taxis serve as backup to ridesharing vehicles. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part B is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123894813&site=ehost-live
466,Assessing hospital system resilience to disaster events involving physical damage and Demand Surge.,Elise Miller-Hooks,Socio-Economic Planning Sciences,380121,,Jun-20,70,,N.PAG,1,142890640,10.1016/j.seps.2019.07.005,Elsevier B.V.,Article,HOSPITALS; DISASTER resilience; DISCRETE event simulation; MEDICAL care; EMERGENCY medical services; Municipal police services; Emergency and Other Relief Services; General (except paediatric) hospitals; General Medical and Surgical Hospitals,Collaboration; Disaster preparedness; Discrete event simulation; Healthcare resilience; Hospital operations in MCI; Interhospital coalition; Queueing networks,"This paper investigates the effectiveness of formalized collaboration strategies through which patients can be transferred and resources, including staff, equipment and supplies, can be shared across hospitals in response to a disaster incident involving mass casualties and area-wide damage. Inflicted damage can affect hospital infrastructure and its supporting lifelines, thus impacting capacity and capability or, ultimately, services that are provided. Using a discrete event simulation framework and underlying open queuing network conceptualization involving patient flows through 9 critical units of each hospital, impacts on critical resources, physical spaces and demand are modeled and the hospital system's resilience to these hazard events is evaluated. Findings from numerical experiments on a case study involving multiple hospitals spaced over a large metropolitan region replicating a system similar to the Johns Hopkins Hospital System show the potential of strategies involving not only transfers and resource sharing, but also joint capacity enhancement alternatives to improve post-disaster emergency health care service delivery through joint action. • Investigates potential benefits of hospital coalitions in disaster. • Assess potential value of patient transfers and resource sharing. • Considers joint capacity enhancement alternatives. • Discrete event simulation conceptualization of hospital system. • Quantifies hospital system resilience to pandemic, MCI and disaster events with damage. [ABSTRACT FROM AUTHOR] Copyright of Socio-Economic Planning Sciences is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142890640&site=ehost-live
467,Assessing strategies for protecting transportation infrastructure from an uncertain climate future.,Elise Miller-Hooks,Transportation Research Part A: Policy & Practice,9658564,,Nov-17,105,,27,15,125235850,10.1016/j.tra.2017.08.010,Elsevier B.V.,Article,FINANCING of transportation; INFRASTRUCTURE (Economics); CLIMATE change; METROPOLITAN areas; INVESTMENTS; Miscellaneous Financial Investment Activities; Investment Advice; All Other Support Activities for Transportation; Other support activities for transportation,Climate change resilience; Critical infrastructure; Expected value of perfect information; Flooding; Protective infrastructure investment planning; Transportation protection; Value of stochastic solution,"This paper investigates the importance of explicitly considering the stochastic nature of future climate impact predictions and predictive accuracy for optimal investment planning in the protection of coastal and inland transportation infrastructure against climate impacts. Such impacts include sea level rise, coastal and riverine flooding resulting from more frequent and intense precipitation events, storms, storm surges and other extreme events. For this purpose, numerical experiments utilizing stochastic optimization based methodologies were conducted on a case study of the Washington, D.C. Greater Metropolitan area proximate to the Potomac River under varying climatic predictions. Results from the numerical experiments suggest a 54% reduction in added costs due to the implementation of chosen protective infrastructure investments. They also indicate a reduction in added costs (capital investment and added delays) on the order of 19% when the investments are chosen to hedge against probable future flooding events as compared with planning for the 50th percentile SLR prediction with associated weather events. A potential gain of nearly 27% in reduced costs through improved predictive accuracy in climatic forecasts is also noted, suggesting significant value in more accurate forecasts. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research Part A: Policy & Practice is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125235850&site=ehost-live
468,Constructs in infrastructure resilience framing – from components to community services and the built and human infrastructures on which they rely.,Elise Miller-Hooks,IISE Transactions,24725854,,Apr-22,,,1,14,156476409,10.1080/24725854.2022.2070801,Taylor & Francis Ltd,Article,,built environment; community services; critical lifelines; human infrastructure; Resilience; socio-technical system,"Abstract This article describes five constructs for framing infrastructure resilience estimation. These constructs range from the consideration of a single component to a community service provided through a set of buildings whose functionality relies on interdependent supporting lifelines. A key aim is to explore how the construct that is adopted affects resilience understanding. It discusses the value of reframing the resilience computation around services that are provided by built environments rather than around the built systems themselves. The built environment would provide little in the way of services if not for human involvement and other needed resources. A construct for framing resilience is expanded to incorporate the role of humans as infrastructure, as well as permanent and consumable limiting resources, in creating service capacity. Taking a service-based viewpoint induces a change in perspective with rippling impact. It affects the choice of metrics for measuring resilience, adaptation strategies to include in assessment, baselines for comparison, and elements of the built environment to incorporate in the evaluation. It necessitates consideration of socio-technical concerns. It also brings hidden issues of inequity to the foreground. This article suggests that underlying many resilience studies is an implicit construct for framing resilience, and explores how the construct affects and enables resilience understanding. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156476409&site=ehost-live
469,"Co-opetition in enhancing global port network resiliency: A multi-leader, common-follower game theoretic approach.",Elise Miller-Hooks,Transportation Research: Part B,1912615,,Feb-18,108,,281,18,127441948,10.1016/j.trb.2018.01.004,Elsevier B.V.,Article,"COOPETITION; GAME theory; DECISION making; MARITIME shipping; ECONOMIC demand; MARKET share; Marine shipping agencies; Other Support Activities for Water Transportation; Deep sea, coastal and Great Lakes water transportation (except by ferries); Navigational Services to Shipping",Co-opetition; Complementarity optimization; Freight transportation protection; Game theory; Multi-leader common-follower; Port resiliency; Protective infrastructure investment planning,"Ports are key elements of global supply chains, providing connection between land- and maritime-based transportation modes. They operate in cooperative, but competitive, co-opetitive , environments wherein individual port throughput is linked through an underlying transshipment network. Short-term port performance and long-term market share can be significantly impacted by a disaster event; thus, ports plan to invest in capacity expansion and protective measures to increase their reliability or resiliency in times of disruption. To account for the co-opetition among ports, a bi-level multiplayer game theoretic approach is used, wherein each individual port takes protective investment decisions while anticipating the response of the common market-clearing shipping assignment problem in the impacted network. This lower-level assignment is modeled as a cost minimization problem, which allows for consideration of gains and losses from other ports decisions through changes in port and service capacities and port cargo handling times. Linear properties of the lower-level formulation permit reformulation of the individual port bi-level optimization problems as single-level problems by replacing the common lower-level by its equivalent Karush Kuhn Tucker (KKT) conditions. Simultaneous consideration of individual port optimization problems creates a multi-leader, common-follower problem, i.e. an unrestricted game, that is modeled as an Equilibrium Problem with Equilibrium Constraints (EPEC). Equilibria solutions are sought by use of a diagonalization technique. Solutions of unrestricted, semi-restricted and restricted games are analyzed and compared for a hypothetical application from the literature involving ports in East Asia and Europe. The proposed co-opetitive approach was found to lead to increased served total demand, significantly increased market share for many ports and improved services for shippers. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part B is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127441948&site=ehost-live
470,Enhancing resilience through port coalitions in maritime freight networks.,Elise Miller-Hooks,Transportation Research Part A: Policy & Practice,9658564,,Mar-22,157,,1,23,155459287,10.1016/j.tra.2022.01.015,Elsevier B.V.,Article,"HARBORS; COALITIONS; DECISION making in investments; FREIGHT & freightage; PORT districts; INDUSTRIAL capacity; SHIPPING rates; Specialized Freight (except Used Goods) Trucking, Local; Other freight transportation arrangement; Freight Transportation Arrangement; Port and Harbor Operations; Other Heavy and Civil Engineering Construction; Regulation and Administration of Transportation Programs",EPEC; Global supply chains; Maritime systems; Port networks; Resilience; Shipping,"• Coalition strategies with capacity sharing/cross-port investment among ports are tested. • Provide equilibrium problem with equilibrium constraints conceptualization of problem. • Give evidence that coalitions can improve system resilience and demand fulfillment rates. • Found positive returns on investment for individual and network of ports via coalition. • Support policies that increase market share through increased reliability of service routes. Reliable port services are key to maritime freight transport system performance. These systems are vulnerable to disasters of anthropogenic or natural cause, which can significantly impact port capacity, handling times and overall system performance. To improve resilience of individual ports, strategies involving capacity sharing and protective cross-port investments through coalition formation are proposed. This collaborative port protection and investment approach to improve individual and system-level port resilience is formulated as an Equilibrium Problem with Equilibrium Constraints. That is, the program is bi-level with multiple players in the upper level and a common liner shipping problem in the lower level. Its solution is obtained at a Nash equilibrium wherein no port stakeholder can achieve better performance by unilaterally changing its investment plan. A Stackelberg equilibrium between upper and lower levels infers that best investment decisions are made given competition between ports and the market's response to improvements. The benefits of regional coalitions in this co-opetitive (competitive and collaborative) environment in terms of port and system resilience, port- and system-level demand fulfilment rates and return on investment are investigated from multiple perspectives, including the perspectives of shippers, port owners and the larger shipping network. With insights gained through study of the proposed coalition policies, this work aims to facilitate port authorities in making decisions on port capacity expansion, infrastructure investment and forming strategic partnerships. Shipping companies may also take into consideration the ability of a port to provide service under disruption events when choosing which ports to include in their service loops. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research Part A: Policy & Practice is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155459287&site=ehost-live
471,Maritime port network resiliency and reliability through co-opetition.,Elise Miller-Hooks,Transportation Research: Part E,13665545,,May-20,137,,N.PAG,1,142890650,10.1016/j.tre.2020.101916,Elsevier B.V.,Article,"COOPETITION; HARBOR management; FREIGHT & freightage; HARBORS; TRADE routes; INTERMODAL freight terminals; RELIABILITY in engineering; Other freight transportation arrangement; Freight Transportation Arrangement; Specialized Freight (except Used Goods) Trucking, Local; Other Heavy and Civil Engineering Construction; Port and Harbor Operations",Co-opetitive games; Maritime network; Ports; Protective infrastructure investment; Reliability; Resiliency; Stackelberg game; Stochastic optimization,"• Models for assessing and improving resiliency and reliability of port networks. • Stochastic co-opetitive formulations as stochastic Nash and Stackelberg games. • Accounting for market interactions, multiple hazards, investments, co-opetition. • Risk-averse (worst-case) and risk-neutral (expected value) strategies. • Stochastic, centralized methods as benchmark. Local and global economies are for many nations highly dependent on the import and export of goods. These goods are shipped through global intermodal (IM) freight land-water transportation systems that rely on truck, rail and maritime networks and their IM terminals. These terminals are crucial to creating and maintaining efficient international trade routes. This paper considers port reliability and resilience, as well as the role of ports in supporting a larger resilient maritime system. Specifically, stochastic, bi-level, game theoretic optimization models for assessing and improving the resiliency and reliability of the global port network are presented. Proposed models are devised for a set of independent ports with interacting investment problems for competitive, but potentially cooperative (co-opetitive) environments. Uncertainties in traversal times and port throughput capacities are accounted for by adopting a stochastic optimization method using expected or max-min functions to simultaneously hedge against the consequences of multiple possible future port-related disaster events. Alternative centralized, but stochastic formulations are also provided. This stochastic, co-opetitive methodology and alternative centralized methods fill an important gap in the maritime resiliency literature. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part E is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142890650&site=ehost-live
472,Optimal time-differentiated pricing for a competitive mixed traditional and crowdsourced event parking market.,Elise Miller-Hooks,Transportation Research: Part C,0968090X,,Nov-21,132,,N.PAG,1,153528935,10.1016/j.trc.2021.103409,Elsevier B.V.,Article,PARKING garages; PARKING facilities; PROXIMITY spaces; SET functions; FUNCTION spaces; Parking Lots and Garages,Crowdsourcing; Emerging markets; Equilibrium; Event management; Multi-player game; Pricing,"• Mathematically conceptualizes a crowdsourced parking market mechanism. • Exploits a multi-period, stochastic EPEC formulation. • Reservation-time based pricing is enabled through price differentiation for late comers. • Uses a diagonalization method with embedded gradient ascent approach for solution. • Supply- and demand-side uncertainties are explicitly modeled. An event-based parking pricing problem, the Crowdsourced Event Parking Market Pricing Problem, is proposed wherein parking lot owners and others who are willing to rent out privately owned spaces compete to attract drivers who are looking for available parking spaces. Each parking location owner's problem is modeled as a bi-level program, where the upper-level parking garages, individuals and consolidators (the players) compete for customers, setting their prices to maximize revenue given the response of the lower-level followers. The lower-level followers choose their parking locations based on the utilities they derive from the spaces, which is a function of the proximity of the spaces to their destinations, parking fees and crowdedness. Prices are set as a function of the time of reservation. Reservation-time based pricing enables price differentiation for late comers either in the form of lower prices to attract last-minute customers when excess spaces are anticipated or higher-pricing if few spaces are expected to remain empty. A multi-period, stochastic Equilibrium Problem with Equilibrium Constraints (EPEC) formulation of the Crowdsourced Event Parking Market Pricing Problem is presented, and a diagonalization method with embedded gradient ascent approach for solution of individual player Mathematical Programs with Equilibrium Constraints (MPECs) is proposed for its solution. Both supply- and demand-side uncertainties are explicitly modeled. Solutions provide competitive parking prices set by reservation time for each parking facility, whether the facility involves a large parking garage or single parking space owner. Numerical experiments were conducted to illustrate the proposed concepts and assess the potential impact of crowdsourced parking spaces on the parking market. The results show that social welfare increases by more than 5% when crowdsourced parking locations account for 7% of the parking market. Results from additional numerical experiments show that ignoring stochasticity results in revenue loss for all parking owners. The developed techniques aim to facilitate existing and new parking facility owners to participate in crowdsourced event-parking markets. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153528935&site=ehost-live
473,Optimal transportation and shoreline infrastructure investment planning under a stochastic climate future.,Elise Miller-Hooks,Transportation Research: Part B,1912615,,Jun-17,100,,156,19,123043555,10.1016/j.trb.2016.12.023,Elsevier B.V.,Article,INFRASTRUCTURE (Economics); INVESTMENT policy; STOCHASTIC processes; INTEGER programming; GENETIC algorithms,Climate change; Multi-stage stochastic programming; Multi-temporal and multi-scale; Noisy genetic algorithms; Resilience; Sea level rise,"This paper studies the problem of optimal long-term transportation investment planning to protect from and mitigate impacts of climate change on roadway performance. The problem of choosing the extent, specific system components, and timing of these investments over a long time horizon (e.g., 40–60 years) is modeled as a multi-stage, stochastic, bi-level, mixed-integer program wherein cost-effective investment decisions are taken in the upper level. The effects of possible episodic precipitation events on experienced travel delays are estimated from solution of a lower-level, traffic equilibrium problem. The episodic events and longer-term sea level changes exist on different time scales, making their integration a crucial element in model development. The optimal investment strategy is obtained at a Stackelberg equilibrium that is reached upon solution to the bilevel program. A recursive noisy genetic algorithm (rNGA), designed to address large-scale applications, is proposed for this purpose. The rNGA seeks the optimal combination of investment decisions to take now given only probabilistic information on the predicted sea level rise trend for a long planning horizon and associated likely extreme climatic events (in terms of their frequencies and intensities) that might arise over that planning period. The proposed solution method enables the evaluation of decisions concerning where, when and to what level to make infrastructure investments. The proposed rNGA has broad applicability to more general multi-stage, stochastic, bilevel, nonconvex, mixed integer programs that arise in many applications. The proposed solution methodology is demonstrated on an example representing a portion of the Washington, D.C. Greater Metropolitan area adjacent to the Potomac River. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part B is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123043555&site=ehost-live
474,Quantifying the resilience of an urban traffic-electric power coupled system.,Elise Miller-Hooks,Reliability Engineering & System Safety,9518320,,Jul-17,163,,79,16,121938725,10.1016/j.ress.2017.01.026,Elsevier B.V.,Article,CITY traffic; TRANSPORTATION; ELECTRIC power; STOCHASTIC programming; RISK assessment; All Other Support Activities for Transportation; Other support activities for transportation,Critical infrastructure interdependencies; Failure scenarios; Risk management; Traffic-electric coupled system; Transportation resilience quantification; Uncertainty analysis,"Transportation system resilience has been the subject of several recent studies. To assess the resilience of a transportation network, however, it is essential to model its interactions with and reliance on other lifelines. Prior works might consider these interactions implicitly, perhaps in the form of hazard impact scenarios wherein services from a second lifeline (e.g. power) are precluded due to a hazard event. In this paper, a bi-level, mixed-integer, stochastic program is presented for quantifying the resilience of a coupled traffic-power network under a host of potential natural or anthropogenic hazard-impact scenarios. A two-layer network representation is employed that includes details of both systems. Interdependencies between the urban traffic and electric power distribution systems are captured through linking variables and logical constraints. The modeling approach was applied on a case study developed on a portion of the signalized traffic-power distribution system in southern Minneapolis. The results of the case study show the importance of explicitly considering interdependencies between critical infrastructures in transportation resilience estimation. The results also provide insights on lifeline performance from an alternate power perspective. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121938725&site=ehost-live
475,Transit system resilience: Quantifying the impacts of disruptions on diverse populations.,Elise Miller-Hooks,Reliability Engineering & System Safety,9518320,,Nov-19,191,,N.PAG,1,138833730,10.1016/j.ress.2019.106561,Elsevier B.V.,Article,FAULT trees (Reliability engineering); SOCIOTECHNICAL systems; SOCIAL systems; PUBLIC transit; POPULATION; TELECOMMUNICATION systems; LOCAL transit access; Other Heavy and Civil Engineering Construction; Urban transit systems; Mixed Mode Transit Systems; Bus and Other Motor Vehicle Transit Systems; Other Urban Transit Systems; Satellite Telecommunications,BDDs; Coupled systems; Fault tree; Interdependent lifelines; Reliability; Resilience; Transit,"• Conceptualizes transit network resilience in terms of a socio-technical system. • Explicitly considers diverse populations of passengers in resilience measurement. • Develops efficient data structure for evaluating system performance. • Makes explicit mechanisms of interdependency arising from social systems. • Assesses component criticality and revisits methods of comparison. The resilience of a community to disruptions in a public transportation system depends not only on the technical system's ability to maintain service levels, but also on the ability of individuals to cope with and adapt to disruptions. This paper proposes a reliability-based methodology for conceptualizing and evaluating resilience of a socio-technical system in the context of diverse populations of passengers who experience the system differently, but share its resources. Considering a transit network as a socio-technical system induces coupling with other technical systems, such as communication networks and other transportation modes, which support user adaptability in disruption. Extending concepts from fault trees and binary decision diagrams (BDDs), a multi-valued dependency graph framework is developed and used to quantify the resilience of the system. Various notions of resilience are explored. Techniques for identifying which mechanisms, whether technically or socially driven, are critical to the resilience of each population are discussed. Sensitivity to various factors through the study of improvement potential provides practical insight into policies and technical improvements that can increase system or user resilience. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138833730&site=ehost-live
476,Chemokinesis-driven accumulation of active colloids in low-mobility regions of fuel gradients.,Jeffrey Moran,Scientific Reports,20452322,,2/26/21,11,1,1,11,149026595,10.1038/s41598-021-83963-x,Springer Nature,Article,COLLOIDS; IMMUNE response; CHEMOKINES; HYDROGEN peroxide; CHEMOTAXIS; Other Basic Inorganic Chemical Manufacturing; All other basic inorganic chemical manufacturing,,"Many motile cells exhibit migratory behaviors, such as chemotaxis (motion up or down a chemical gradient) or chemokinesis (dependence of speed on chemical concentration), which enable them to carry out vital functions including immune response, egg fertilization, and predator evasion. These have inspired researchers to develop self-propelled colloidal analogues to biological microswimmers, known as active colloids, that perform similar feats. Here, we study the behavior of half-platinum half-gold (Pt/Au) self-propelled rods in antiparallel gradients of hydrogen peroxide fuel and salt, which tend to increase and decrease the rods' speed, respectively. Brownian Dynamics simulations, a Fokker–Planck theoretical model, and experiments demonstrate that, at steady state, the rods accumulate in low-speed (salt-rich, peroxide-poor) regions not because of chemotaxis, but because of chemokinesis. Chemokinesis is distinct from chemotaxis in that no directional sensing or reorientation capabilities are required. The agreement between simulations, model, and experiments bolsters the role of chemokinesis in this system. This work suggests a novel strategy of exploiting chemokinesis to effect accumulation of motile colloids in desired areas. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149026595&site=ehost-live
477,Electric double layer overlap limits flow rate in Janus electrocatalytic self-pumping membranes.,Jeffrey Moran,Electrochimica Acta,134686,,Sep-22,426,,N.PAG,1,158403019,10.1016/j.electacta.2022.140762,Elsevier B.V.,Article,ELECTRIC double layer; ELECTRO-osmosis; ELECTRIC fields; FLUID flow; ION transport (Biology); HYDROGEN ions,Electric double layer; Electrocatalysis; Electrokinetics; Membrane; Self-pumping,"• Self-pumping flow is an electroosmotic flow driven by self-generated electric field. • The self-generated electric field arises from reaction-induced charge polarization. • Self-pumping fluid speed can be increased 20-fold by avoiding EDL overlap. • As EDLs separate, electromigration more strongly influences ion transport. Nanoporous membranes with platinum (Pt) and gold (Au) coated on opposite faces can autonomously pump fluid in the presence of hydrogen peroxide, but the physics is not fully understood. Here, we show with simulation results that the self-pumping flow rate can be considerably increased by avoiding the overlap of electric double layers (EDL) inside pores. Due to catalytic electrochemical reactions on Pt and Au, hydrogen ions (H+) are generated and depleted on opposite sides of the membrane, establishing a self-generated electric field and associated electro-osmotic flow through the pores. By optimizing the pore radius, EDL overlap is avoided and an area-averaged self-pumping flow speed of 23 µm/s can be achieved, which is 20 times higher than previously reported. By conducting the first-ever physico-chemical computational model of self-pumping membranes, this work reveals the mechanism of self-pumping flow in porous two-sided ""Janus"" membranes and highlights the potential of developing biomimetic membranes and lab-on-chip devices that can precisely and remotely control fluid flow in pores or channels in an ""on/off"" manner. [Display omitted] [ABSTRACT FROM AUTHOR] Copyright of Electrochimica Acta is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158403019&site=ehost-live
478,Generic Rules for Distinguishing Autophoretic Colloidal Motors.,Jeffrey Moran,Angewandte Chemie,448249,,3/14/22,134,12,1,11,155658521,10.1002/ange.202116041,"John Wiley & Sons, Inc.",Article,IONIC solutions; ION sources; MICROMOTORS; POPULATION density; IONIC strength,Ionic Strength; Mechanism; Micro/nanomotor; Self-Diffusiophoresis; Self-Electrophoresis,"Distinguishing the operating mechanisms of nano‐ and micromotors powered by chemical gradients, i.e. ""autophoresis"", holds the key for fundamental and applied reasons. In this article, we propose and experimentally confirm that the speeds of a self‐diffusiophoretic colloidal motor scale inversely to its population density but not for self‐electrophoretic motors, because the former is an ion source and thus increases the solution ionic strength over time while the latter does not. They also form clusters in visually distinguishable and quantifiable ways. This pair of rules is simple, powerful, and insensitive to the specific material composition, shape or size of a colloidal motor, and does not require any measurement beyond typical microscopy. These rules are not only useful in clarifying the operating mechanisms of typical autophoretic micromotors, but also in predicting the dynamics of unconventional ones that are yet to be experimentally realized, even those involving enzymes. [ABSTRACT FROM AUTHOR] Copyright of Angewandte Chemie is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155658521&site=ehost-live
479,Generic Rules for Distinguishing Autophoretic Colloidal Motors.,Jeffrey Moran,Angewandte Chemie International Edition,14337851,,3/14/22,61,12,1,11,155662893,10.1002/anie.202116041,"John Wiley & Sons, Inc.",Article,IONIC solutions; ION sources; MICROMOTORS; POPULATION density; IONIC strength,Ionic Strength; Mechanism; Micro/nanomotor; Self-Diffusiophoresis; Self-Electrophoresis,"Distinguishing the operating mechanisms of nano‐ and micromotors powered by chemical gradients, i.e. ""autophoresis"", holds the key for fundamental and applied reasons. In this article, we propose and experimentally confirm that the speeds of a self‐diffusiophoretic colloidal motor scale inversely to its population density but not for self‐electrophoretic motors, because the former is an ion source and thus increases the solution ionic strength over time while the latter does not. They also form clusters in visually distinguishable and quantifiable ways. This pair of rules is simple, powerful, and insensitive to the specific material composition, shape or size of a colloidal motor, and does not require any measurement beyond typical microscopy. These rules are not only useful in clarifying the operating mechanisms of typical autophoretic micromotors, but also in predicting the dynamics of unconventional ones that are yet to be experimentally realized, even those involving enzymes. [ABSTRACT FROM AUTHOR] Copyright of Angewandte Chemie International Edition is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155662893&site=ehost-live
480,Numerical study of the effect of soft layer properties on bacterial electroporation.,Jeffrey Moran,Bioelectrochemistry,15675394,,Oct-18,123,,261,12,130721351,10.1016/j.bioelechem.2017.09.004,Elsevier B.V.,Article,ELECTROPORATION; GRAM-positive bacteria; POLYELECTROLYTES; CELL membranes; ION transport (Biology); BACTERIA,Finite-element method; Gram-positive bacteria; Microorganisms; Pore radius dynamics; Soft particle electrokinetics,"We present a numerical model of electroporation in a gram-positive bacterium, which accounts for the presence of a negatively charged soft polyelectrolyte layer (which may include a periplasmic space, peptidoglycan layer, cilia, flagella, and other surface appendages) surrounding its plasma membrane. We model the ion transport within and outside the soft layer using the soft layer electrokinetics-based Poisson-Nernst-Planck formalism. Additionally, we model the electroporation dynamics on the plasma membrane using the pore nucleation-based electroporation formalism developed by Krassowska and Filev. We find that ion transport within the soft layer (surface conduction), which depends on the relative importance of the soft layer charged group concentration compared to the buffer concentration, significantly alters the transmembrane voltage across the plasma membrane and hence the pore characteristics. Our numerical simulations suggest that surface conduction significantly lowers the pore number in the plasma membrane. This observation is consistent with experimental studies that show that gram-positive bacteria, in general, have lower transformation efficiencies compared to gram-negative bacteria. Our studies highlight a strong dependence of bacterial electroporation on cell envelope properties and buffer conditions, which need to be taken into consideration when designing electroporation protocols. [ABSTRACT FROM AUTHOR] Copyright of Bioelectrochemistry is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=130721351&site=ehost-live
481,Thermal transport dynamics in active heat transfer fluids (AHTF).,Jeffrey Moran,Journal of Applied Physics,218979,,5/7/21,129,17,1,12,150233104,10.1063/5.0047283,American Institute of Physics,Article,HEAT transfer fluids; BIOLOGICAL transport; NANOFLUIDS; ROTATIONAL motion; BROWNIAN motion; THERMAL conductivity; NANOFLUIDICS; WIENER processes,,"We present results of molecular dynamics calculations of the effective thermal conductivity of nanofluids containing self-propelled nanoparticles. The translational and rotational dynamics observed in the simulations follow the behavior expected from the standard theoretical analysis of Brownian and self-propelled nanoparticles. The superposition of self-propulsion and rotational Brownian motion causes the behavior of the self-propelled nanoparticles to resemble Brownian diffusion with an effective diffusivity that is larger than the standard Brownian value by a factor of several thousand. As a result of the enhanced diffusion (and the convective mixing resulting from the motion), we observe a discriminable increase of the effective thermal conductivity of the solution containing self-propelled nanoparticles. While the increases we observe are in the range of several percent, they are significant considering that, without propulsion, the nanofluid thermal conductivity is essentially not affected by the Brownian motion and can be understood within the effective medium theory of thermal conduction. Our results constitute a proof of concept that self-propelled particles have the potential to enhance thermal conductivity of the liquid in which they are immersed, an idea that could ultimately be implemented in a broad variety of cooling applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of Applied Physics is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150233104&site=ehost-live
482,"Methanol, ethanol and hydrogen sensing using metal oxide and metal (TiO2Pt) composite nanoclusters on GaN nanowires: a new route towards tailoring the selectivity of nanowire/nanocluster chemical sensors.",Rao V. Mulpuri,Nanotechnology,9574484,,5/4/12,23,17,1,1,98022211,10.1088/0957-4484/23/17/175501,IOP Publishing,Article,CHEMICAL detectors; SEMICONDUCTOR nanowires; TITANIUM dioxide; GALLIUM nitride; SPUTTERING (Physics); BENZENE compounds; ULTRAVIOLET radiation; HEAT of adsorption; Other Basic Inorganic Chemical Manufacturing; All other basic inorganic chemical manufacturing,,"We demonstrate a new method for tailoring the selectivity of chemical sensors using semiconductor nanowires (NWs) decorated with metal and metal oxide multicomponent nanoclusters (NCs). Here we present the change of selectivity of titanium dioxide (TiO2) nanocluster-coated gallium nitride (GaN) nanowire sensor devices on the addition of platinum (Pt) nanoclusters. The hybrid sensor devices were developed by fabricating two-terminal devices using individual GaN NWs followed by the deposition of TiO2 and/or Pt nanoclusters (NCs) using the sputtering technique. This paper present the sensing characteristics of GaN/(TiO2Pt) nanowirenanocluster (NWNC) hybrids and GaN/(Pt) NWNC hybrids, and compare their selectivity with that of the previously reported GaN/TiO2 sensors. The GaN/TiO2 NWNC hybrids showed remarkable selectivity to benzene and related aromatic compounds, with no measurable response for other analytes. Addition of Pt NCs to GaN/TiO2 sensors dramatically altered their sensing behavior, making them sensitive only to methanol, ethanol and hydrogen, but not to any other chemicals we tested. The GaN/(TiO2Pt) hybrids were able to detect ethanol and methanol concentrations as low as 100 nmol mol_1 (ppb) in air in approximately 100 s, and hydrogen concentrations from 1 mol mol_1 (ppm) to 1% in nitrogen in less than 60 s. However, GaN/Pt NWNC hybrids showed limited sensitivity only towards hydrogen and not towards any alcohols. All these hybrid sensors worked at room temperature and are photomodulated, i.e. they responded to analytes only in the presence of ultraviolet (UV) light. We propose a qualitative explanation based on the heat of adsorption, ionization energy and solvent polarity to explain the observed selectivity of the different hybrids. These results are significant from the standpoint of applications requiring room-temperature hydrogen sensing and sensitive alcohol monitoring. These results demonstrate the tremendous potential for tailoring the selectivity of the hybrid nanosensors for a multitude of environmental and industrial sensing applications. [ABSTRACT FROM AUTHOR] Copyright of Nanotechnology is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98022211&site=ehost-live
483,A class of distributed event-triggered average consensus algorithms for multi-agent systems.,Cameron Nowzari,International Journal of Control,207179,,Feb-22,95,2,502,14,154955760,10.1080/00207179.2020.1799244,Taylor & Francis Ltd,Article,DISTRIBUTED algorithms; MULTIAGENT systems; WIRELESS sensor networks; LYAPUNOV functions; ALGORITHMS; CONSENSUS (Social sciences),clock synchronisation; distributed coordination; Event-triggered control; multi-agent consensus; varying performance needs,"This paper proposes a class of distributed event-triggered algorithms that solve the average consensus problem in multi-agent systems. By designing events such that a specifically chosen Lyapunov function is monotonically decreasing, event-triggered algorithms succeed in reducing communications among agents while still ensuring that the entire system converges to the desired state. However, depending on the chosen Lyapunov function the transient behaviours can be very different. Moreover, performance requirements also vary from application to application. Consequently, we are instead interested in considering a class of Lyapunov functions such that each Lyapunov function produces a different event-triggered coordination algorithm to solve the multi-agent average consensus problem. The proposed class of algorithms all guarantee exponential convergence of the resulting system and exclusion of Zeno behaviours. This allows us to easily implement different algorithms that all guarantee correctness to meet varying performance needs. We show that our findings can be applied to the practical clock synchronisation problem in wireless sensor networks (WSNs) and further corroborate their effectiveness with simulation results. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Control is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154955760&site=ehost-live
484,Event-triggered communication and control of networked systems for multi-agent consensus.,Cameron Nowzari,Automatica,51098,,Jul-19,105,,1,27,137030433,10.1016/j.automatica.2019.03.009,Elsevier B.V.,Article,MULTIAGENT systems; CONSENSUS (Social sciences); TOPOLOGY,Distributed coordination; Event-triggered control; Multi-agent consensus; Networked systems,"This article provides an introduction to event-triggered coordination for multi-agent average consensus. We provide a comprehensive account of the motivations behind the use of event-triggered strategies for consensus, the methods for algorithm synthesis, the technical challenges involved in establishing desirable properties of the resulting implementations, and their applications in distributed control. We pay special attention to the assumptions on the capabilities of the network agents and the resulting features of the algorithm execution, including the interconnection topology, the evaluation of triggers, and the role of imperfect information. The issues raised in our discussion transcend the specific consensus problem and are indeed characteristic of cooperative algorithms for networked systems that solve other coordination tasks. As our discussion progresses, we make these connections clear, highlighting general challenges and tools to address them widespread in the event-triggered control of networked systems. [ABSTRACT FROM AUTHOR] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137030433&site=ehost-live
485,Robust Economic Model Predictive Control of Continuous-Time Epidemic Processes.,Cameron Nowzari,IEEE Transactions on Automatic Control,189286,,Mar-20,65,3,1116,16,143314014,10.1109/TAC.2019.2919136,IEEE,Article,ECONOMIC models; PREDICTION models; STOCHASTIC systems; STOCHASTIC processes; STOCHASTIC convergence; PREDICTIVE control systems,Convergence; Diseases; Epidemic processes; model predictive control; Modeling; networked systems; Nickel; Predictive control; Stochastic processes,"In this paper, we develop a robust economic model predictive controller for the containment of stochastic susceptible-exposed-infected-vigilant (SEIV) epidemic processes, which drives the process to extinction quickly, while minimizing the rate at which control resources are used. The study we present here is significant in that it addresses the problem of efficiently controlling general stochastic epidemic systems without relying on mean-field approximation, which is an important issue in the theory of stochastic epidemic processes. This enables us to provide rigorous convergence guarantees on the stochastic epidemic model itself, improving over the mean-field type convergence results of most prior work. There are two primary technical difficulties addressed in treating this problem: 1) constructing a means of tractably approximating the evolution of the process so that the designed approximation is robust to the modeling error introduced by the applied moment closure and 2) guaranteeing that the designed controller causes the closed-loop system to drive the SEIV process to extinction quickly. As an application, we use the developed framework for optimizing the use of quarantines in containing an SEIV epidemic outbreak. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143314014&site=ehost-live
486,The impact of catastrophic collisions and collision avoidance on a swarming behavior.,Cameron Nowzari,Robotics & Autonomous Systems,9218890,,Jun-21,140,,N.PAG,1,149887105,10.1016/j.robot.2021.103754,Elsevier B.V.,Article,OBSTACLE avoidance (Robotics); IMPACT (Mechanics); GOAL (Psychology),,"Swarms of autonomous agents are useful in many applications due to their ability to accomplish tasks in a decentralized manner, making them more robust to failures. Due to the difficulty in running experiments with large numbers of hardware agents, researchers often make simplifying assumptions and remove constraints that might be present in a real swarm deployment. While simplifying away some constraints is tolerable, we feel that two in particular have been overlooked: one, that agents in a swarm take up physical space, and two, that agents might be damaged in collisions. Many existing works assume agents have negligible size or pass through each other with no added penalty. It seems possible to ignore these constraints using collision avoidance, but we show using an illustrative example that this is easier said than done. In particular, we show that collision avoidance can interfere with the intended swarming behavior and significant parameter tuning is necessary to ensure the behavior emerges as best as possible while collisions are avoided. We compare four different collision avoidance algorithms, two of which we consider to be the best decentralized collision avoidance algorithms available. Despite putting significant effort into tuning each algorithm to perform at its best, we believe our results show that further research is necessary to develop swarming behaviors that can achieve their goal while avoiding collisions with agents of non-negligible volume. • A scalar metric for assessing the quality of a rotating ring formation. • Simulation of destructive collisions with 'respawning' to mimic real platforms. • Investigation of how 4 collision avoidance algorithms interfere with a swarm behavior. • Extensive tuning of each collision avoidance algorithm for fair comparisons. • Heat maps of the parameter space to illustrate the overall tuning difficulty. [ABSTRACT FROM AUTHOR] Copyright of Robotics & Autonomous Systems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149887105&site=ehost-live
487,"An experimental comparison of edge, edge-pair, and prime path criteria.",Jeff Offutt,Science of Computer Programming,1676423,,Jan-18,152,,99,17,126478868,10.1016/j.scico.2017.10.003,Elsevier B.V.,Article,"MUTATION testing of computer software; ASSIMILATION theory (Cognitive learning theory); GRAPH theory; QUANTITATIVE research; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers",Edge pair coverage; Mutation testing; Prime path coverage; Structural testing,"Background Many criteria have been proposed to generate test inputs. Criteria are usually compared in terms of subsumption: if a criterion C1 subsumes C2, it is guaranteed that every test set that satisfies C1 will also satisfy C2. An implication of this notion of subsumption is that C1-adequate tests tend to find more faults than C2-adequate tests, but C1-adequate tests tend to be larger. Thus, while useful, the idea of subsumption does not elaborate on some practical properties of expensive criteria as, for instance, how many more faults a C1-adequate test set will find? More generally, what is the return on investment for choosing more expensive criteria? Method To provide a more accurate idea of the fault finding ability and cost of several criteria, we set out to compare three structural graph coverage criteria: edge coverage (EC), edge-pair coverage (EPC), and prime path coverage (PPC). PPC and EPC subsume EC. To compare these criteria we examined 189 functions from 39 C programs, used mutants as a proxy for faults, and performed a statistical analysis of the results. Result The three criteria are very similar in terms of effectiveness when all mutants are taken into account: PPC killed 98% of the mutants, EPC 97%, and EC 94%. However, the difference between the criteria is emphasized with minimal mutant sets: PPC killed 75% of the mutants, EPC killed 67%, and EC killed only 57%. As for the cost of these criteria, we found that there is not much difference in terms of the number of TRs. We expected PPC to have the most TRs, so we were surprised to find that, on average, the number of TRs for EPC was highest. Conclusion PPC can detect more faults, specially in programs that have complicated control flows, but at higher cost. Thus, a practical tester can make an informed cost versus benefit decision. A better understanding of which structures in the programs contribute to the expense might help to choose when to use PPC. [ABSTRACT FROM AUTHOR] Copyright of Science of Computer Programming is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126478868&site=ehost-live
488,Categorization of Common. Coupling and Its Application to the Maintainability of the Linux Kernel.,Jeff Offutt,IEEE Transactions on Software Engineering,985589,,Oct-04,30,10,694,13,14654398,,IEEE,Article,OPEN source software; SYSTEMS software; COMPUTER operating systems; DATA transmission systems; DIGITAL communications; Software publishers (except video game publishers); Software Publishers,common coupling; definition-use analysis; dependencies; kernel-based software; Linux; Modularity; open-source software,"Data coupling between modules, especially common coupling, has long been considered a source of concern in software design, but the issue is somewhat more complicated for products that are comprised of kernel modules together with optional nonkernel modules. This paper presents a refined categorization of common coupling based on definitions and uses between kernel and nonkernel modules and applies the categorization to a case study. Common coupling is usually avoided when possible because of the potential for introducing risky dependencies among software modules. The relative risk of these dependencies is strongly related to the specific definition-use relationships. In a previous paper, we presented results from a longitudinal analysis of multiple versions of the open-source operating system Linux. This paper applies the new common coupling categorization to version 2.4.20 of Linux, counting the number of instances of common coupling between each of the 26 kernel modules and all the other nonkernel modules. We also categorize each coupling in terms of the definition-use relationships. Results show that the Linux kernel contains a large number of common couplings of all types, raising a concern about the long-term maintainability of Linux. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14654398&site=ehost-live
489,"Evaluating the role of professional development on elementary teachers’ knowledge, comfort, and beliefs related to teaching computer science to students with high-incidence disabilities.",Jeff Offutt,Journal of Research on Technology in Education,15391523,,Jun-22,,,1,17,157864318,10.1080/15391523.2022.2089408,Taylor & Francis Ltd,Article,,Computer science education; professional development; special education; teach perceptions; teacher beliefs; universal design for learning,"Abstract This article reports results from the implementation of a model of professional development (PD) to help K-5 teachers develop the knowledge and skills to teach Computer Science (CS) in classrooms of diverse students, including students with high-incidence disabilities. This article describes our Inclusive CS model of PD, how we made the PD model available to teachers during a pandemic and presents quantitative and qualitative results about the impact of the PD on teachers’ knowledge, comfort, and beliefs related to teaching computer science to students. Results indicate that the teachers’ knowledge, comfort, beliefs and perceptions about teaching CS to students with disabilities significantly improved. Teachers’ knowledge and understanding of Universal Design for Learning for supporting students in learning about CS also improved. [ABSTRACT FROM AUTHOR] Copyright of Journal of Research on Technology in Education is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157864318&site=ehost-live
490,Modeling presentation layers of web applications for testing.,Jeff Offutt,Software & Systems Modeling,16191366,,Apr-10,9,2,257,24,48999313,10.1007/s10270-009-0125-4,Springer Nature,Article,"WEB-based user interfaces; COMPUTER software development; HTML (Document markup language); COMPUTER software; SOFTWARE engineering; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; Software publishers (except video game publishers); Computer and software stores; Custom Computer Programming Services; Computer systems design and related services (except video game design and development)",Test criteria; Web applications; Web modeling,"Web software applications have become complex, sophisticated programs that are based on novel computing technologies. Their most essential characteristic is that they represent a different kind of software deployment—most of the software is never delivered to customers’ computers, but remains on servers, allowing customers to run the software across the web. Although powerful, this deployment model brings new challenges to developers and testers. Checking static HTML links is no longer sufficient; web applications must be evaluated as complex software products. This paper focuses on three aspects of web applications that are unique to this type of deployment: (1) an extremely loose form of coupling that features distributed integration, (2) the ability that users have to directly change the potential flow of execution, and (3) the dynamic creation of HTML forms. Taken together, these aspects allow the potential control flow to vary with each execution, thus the possible control flows cannot be determined statically, prohibiting several standard analysis techniques that are fundamental to many software engineering activities. This paper presents a new way to model web applications, based on software couplings that are new to web applications, dynamic flow of control, distributed integration, and partial dynamic web application development. This model is based on the notion of atomic sections, which allow analysis tools to build the analog of a control flow graph for web applications. The atomic section model has numerous applications in web applications; this paper applies the model to the problem of testing web applications. [ABSTRACT FROM AUTHOR] Copyright of Software & Systems Modeling is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=48999313&site=ehost-live
491,Mutation at the multi-class and system levels,Jeff Offutt,Science of Computer Programming,1676423,,Apr-13,78,4,364,24,85250477,10.1016/j.scico.2012.02.005,Elsevier B.V.,Article,"MUTATION testing of computer software; PROGRAMMING languages; COMPUTER network protocols; WEB services; SYSTEM integration; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; Software Publishers; Data Processing, Hosting, and Related Services; Computer Systems Design Services",Flexible weak mutation; Mutation; Mutation process; System testing; Testing,"Abstract: Mutation analysis has been applied to many testing problems, including numerous programming languages, specifications, network protocols, web services, and security policies. Program mutation, where mutation analysis is applied to programs, has been applied to the unit level (functions and methods), integration of pairs of functions, and individual classes. However, program mutation has not been applied to the problem of integration testing of multiple classes or entire software programs; thus, there is no system level mutation. This paper introduces a project on the problem of integration testing of multiple classes (multi-class) and system level mutation testing. The technical differences between using mutation to test single classes and multiple classes are explored, and new system level mutation operators are defined. A new execution style for detecting killed mutants, flexible weak mutation, is introduced. A mutation tool, Bacterio, still under construction, is also described. [Copyright &y& Elsevier] Copyright of Science of Computer Programming is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=85250477&site=ehost-live
492,Putting the Engineering into Software Engineering Education.,Jeff Offutt,IEEE Software,7407459,,Jan-13,30,1,96,1,84675195,10.1109/MS.2013.12,IEEE,Article,"SOFTWARE engineering education; COMPUTER science education; ENGINEERING education; TEACHING methods; EDUCATIONAL objectives; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Computer engineering education; Engineering education; scientific discipline; Software engineering; software engineering education; teaching paradigm,"Based on over 20 years of teaching and research experience, the author provides his assessment of software engineering education. He then builds on the analysis to provide recommendations on how we need to diverge from computer science to increase our impact, gain credibility, and ultimately ensure the success and recognition of our young discipline. A key behind the author's message is that we need to become a true engineering discipline. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Software is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=84675195&site=ehost-live
493,SE 2014: Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering.,Jeff Offutt,Computer (00189162),189162,,Nov-15,48,11,106,4,111647090,10.1109/MC.2015.345,IEEE,Article,GUIDELINES; CURRICULUM; UNDERGRADUATES; SOFTWARE engineering education; HIGHER education; ACADEMIC degrees,Computer science education; degree; Education courses; SE2014; software; Software engineering; standards,"Revised curriculum guidelines help university faculty create or update undergraduate software engineering programs. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=111647090&site=ehost-live
494,Testing concurrent user behavior of synchronous web applications with Petri nets.,Jeff Offutt,Software & Systems Modeling,16191366,,Apr-19,18,2,913,24,135694871,10.1007/s10270-018-0655-8,Springer Nature,Article,PETRI nets; WEB-based user interfaces; SOFTWARE development tools; DESIGN techniques; Custom Computer Programming Services; Computer systems design and related services (except video game design and development),Model-based testing; Petri nets; Test criteria; Web applications,"Web applications are now used in every aspect of our lives to manage work, provide products and services, read email, and provide entertainment. The software technologies used to build web applications provide features that help designers provide flexible functionality, but that are challenging to model and test. In particular, the network-based request-response model of programming means that web applications are inherently ""stateless"" and implicitly concurrent. They are stateless because a new network connection is made for each request (for example, when a user clicks a submit button). Thus, the server does not, by default, recognize multiple requests from the same user. Web applications are also concurrent because multiple users can use the same web application at the same time, creating contention for the same resources. Unfortunately, most web application testing does not adequately evaluate these aspects of web applications, leaving many software faults in deployed web applications. Part of this problem is because most traditional software modeling tools (such as UML) do not have built-in support for the stateless and concurrent aspects of web applications. This research project uses a novel model that is based on Petri nets to describe certain aspects of the behavior of web applications. This paper makes several contributions. We present a novel technique to design tests from this model that explicitly tests concurrency in web applications. We present novel coverage criteria that are defined on the Petri net model. We present results from an empirical study of 18 web applications with 343 components and 30,186 lines of code, followed by a case study on a large industrial web application. The tests found significantly more faults than traditional requirements-based tests, with fewer tests. [ABSTRACT FROM AUTHOR] Copyright of Software & Systems Modeling is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135694871&site=ehost-live
495,Testing Web applications by modeling with FSMs.,Jeff Offutt,Software & Systems Modeling,16191366,,Jul-05,4,3,326,20,17551496,10.1007/s10270-004-0077-7,Springer Nature,Article,"WEB services; APPLICATION software; TESTING; SOFTWARE engineering; COMPUTER systems; COMPUTER simulation; Software Publishers; Software publishers (except video game publishers); Custom Computer Programming Services; Computer systems design and related services (except video game design and development); Computer Systems Design Services; Data Processing, Hosting, and Related Services",Finite state machines; System testing; Testing of Web applications,"Researchers and practitioners are still trying to find effective ways to model and test Web applications. This paper proposes a system-level testing technique that combines test generation based on finite state machines with constraints. We use a hierarchical approach to model potentially large Web applications. The approach builds hierarchies of Finite State Machines (FSMs) that model subsystems of the Web applications, and then generates test requirements as subsequences of states in the FSMs. These subsequences are then combined and refined to form complete executable tests. The constraints are used to select a reduced set of inputs with the goal of reducing the state space explosion otherwise inherent in using FSMs. The paper illustrates the technique with a running example of a Web-based course student information system and introduces a prototype implementation to support the technique. [ABSTRACT FROM AUTHOR] Copyright of Software & Systems Modeling is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17551496&site=ehost-live
496,21 Years of Distributed Denial-of Service: Current State of Affairs.,Eric Osterweil,Computer (00189162),189162,,Jul-20,53,7,88,5,144376186,10.1109/MC.2020.2983711,IEEE,Article,,Aggregates; Computer crime; Denial-of-service attack; Internet; Protocols; Servers,"The Internet's features and capacity have evolved, but is the nature of its security noticeably better? We examine the fundamental nature of distributed denial-of-service (DDoS) and the state of the union of our defenses in today's DDoS wars. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144376186&site=ehost-live
496,21 Years of Distributed Denial-of Service: Current State of Affairs.,Angelos Stavrou,Computer (00189162),189162,,Jul-20,53,7,88,5,144376186,10.1109/MC.2020.2983711,IEEE,Article,,Aggregates; Computer crime; Denial-of-service attack; Internet; Protocols; Servers,"The Internet's features and capacity have evolved, but is the nature of its security noticeably better? We examine the fundamental nature of distributed denial-of-service (DDoS) and the state of the union of our defenses in today's DDoS wars. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144376186&site=ehost-live
497,21 Years of Distributed Denial-of-Service: A Call to Action.,Eric Osterweil,Computer (00189162),189162,,Aug-20,53,8,94,6,144933842,10.1109/MC.2020.2993330,IEEE,Article,"COMPUTER crimes; WEB services; INTERNET; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; Data Processing, Hosting, and Related Services",Bandwidth; Computer crime; Denial-of-service attack; Protocols; Web and internet services,"We are falling behind in the war against distributed denial-of-service attacks. Unless we act now, the future of the Internet could be at stake. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144933842&site=ehost-live
497,21 Years of Distributed Denial-of-Service: A Call to Action.,Angelos Stavrou,Computer (00189162),189162,,Aug-20,53,8,94,6,144933842,10.1109/MC.2020.2993330,IEEE,Article,"COMPUTER crimes; WEB services; INTERNET; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; Data Processing, Hosting, and Related Services",Bandwidth; Computer crime; Denial-of-service attack; Protocols; Web and internet services,"We are falling behind in the war against distributed denial-of-service attacks. Unless we act now, the future of the Internet could be at stake. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144933842&site=ehost-live
498,Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-the-loop machine learning.,Hemant Purohit,International Journal of Human-Computer Studies,10715819,,Apr-22,160,,N.PAG,1,154896355,10.1016/j.ijhcs.2022.102772,Academic Press Inc.,Article,HUMAN error; MACHINE learning; WEB analytics; MEMORY; ACTIVE learning; SOCIAL media,Active learning; Annotation schedule; Human-AI collaboration; Human-centered computing; Memory decay,"• Study of human annotation task in hybrid stream processing systems. • Presenting a generic human error framework of serial ordering-based mistakes and slips. • Verifying of the proposed human error framework through extensive experiments. • Presenting a novel method for human error-mitigation in an active learning paradigm. • Validating the novel method through simulation-based experiments. High-quality human annotations are necessary for creating effective machine learning-driven stream processing systems. We study hybrid stream processing systems based on a Human-In-The-Loop Machine Learning (HITL-ML) paradigm, in which one or many human annotators and an automatic classifier (trained at least partially by the human annotators) label an incoming stream of instances. This is typical of many near-real-time social media analytics and web applications, including annotating social media posts during emergencies by digital volunteer groups. From a practical perspective, low-quality human annotations result in wrong labels for retraining automated classifiers and indirectly contribute to the creation of inaccurate classifiers. Considering human annotation as a psychological process allows us to address these limitations. We show that human annotation quality is dependent on the ordering of instances shown to annotators and can be improved by local changes in the instance sequence/order provided to the annotators, yielding a more accurate annotation of the stream. We adapt a theoretically-motivated human error framework of mistakes and slips for the human annotation task to study the effect of ordering instances (i.e., an ""annotation schedule""). Further, we propose an error-avoidance approach to the active learning paradigm for stream processing applications robust to these likely human errors (in the form of slips) when deciding a human annotation schedule. We support the human error framework using crowdsourcing experiments and evaluate the proposed algorithm against standard baselines for active learning via extensive experimentation on classification tasks of filtering relevant social media posts during natural disasters. According to these experiments, considering the order in which data instances are presented to a human annotator leads to increased accuracy for machine learning and awareness of the potential properties of human memory for the class concept, which may affect annotation for automated classifiers. Our results allow the design of hybrid stream processing systems based on the HITL-ML paradigm, which requires the same amount of human annotations, but that has fewer human annotation errors. Automated systems that help reduce human annotation errors could benefit several web stream processing applications, including social media analytics and news filtering. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Human-Computer Studies is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154896355&site=ehost-live
499,"Sex, Lies, and Stereotypes: Gendered Implications of Fake News for Women in Politics.",Hemant Purohit,Public Integrity,10999922,,Sep/Oct2019,21,5,491,12,138322711,10.1080/10999922.2019.1626695,Taylor & Francis Ltd,Article,"UNITED States presidential election, 2016; WOMEN leaders; CLINTON, Hillary Rodham, 1947-; TRUMP, Donald, 1946-",fake news; gender bias; women in politics,"This analysis examines the literature on gendered media coverage of women candidates for higher office, and considers how biases in the treatment of candidates based on gender may be evident in or exacerbated by the promulgation of fake news. Using the 2016 Presidential election cycle in the United States as a case study, two fake news stories are investigated, which, like most fake news stories at the time, exhibited coverage in favor of the candidacy of Donald Trump and demonized or denigrated his opponent, Hillary Clinton. Findings suggest that the Pizzagate and Hillary Health Scare stories evince gendered narratives supporting stereotypes of women as unfit for leadership positions, and either villainize or trivialize women, depending on their perceived degree of power. Using a dataset of news articles and tweets from the months surrounding the 2016 election, evidence is offered of more negative coverage of the female versus male contender, in keeping with the findings of the literature, though the presence of potentially confounding factors, including personality and party, is acknowledged. [ABSTRACT FROM AUTHOR] Copyright of Public Integrity is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138322711&site=ehost-live
500,Extremes of locally stationary Gaussian and chi fields on manifolds.,Wanli Qiao,Stochastic Processes & Their Applications,3044149,,Mar-21,133,,166,27,148365508,10.1016/j.spa.2020.11.006,Elsevier B.V.,Article,VORONOI polygons; STATIONARY processes; PROBABILITY theory,Chi-fields; Excursion probabilities; Gaussian fields; Local stationarity; Positive reach; Voronoi diagrams,"Depending on a parameter h ∈ (0 , 1 ] , let { X h (t) , t ∈ M h } be a class of centered Gaussian fields indexed by compact manifolds M h with positive reach. For locally stationary Gaussian fields X h , we study the asymptotic excursion probabilities of X h on M h. Two cases are considered: (i) h is fixed and (ii) h → 0. These results are also extended to obtain the limit behaviors of the extremes of locally stationary χ -fields on manifolds. [ABSTRACT FROM AUTHOR] Copyright of Stochastic Processes & Their Applications is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148365508&site=ehost-live
501,From mutations to mechanisms and dysfunction via computation and mining of protein energy landscapes.,Wanli Qiao,BMC Genomics,14712164,,9/24/2018 Supplement 7,19,,1,13,131918149,10.1186/s12864-018-5024-z,BioMed Central,Article,PROTEINS; MOLECULES; EQUILIBRIUM; DYNAMICS; GENETIC mutation,Basins; Energy landscape; Equilibrium dynamics; Landscape mining; Landscape reconstruction; Pathogenic mutations; Protein dysfunction; Saddles,"Background: The protein energy landscape underscores the inherent nature of proteins as dynamic molecules interconverting between structures with varying energies. Reconstructing a protein's energy landscape holds the key to characterizing a protein's equilibrium conformational dynamics and its relationship to function. Many pathogenic mutations in protein sequences alter the equilibrium dynamics that regulates molecular interactions and thus protein function. In principle, reconstructing energy landscapes of a protein's healthy and diseased variants is a central step to understanding how mutations impact dynamics, biological mechanisms, and function. Results: Recent computational advances are yielding detailed, sample-based representations of protein energy landscapes. In this paper, we propose and describe two novel methods that leverage computed, sample-based representations of landscapes to reconstruct them and extract from them informative local structures that reveal the underlying organization of an energy landscape. Such structures constitute landscape features that, as we demonstrate here, can be utilized to detect alterations of landscapes upon mutation. Conclusions: The proposed methods detect altered protein energy landscape features in response to sequence mutations. By doing so, the methods allow formulating hypotheses on the impact of mutations on specific biological activities of a protein. This work demonstrates that the availability of energy landscapes of healthy and diseased variants of a protein opens up new avenues to harness the quantitative information embedded in landscapes to summarize mechanisms via which mutations alter protein dynamics to percolate to dysfunction. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131918149&site=ehost-live
501,From mutations to mechanisms and dysfunction via computation and mining of protein energy landscapes.,Amarda Shehu,BMC Genomics,14712164,,9/24/2018 Supplement 7,19,,1,13,131918149,10.1186/s12864-018-5024-z,BioMed Central,Article,PROTEINS; MOLECULES; EQUILIBRIUM; DYNAMICS; GENETIC mutation,Basins; Energy landscape; Equilibrium dynamics; Landscape mining; Landscape reconstruction; Pathogenic mutations; Protein dysfunction; Saddles,"Background: The protein energy landscape underscores the inherent nature of proteins as dynamic molecules interconverting between structures with varying energies. Reconstructing a protein's energy landscape holds the key to characterizing a protein's equilibrium conformational dynamics and its relationship to function. Many pathogenic mutations in protein sequences alter the equilibrium dynamics that regulates molecular interactions and thus protein function. In principle, reconstructing energy landscapes of a protein's healthy and diseased variants is a central step to understanding how mutations impact dynamics, biological mechanisms, and function. Results: Recent computational advances are yielding detailed, sample-based representations of protein energy landscapes. In this paper, we propose and describe two novel methods that leverage computed, sample-based representations of landscapes to reconstruct them and extract from them informative local structures that reveal the underlying organization of an energy landscape. Such structures constitute landscape features that, as we demonstrate here, can be utilized to detect alterations of landscapes upon mutation. Conclusions: The proposed methods detect altered protein energy landscape features in response to sequence mutations. By doing so, the methods allow formulating hypotheses on the impact of mutations on specific biological activities of a protein. This work demonstrates that the availability of energy landscapes of healthy and diseased variants of a protein opens up new avenues to harness the quantitative information embedded in landscapes to summarize mechanisms via which mutations alter protein dynamics to percolate to dysfunction. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131918149&site=ehost-live
502,"Protocol for a quasi-experimental, 950 county study examining implementation outcomes and mechanisms of Stepping Up, a national policy effort to improve mental health and substance use services for justice-involved individuals.",Niloofar Ramezani,Implementation Science,17485908,,3/29/21,16,1,1,14,149527386,10.1186/s13012-021-01095-2,BioMed Central,journal article,"MENTAL health policy; SUBSTANCE abuse; COMMUNITY mental health services; GOVERNMENT policy; MENTAL health services; MENTAL health; CRIMINAL justice system; Residential Mental Health and Substance Abuse Facilities; Other Individual and Family Services; Outpatient Mental Health and Substance Abuse Centers; Other Justice, Public Order, and Safety Activities; Offices of Mental Health Practitioners (except Physicians); Psychiatric and Substance Abuse Hospitals; Administration of Public Health Programs",Community; Implementation; Jail; Mechanism; Mental health; Outcome; Substance use,"<bold>Background: </bold>The criminal justice system is the largest provider of mental health services in the USA. Many jurisdictions are interested in reducing the use of the justice system for mental health problems. The national Stepping Up Initiative helps agencies within counties work together more effectively to reduce the number of individuals with mental illness in jails and to improve access to mental health services in the community. This study will compare Stepping Up counties to matched comparison counties over time to (1) examine the effectiveness of Stepping Up and (2) test hypothesized implementation mechanisms to inform multi-agency implementation efforts more broadly.<bold>Methods: </bold>The study will survey 950 counties at baseline, 18 months, and 36 months in a quasi-experimental design comparing implementation mechanisms and outcomes between 475 Stepping Up counties and 475 matched comparison counties. Surveys will be sent to up to four respondents per county including administrators of jail, probation, community mental health services, and community substance use treatment services (3800 total respondents). We will examine whether Stepping Up counties show faster improvements in implementation outcomes (number of justice-involved clients receiving behavioral health services, number of behavioral health evidence-based practices and policies [EBPPs] available to justice-involved individuals, and resources for behavioral health EBPP for justice-involved individuals) than do matched comparison counties. We will also evaluate whether engagement of hypothesized mechanisms explains differences in implementation outcomes. Implementation target mechanisms include (1) use of and capacity for performance monitoring, (2) use and functioning of interagency teams, (3) common goals and mission across agencies, and (4) system integration (i.e., building an integrated system of care rather than adding one program or training). Finally, we will characterize implementation processes and critical incidents using survey responses and qualitative interviews.<bold>Discussion: </bold>There are few rigorous, prospective studies examining implementation mechanisms and their relationship with behavioral health implementation outcomes in justice and associated community behavioral health settings. There is also limited understanding of implementation mechanisms that occur across systems with multiple goals. This study will describe implementation outcomes of Stepping Up and will elucidate target mechanisms that are effective in multi-goal, multi-agency systems. [ABSTRACT FROM AUTHOR] Copyright of Implementation Science is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149527386&site=ehost-live
503,Protocol for the Mason: Health Starts Here prospective cohort study of young adult college students.,Niloofar Ramezani,BMC Public Health,14712458,,5/12/21,21,1,1,15,150301669,10.1186/s12889-021-10969-5,BioMed Central,journal article,COVID-19 pandemic; MENTAL health of college students; SOCIOECONOMIC status; GENDER identity; PERIODIC health examinations,,"<bold>Background: </bold>Young adulthood is a period of increasing independence for the 40% of young adults enrolled in U.S. colleges. Previous research indicates differences in how students' health behaviors develop and vary by gender, race, ethnicity, and socioeconomic status. George Mason University is a state institution that enrolls a highly diverse student population, making it an ideal setting to launch a longitudinal cohort study using multiple research methods to evaluate the effects of health behaviors on physical and psychological functioning, especially during the COVID-19 pandemic.<bold>Results: </bold>Mason: Health Starts Here was developed as a longitudinal cohort study of successive waves of first year students that aims to improve understanding of the natural history and determinants of young adults' physical health, mental health, and their role in college completion. The study recruits first year students who are 18 to 24 years old and able to read and understand English. All incoming first year students are recruited through various methods to participate in a longitudinal cohort for 4 years. Data collection occurs in fall and spring semesters, with online surveys conducted in both semesters and in-person clinic visits conducted in the fall. Students receive physical examinations during clinic visits and provide biospecimens (blood and saliva).<bold>Conclusions: </bold>The study will produce new knowledge to help understand the development of health-related behaviors during young adulthood. A long-term goal of the cohort study is to support the design of effective, low-cost interventions to encourage young adults' consistent performance of healthful behaviors, improve their mental health, and improve academic performance. [ABSTRACT FROM AUTHOR] Copyright of BMC Public Health is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150301669&site=ehost-live
504,Substance use and firearm access among college freshmen.,Niloofar Ramezani,Journal of American College Health,7448481,,Apr-22,,,1,5,156826695,10.1080/07448481.2022.2068959,Taylor & Francis Ltd,Article,,Gun access; heavy episodic drinking; university,"Abstract Objective Participants Methods Results Conclusions Examine the proportion of students with rapid firearm access and associations with recent alcohol and marijuana use.Cross-sectional data from college freshmen (<italic>n</italic> = 183) in 2020 who participated in the <italic>Mason: Health Starts Here</italic> study.Using logistic regression, associations were examined between past 30-day substance use and access to firearms within 15-min.More than 10% of students could rapidly access a firearm, 53% of whom were current binge drinkers, compared to 13% of those who could not rapidly access firearms. Non-Hispanic White students (AOR = 4.1, 95%CI = 1.3,12.7) and past 30-day binge drinkers (AOR = 6.4, 95%CI = 2.1,19.7) had greater odds of having rapid firearm access. Age, sex, and past 30-day marijuana use were not associated with rapid access.A notable proportion of students had rapid firearm access, which was strongly associated with recent binge drinking. Campus prevention programs should consider how their alcohol and firearm policies could be enhanced to prevent violence/self-harm. [ABSTRACT FROM AUTHOR] Copyright of Journal of American College Health is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156826695&site=ehost-live
505,"The relationship between community public health, behavioral health service accessibility, and mass incarceration.",Niloofar Ramezani,BMC Health Services Research,14726963,,7/29/22,22,1,1,11,158273191,10.1186/s12913-022-08306-6,BioMed Central,Article,,Access to care; Behavioral health; Community public health; Community service accessibility; Jail population; Mass incarceration,"<bold>Background: </bold>The relationship between healthcare service accessibility in the community and incarceration is an important, yet not widely understood, phenomenon. Community behavioral health and the criminal legal systems are treated separately, which creates a competing demand to confront mass incarceration and expand available services. As a result, the relationship between behavioral health services, demographics and community factors, and incarceration rate has not been well addressed. Understanding potential drivers of incarceration, including access to community-based services, is necessary to reduce entry into the legal system and decrease recidivism. This study identifies county-level demographic, socioeconomic, healthcare services availability/accessibility, and criminal legal characteristics that predict per capita jail population across the U.S. More than 10 million individuals pass through U.S. jails each year, increasing the urgency of addressing this challenge.<bold>Methods: </bold>The selection of variables for our model proceeded in stages. The study commenced by identifying potential descriptors and then using machine learning techniques to select non-collinear variables to predict county jail population per capita. Beta regression was then applied to nationally available data from all 3,141 U.S. counties to identify factors predicting county jail population size. Data sources include the Vera Institute's incarceration database, Robert Wood Johnson Foundation's County Health Rankings and Roadmaps, Uniform Crime Report, and the U.S. Census.<bold>Results: </bold>Fewer per capita psychiatrists (z-score = -2.16; p = .031), lower percent of drug treatment paid by Medicaid (-3.66; p < .001), higher per capita healthcare costs (5.71; p < .001), higher number of physically unhealthy days in a month (8.6; p < .001), lower high school graduation rate (-4.05; p < .001), smaller county size (-2.66, p = .008; -2.71, p = .007; medium and large versus small counties, respectively), and more police officers per capita (8.74; p < .001) were associated with higher per capita jail population. Controlling for other factors, violent crime rate did not predict incarceration rate.<bold>Conclusions: </bold>Counties with smaller populations, larger percentages of individuals that did not graduate high school, that have more health-related issues, and provide fewer community treatment services are more likely to have higher jail population per capita. Increasing access to services, including mental health providers, and improving the affordability of drug treatment and healthcare may help reduce incarceration rates. [ABSTRACT FROM AUTHOR] Copyright of BMC Health Services Research is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158273191&site=ehost-live
506,"What Matters More in Explaining Drug Court Graduation and Rearrest: Program Features, Individual Characteristics, or Some Combination.",Niloofar Ramezani,International Journal of Offender Therapy & Comparative Criminology,0306624X,,Apr-22,,,1,,156459114,10.1177/0306624x221086558,Sage Publications Inc.,Article,,drug courts; hierarchical linear modeling; program graduation; programming; rearrest; risk need assessments,"This study examines the program- and individual-level factors that impact the success of drug court clients in terms of: (1) graduation; and (2) not being arrested while participating in the court program. The data consist of 848 individuals in nine drug courts. This paper discusses how different individual- and program-level factors impact the success of drug court participants. The findings suggest that individual- and program-level factors are both important in predicting program graduation and arrest during drug court participation, while controlling for participant demographics. Clients’ education, drug/alcohol usage, program staffing, and clinical standards impact program graduation while criminal history, drug/alcohol usage, number of program hours offered, program staffing, and use of rewards and sanctions predict in-program arrest. Models combining both program- and individual-level factors performed better than either alone, leading to recommendations that agencies should emphasize improving program quality while targeting clients’ needs to achieve greater success. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Offender Therapy & Comparative Criminology is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156459114&site=ehost-live
507,Colonic microbiome is altered in alcoholism.,Huzefa Rangwala,American Journal of Physiology: Gastrointestinal & Liver Physiology,1931857,,May-12,65,5,G966,14,95871562,10.1152/ajpgi.00380.2011,American Physiological Society,Article,,alcohol; alcoholic liver disease; colon; colonic microbiota; length heterogeneity polymerase chain reaction; pyrosequencing,"Several studies indicate the importance of colonic microbiota in metabolic and inflammatory disorders and importance of diet on microbiota composition. The effects of alcohol, one of the prominent components of diet, on colonic bacterial composition is largely unknown. Mounting evidence suggests that gut-derived bacterial endotoxins are cofactors for alcohol-induced tissue injury and organ failure like alcoholic liver disease (ALD) that only occur in a subset of alcoholics. We hypothesized that chronic alcohol consumption results in alterations of the gut microbiome in a subgroup of alcoholics, and this may be responsible for the observed inflammatory state and endotoxemia in alcoholics. Thus we interrogated the mucosa-associated colonic microbiome in 48 alcoholics with and without ALD as well as 18 healthy subjects. Colonic biopsy samples from subjects were analyzed for microbiota composition using length heterogeneity PCR fingerprinting and multitag pyrosequencing. A subgroup of alcoholics have an altered colonic microbiome (dysbiosis). The alcoholics with dysbiosis had lower median abundances of Bacteroidetes and higher ones of Proteobacteria. The observed alterations appear to correlate with high levels of serum endotoxin in a subset of the samples. Network topology analysis indicated that alcohol use is correlated with decreased connectivity of the microbial network, and this alteration is seen even after an extended period of sobriety. We show that the colonic mucosaassociated bacterial microbiome is altered in a subset of alcoholics. The altered microbiota composition is persistent and correlates with endotoxemia in a subgroup of alcoholics. [ABSTRACT FROM AUTHOR] Copyright of American Journal of Physiology: Gastrointestinal & Liver Physiology is the property of American Physiological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95871562&site=ehost-live
508,Evaluation of short read metagenomic assembly.,Huzefa Rangwala,BMC Genomics,14712164,,2011 Supplement 2,12,Suppl 2,1,13,64164241,10.1186/1471-2164-12-S2-S8,BioMed Central,Article,GENOMES; ORGANISMS; NUCLEOTIDE sequence; ERROR rates; GENETICS,,"Background: Metagenomic assembly is a challenging problem due to the presence of genetic material from multiple organisms. The problem becomes even more difficult when short reads produced by next generation sequencing technologies are used. Although whole genome assemblers are not designed to assemble metagenomic samples, they are being used for metagenomics due to the lack of assemblers capable of dealing with metagenomic samples. We present an evaluation of assembly of simulated short-read metagenomic samples using a state-of-art de Bruijn graph based assembler. Results: We assembled simulated metagenomic reads from datasets of various complexities using a state-of-art de Bruijn graph based parallel assembler. We have also studied the effect of k-mer size used in de Bruijn graph on metagenomic assembly and developed a clustering solution to pool the contigs obtained from different assembly runs, which allowed us to obtain longer contigs. We have also assessed the degree of chimericity of the assembled contigs using an entropy/impurity metric and compared the metagenomic assemblies to assemblies of isolated individual source genomes. Conclusions: Our results show that accuracy of the assembled contigs was better than expected for the metagenomic samples with a few dominant organisms and was especially poor in samples containing many closely related strains. Clustering contigs from different k-mer parameter of the de Bruijn graph allowed us to obtain longer contigs, however the clustering resulted in accumulation of erroneous contigs thus increasing the error rate in clustered contigs. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=64164241&site=ehost-live
509,IDMIL: an alignment-free Interpretable Deep Multiple Instance Learning (MIL) for predicting disease from whole-metagenomic data.,Huzefa Rangwala,Bioinformatics,13674803,,2020 Supplement,36,,i39,9,144525612,10.1093/bioinformatics/btaa477,Oxford University Press / USA,Article,CONVOLUTIONAL neural networks; ENVIRONMENTAL forensics; SUPERVISED learning; FORENSIC sciences; HUMAN body; FEATURE extraction; Testing Laboratories,,"Motivation The human body hosts more microbial organisms than human cells. Analysis of this microbial diversity provides key insight into the role played by these microorganisms on human health. Metagenomics is the collective DNA sequencing of coexisting microbial organisms in an environmental sample or a host. This has several applications in precision medicine, agriculture, environmental science and forensics. State-of-the-art predictive models for phenotype predictions from metagenomic data rely on alignments, assembly, extensive pruning, taxonomic profiling and reference sequence databases. These processes are time consuming and they do not consider novel microbial sequences when aligned with the reference genome, limiting the potential of whole metagenomics. We formulate the problem of predicting human disease from whole-metagenomic data using Multiple Instance Learning (MIL), a popular supervised learning paradigm. Our proposed alignment-free approach provides higher accuracy in prediction by harnessing the capability of deep convolutional neural network (CNN) within a MIL framework and provides interpretability via neural attention mechanism. Results The MIL formulation combined with the hierarchical feature extraction capability of deep-CNN provides significantly better predictive performance compared to popular existing approaches. The attention mechanism allows for the identification of groups of sequences that are likely to be correlated to diseases providing the much-needed interpretation. Our proposed approach does not rely on alignment, assembly and reference sequence databases; making it fast and scalable for large-scale metagenomic data. We evaluate our method on well-known large-scale metagenomic studies and show that our proposed approach outperforms comparative state-of-the-art methods for disease prediction. Availability and implementation https://github.com/mrahma23/IDMIL. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144525612&site=ehost-live
510,Mining manufacturing data for discovery of high productivity process characteristics,Huzefa Rangwala,Journal of Biotechnology,1681656,,Jun-10,147,4-Mar,186,12,51291934,10.1016/j.jbiotec.2010.04.005,Elsevier B.V.,Article,DATA mining; BIOLOGICAL products; MANUFACTURING industries; MULTIVARIATE analysis; BIOCHEMICAL engineering; CELL culture; SUPPORT vector machines; REGRESSION analysis; Biological Product (except Diagnostic) Manufacturing,Bioprocess; Cell culture; Data mining; Manufacturing; Multivariate data analysis; Support vector machine,"Abstract: Modern manufacturing facilities for bioproducts are highly automated with advanced process monitoring and data archiving systems. The time dynamics of hundreds of process parameters and outcome variables over a large number of production runs are archived in the data warehouse. This vast amount of data is a vital resource to comprehend the complex characteristics of bioprocesses and enhance production robustness. Cell culture process data from 108 ‘trains’ comprising production as well as inoculum bioreactors from Genentech''s manufacturing facility were investigated. Each run constitutes over one-hundred on-line and off-line temporal parameters. A kernel-based approach combined with a maximum margin-based support vector regression algorithm was used to integrate all the process parameters and develop predictive models for a key cell culture performance parameter. The model was also used to identify and rank process parameters according to their relevance in predicting process outcome. Evaluation of cell culture stage-specific models indicates that production performance can be reliably predicted days prior to harvest. Strong associations between several temporal parameters at various manufacturing stages and final process outcome were uncovered. This model-based data mining represents an important step forward in establishing a process data-driven knowledge discovery in bioprocesses. Implementation of this methodology on the manufacturing floor can facilitate a real-time decision making process and thereby improve the robustness of large scale bioprocesses. [Copyright &y& Elsevier] Copyright of Journal of Biotechnology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=51291934&site=ehost-live
511,Modulation of the Metabiome by Rifaximin in Patients with Cirrhosis and Minimal Hepatic Encephalopathy.,Huzefa Rangwala,PLoS ONE,19326203,,Apr-13,8,4,1,11,87677218,10.1371/journal.pone.0060042,Public Library of Science,Article,TREATMENT of cirrhosis of the liver; HEPATIC encephalopathy; ANTIBIOTICS; BRAIN function localization; SERUM; NEURAL circuitry; LIPID metabolism; MEDICAL statistics; HEALTH outcome assessment; Pharmaceuticals and pharmacy supplies merchant wholesalers; Drugs and Druggists' Sundries Merchant Wholesalers; Medicinal and Botanical Manufacturing; Pharmaceutical and medicine manufacturing; Biological Product (except Diagnostic) Manufacturing,Biochemistry; Biology; Cirrhosis; Clinical research design; Clinical trials; Cognition; Cognitive neurology; Cognitive neuroscience; Gastroenterology and hepatology; Genetics and Genomics; Host-pathogen interaction; Lipid metabolism; Liver diseases; Medicine; Metabolism; Microbiology; Neurological Disorders; Neurology; Neuroscience; Proteomics; Research Article; Spectrometric identification of proteins; Systems biology,"Hepatic encephalopathy (HE) represents a dysfunctional gut-liver-brain axis in cirrhosis which can negatively impact outcomes. This altered gut-brain relationship has been treated using gut-selective antibiotics such as rifaximin, that improve cognitive function in HE, especially its subclinical form, minimal HE (MHE). However, the precise mechanism of the action of rifaximin in MHE is unclear. We hypothesized that modulation of gut microbiota and their end-products by rifaximin would affect the gut-brain axis and improve cognitive performance in cirrhosis. Aim To perform a systems biology analysis of the microbiome, metabolome and cognitive change after rifaximin in MHE. Methods: Twenty cirrhotics with MHE underwent cognitive testing, endotoxin analysis, urine/serum metabolomics (GC and LC-MS) and fecal microbiome assessment (multi-tagged pyrosequencing) at baseline and 8 weeks post-rifaximin 550 mg BID. Changes in cognition, endotoxin, serum/urine metabolites (and microbiome were analyzed using recommended systems biology techniques. Specifically, correlation networks between microbiota and metabolome were analyzed before and after rifaximin. Results: There was a significant improvement in cognition(six of seven tests improved,p<0.01) and endotoxemia (0.55 to 0.48 Eu/ml, p = 0.02) after rifaximin. There was a significant increase in serum saturated (myristic, caprylic, palmitic, palmitoleic, oleic and eicosanoic) and unsaturated (linoleic, linolenic, gamma-linolenic and arachnidonic) fatty acids post-rifaximin. No significant microbial change apart from a modest decrease in Veillonellaceae and increase in Eubacteriaceae was observed. Rifaximin resulted in a significant reduction in network connectivity and clustering on the correlation networks. The networks centered on Enterobacteriaceae, Porphyromonadaceae and Bacteroidaceae indicated a shift from pathogenic to beneficial metabolite linkages and better cognition while those centered on autochthonous taxa remained similar. Conclusions: Rifaximin is associated with improved cognitive function and endotoxemia in MHE, which is accompanied by alteration of gut bacterial linkages with metabolites without significant change in microbial abundance. Trial Registration: ClinicalTrials.gov NCT01069133 [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=87677218&site=ehost-live
512,Predicting Student Performance Using Personalized Analytics.,Huzefa Rangwala,Computer (00189162),189162,,Apr-16,49,4,61,9,114640262,10.1109/MC.2016.119,IEEE,Article,RATING of students; FACTORIZATION; GRADE repetition; GRADING of students; GRADE advancement,Big data; computing in education; data analysis; data mining; Data models; Data retention; Education; learning-management systems; LMSs; massive open online courses; matrix factorization; MOOCs; multilinear regression; Predictive models; Recommender systems; Servers,"To help solve the ongoing problem of student retention, new expected performance-prediction techniques are needed to facilitate degree planning and determine who might be at risk of failing or dropping a class. Personalized multiregression and matrix factorization approaches based on recommender systems, initially developed for e-commerce applications, accurately forecast students' grades in future courses as well as on in-class assessments. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114640262&site=ehost-live
513,Real-Time Classification of Hand Motions Using Ultrasound Imaging of Forearm Muscles.,Huzefa Rangwala,IEEE Transactions on Biomedical Engineering,189294,,Aug-16,63,8,1687,12,116975131,10.1109/TBME.2015.2498124,IEEE,Article,ULTRASONIC imaging; ELECTROMYOGRAPHY; SIGNAL-to-noise ratio; SIGNAL processing; INFORMATION measurement,Image motion analysis; Imaging; Muscles; pattern classification; prosthetic control; Real-time systems; rehabilitation; Thumb; Ultrasonic imaging; ultrasound imaging; Wrist,"Surface electromyography (sEMG) has been the predominant method for sensing electrical activity for a number of applications involving muscle–computer interfaces, including myoelectric control of prostheses and rehabilitation robots. Ultrasound imaging for sensing mechanical deformation of functional muscle compartments can overcome several limitations of sEMG, including the inability to differentiate between deep contiguous muscle compartments, low signal-to-noise ratio, and lack of a robust graded signal. The objective of this study was to evaluate the feasibility of real-time graded control using a computationally efficient method to differentiate between complex hand motions based on ultrasound imaging of forearm muscles. Dynamic ultrasound images of the forearm muscles were obtained from six able-bodied volunteers and analyzed to map muscle activity based on the deformation of the contracting muscles during different hand motions. Each participant performed 15 different hand motions, including digit flexion, different grips (i.e., power grasp and pinch grip), and grips in combination with wrist pronation. During the training phase, we generated a database of activity patterns corresponding to different hand motions for each participant. During the testing phase, novel activity patterns were classified using a nearest neighbor classification algorithm based on that database. The average classification accuracy was 91%. Real-time image-based control of a virtual hand showed an average classification accuracy of 92%. Our results demonstrate the feasibility of using ultrasound imaging as a robust muscle–computer interface. Potential clinical applications include control of multiarticulated prosthetic hands, stroke rehabilitation, and fundamental investigations of motor control and biomechanics. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Biomedical Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116975131&site=ehost-live
513,Real-Time Classification of Hand Motions Using Ultrasound Imaging of Forearm Muscles.,Siddhartha Sikdar,IEEE Transactions on Biomedical Engineering,189294,,Aug-16,63,8,1687,12,116975131,10.1109/TBME.2015.2498124,IEEE,Article,ULTRASONIC imaging; ELECTROMYOGRAPHY; SIGNAL-to-noise ratio; SIGNAL processing; INFORMATION measurement,Image motion analysis; Imaging; Muscles; pattern classification; prosthetic control; Real-time systems; rehabilitation; Thumb; Ultrasonic imaging; ultrasound imaging; Wrist,"Surface electromyography (sEMG) has been the predominant method for sensing electrical activity for a number of applications involving muscle–computer interfaces, including myoelectric control of prostheses and rehabilitation robots. Ultrasound imaging for sensing mechanical deformation of functional muscle compartments can overcome several limitations of sEMG, including the inability to differentiate between deep contiguous muscle compartments, low signal-to-noise ratio, and lack of a robust graded signal. The objective of this study was to evaluate the feasibility of real-time graded control using a computationally efficient method to differentiate between complex hand motions based on ultrasound imaging of forearm muscles. Dynamic ultrasound images of the forearm muscles were obtained from six able-bodied volunteers and analyzed to map muscle activity based on the deformation of the contracting muscles during different hand motions. Each participant performed 15 different hand motions, including digit flexion, different grips (i.e., power grasp and pinch grip), and grips in combination with wrist pronation. During the training phase, we generated a database of activity patterns corresponding to different hand motions for each participant. During the testing phase, novel activity patterns were classified using a nearest neighbor classification algorithm based on that database. The average classification accuracy was 91%. Real-time image-based control of a virtual hand showed an average classification accuracy of 92%. Our results demonstrate the feasibility of using ultrasound imaging as a robust muscle–computer interface. Potential clinical applications include control of multiarticulated prosthetic hands, stroke rehabilitation, and fundamental investigations of motor control and biomechanics. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Biomedical Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116975131&site=ehost-live
514,Solid-Phase Microextraction and the Human Fecal VOC Metabolome.,Huzefa Rangwala,PLoS ONE,19326203,,2011,6,4,1,9,61167779,10.1371/journal.pone.0018471,Public Library of Science,Article,VOLATILE organic compounds; SOLID phase extraction; FIBERS; MASS spectrometry; METABOLITES; FECES,,"The diagnostic potential and health implications of volatile organic compounds (VOCs) present in human feces has begun to receive considerable attention. Headspace solid-phase microextraction (SPME) has greatly facilitated the isolation and analysis of VOCs from human feces. Pioneering human fecal VOC metabolomic investigations have utilized a single SPME fiber type for analyte extraction and analysis. However, we hypothesized that the multifarious nature of metabolites present in human feces dictates the use of several diverse SPME fiber coatings for more comprehensive metabolomic coverage. We report here an evaluation of eight different commercially available SPME fibers, in combination with both GC-MS and GCFID, and identify the 50/30 μm CAR-DVB-PDMS, 85 μm CAR-PDMS, 65 μm DVB-PDMS, 7 μm PDMS, and 60 μm PEG SPME fibers as a minimal set of fibers appropriate for human fecal VOC metabolomics, collectively isolating approximately 90% of the total metabolites obtained when using all eight fibers. We also evaluate the effect of extraction duration on metabolite isolation and illustrate that ex vivo enteric microbial fermentation has no effect on metabolite composition during prolonged extractions if the SPME is performed as described herein. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=61167779&site=ehost-live
516,Flipping an Engineering Thermodynamics Course to Improve Student Self- Efficacy.,Colin Reagle,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2017,,,12339,17,125730557,,ASEE,Article,FLIPPED classrooms; THERMODYNAMICS; ENGINEERING students; ENGINEERING education; ENGINEERING teachers,,"Thermodynamics is well documented as a difficult course in the engineering and technology curricula that require it. The flipped lecture format has been similarly documented to improve student-teacher interaction and student engagement. This work attempts to address whether flipping a difficult, demanding thermodynamics course improves student self-efficacy. Student surveys were conducted in multiple sections of a thermodynamics course over two years to evaluate student perceptions of the flipped course format. Students had positive perceptions about how class time was used in the flipped lecture style which was expected based on previous literature. Nearly all of the respondents agreed that using class time for discussion and problemsolving was very useful. No specific topic was singled out as unsuitable for the flip format; however several comments suggest that highly conceptual topics or topics that may be difficult to understand without examples are not suited for the flip format. Many students commented that the video lectures allowed them to be more prepared when they went to class and more actively engaged with class material. Most students also agree that they are confident in their ability to solve problems and apply their knowledge to new problems introduced in the course and in their ability to solve related problems in their future academic and professional endeavors. A majority of students also agreed that the course helped them to develop their own questions about the material and become more independent learners. These responses strongly support the use of the flipped class format for teaching technical courses and to improve self-efficacy. The methodology of flipping the course, lessons learned and the instructor's observations are also presented here. Developing flipped course materials is very time-consuming. Our results document the positive benefits of this teaching style for both students and instructors. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125730557&site=ehost-live
517,A Sorting Network on Trees.,Dana Richards,Parallel Processing Letters,1296264,,Dec-19,29,4,N.PAG,12,140235465,10.1142/S0129626419500154,World Scientific Publishing Company,Article,"COMPLETE graphs; TOPOLOGY; TREES; HYPERCUBES; ALGORITHMS; Flower, Nursery Stock, and Florists' Supplies Merchant Wholesalers",complexity; Oblivious sorting; sorting network,"Sorting networks are a class of parallel oblivious sorting algorithms. Not only do they have interesting theoretical properties but they can be fabricated. A sorting network is a sequence of parallel compare-exchange operations using comparators which are grouped into stages. This underlying graph defines the topology of the network. The majority of results on sorting networks concern the unrestricted case where the underlying graph is the complete graph. Prior results are also known for paths, hypercubes, and meshes. In this paper we introduce a sorting network whose underlying topology is a tree and formalize the concept of sorting networks on a restricted graph topology by introducing a new parameter for graphs called its sorting number. The main result of the paper is a description of an O (min (n Δ 2 , n 2)) depth sorting network on a tree with maximum degree Δ. [ABSTRACT FROM AUTHOR] Copyright of Parallel Processing Letters is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140235465&site=ehost-live
518,Broadcasting from multiple originators,Dana Richards,Discrete Applied Mathematics,0166218X,,Jul-09,157,13,2886,6,41239697,10.1016/j.dam.2009.02.013,Elsevier B.V.,Article,GRAPH theory; BROADCASTING industry; FUNDAMENTAL groups (Mathematics); GRAPHIC methods; ASYMPTOTES; MATHEMATICAL analysis; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,Broadcast; Minimum broadcast graph; Multiple originator; Relaxed broadcast,"Abstract: We begin an investigation of broadcasting from multiple originators, a variant of broadcasting in which any vertices may be the originators of a message in a network of vertices. The requirement is that the message be distributed to all vertices in minimum time. A minimum -originator broadcast graph is a graph on vertices with the fewest edges such that any subset of vertices can broadcast in minimum time. is the number of edges in such a graph. In this paper, we present asymptotic upper and lower bounds on . We also present an exact result for the case when . We also give an upper bound on the number of edges in a relaxed version of this problem in which one additional time unit is allowed for the broadcast. [Copyright &y& Elsevier] Copyright of Discrete Applied Mathematics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=41239697&site=ehost-live
519,Let the games continue.,Dana Richards,Scientific American,368733,,Oct-14,311,4,90,6,98530162,10.1038/scientificamerican1014-90,Scientific American,journal article,"RECREATIONAL mathematics; POLYOMINOES; CELLULAR automata; PARADOXES; TILING (Mathematics); GARDNER, Martin, 1914-2010; CONWAY, John H. (John Horton), 1937-",,"The article focuses on late author of ""Scientific American's"" Mathematical Games column, Martin Gardner. Topics include his influence and popularity within recreational mathematics, Gardner's early entries in the journal ""Scripta Mathematica"" and ""Scientific American"" on mathematical magic and machines that could solve basic problems, polyominoes and mathematician John Horton Conway's ""Game of Life,"" and Gardner's interest in Penrose tilings and Newcomb's Paradox.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98530162&site=ehost-live
520,Martin Gardner (1914-2010).,Dana Richards,Science,368075,,7/9/10,329,5988,157,1,52518333,10.1126/science.1194002,American Association for the Advancement of Science,Obituary,"GARDNER, Martin, 1914-2010",,An obituary for science author Martin Gardner is presented.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=52518333&site=ehost-live
521,Minimum multiple originator broadcast graphs.,Dana Richards,Discrete Applied Mathematics,0166218X,,Jan2017 Part 3,216,,646,16,120016179,10.1016/j.dam.2016.06.015,Elsevier B.V.,Article,GRAPH theory; BROADCASTING industry; GEOMETRIC vertices; EDGES (Geometry); SET theory; APPLIED mathematics; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,Broadcast; Minimum broadcast graph; Multiple originator,"Broadcasting from multiple originators is a variant of broadcasting in which any k vertices may be the originators of a message in a network of n vertices. A minimum broadcast graph has the fewest possible edges while still allowing minimum time broadcasting from any set of k originators. We provide a census of all known such graphs. [ABSTRACT FROM AUTHOR] Copyright of Discrete Applied Mathematics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=120016179&site=ehost-live
522,Asymptotically best response-adaptive randomization procedures,William Rosenberger,Journal of Statistical Planning & Inference,3783758,,Jun-06,136,6,1911,12,19687863,10.1016/j.jspi.2005.08.011,Elsevier B.V.,Article,"CLINICAL trials; THERAPEUTICS; HEALTH outcome assessment; MEDICAL research; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Adaptive designs; Clinical trials; Doubly-adaptive biased coin design; Neyman allocation; Primary 62G10; Rao–Cramér lower bound; Urn models,"Abstract: We derive a lower bound on the asymptotic variance of the allocation proportions from response-adaptive randomization procedures when the allocation proportions are asymptotically normal. A procedure that attains this lower bound is defined to be asymptotically best. We then compare the asymptotic variances of five procedures, for which allocation proportions converge, to the lower bound. We find that a procedure by Zelen and a procedure by Ivanova attain the lower bound and a procedure by Eisele and its extension to treatments can attain the lower bound but are, in general, not asymptotically best. We discuss the tradeoffs among the benefits of randomization, the benefits of attaining the lower bound, and the benefits of targeting an optimal allocation. We conclude that none of these procedures possesses all of these benefits. [Copyright &y& Elsevier] Copyright of Journal of Statistical Planning & Inference is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=19687863&site=ehost-live
523,Commentary on 'Designs for dose-escalation trials with quantitative responses'.,William Rosenberger,Statistics in Medicine,2776715,,Dec-09,28,30,3751,3,63566640,10.1002/sim.3730,"John Wiley & Sons, Inc.",Article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=63566640&site=ehost-live
524,Conditional Monte Carlo randomization tests for regression models.,William Rosenberger,Statistics in Medicine,2776715,,8/15/14,33,18,3078,11,103353557,10.1002/sim.6149,"John Wiley & Sons, Inc.",Article,,generalized linear mixed models; generalized linear models; linear rank test; longitudinal data; martingale residuals; time‐to‐event data; time-to-event data,"We discuss the computation of randomization tests for clinical trials of two treatments when the primary outcome is based on a regression model. We begin by revisiting the seminal paper of Gail, Tan, and Piantadosi (1988), and then describe a method based on Monte Carlo generation of randomization sequences. The tests based on this Monte Carlo procedure are design based, in that they incorporate the particular randomization procedure used. We discuss permuted block designs, complete randomization, and biased coin designs. We also use a new technique by Plamadeala and Rosenberger (2012) for simple computation of conditional randomization tests. Like Gail, Tan, and Piantadosi, we focus on residuals from generalized linear models and martingale residuals from survival models. Such techniques do not apply to longitudinal data analysis, and we introduce a method for computation of randomization tests based on the predicted rate of change from a generalized linear mixed model when outcomes are longitudinal. We show, by simulation, that these randomization tests preserve the size and power well under model misspecification. [ABSTRACT FROM AUTHOR] Copyright of Statistics in Medicine is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103353557&site=ehost-live
525,Covariate‐adjusted response‐adaptive randomization for multi‐arm clinical trials using a modified forward looking Gittins index rule.,William Rosenberger,Biometrics,0006341X,,Mar-18,74,1,49,9,128572256,10.1111/biom.12738,Wiley-Blackwell,Article,"CLINICAL trials; STATISTICAL methods in clinical trials; CLINICAL trials monitoring; ANALYSIS of covariance; RANDOMIZATION (Statistics); Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Adaptive designs; CARA randomization; Ethics; Multi‐armed bandit; Sequential allocation,"Summary: We introduce a non‐myopic, covariate‐adjusted response adaptive (CARA) allocation design for multi‐armed clinical trials. The allocation scheme is a computationally tractable procedure based on the Gittins index solution to the classic multi‐armed bandit problem and extends the procedure recently proposed in Villar et al. (2015). Our proposed CARA randomization procedure is defined by reformulating the bandit problem with covariates into a classic bandit problem in which there are multiple combination arms, considering every arm per each covariate category as a distinct treatment arm. We then apply a heuristically modified Gittins index rule to solve the problem and define allocation probabilities from the resulting solution. We report the efficiency, balance, and ethical performance of our approach compared to existing CARA methods using a recently published clinical trial as motivation. The net savings in terms of expected number of treatment failures is considerably larger and probably enough to make this design attractive for certain studies where known covariates are expected to be important, stratification is not desired, treatment failures have a high ethical cost, and the disease under study is rare. In a two‐armed context, this patient benefit advantage comes at the expense of increased variability in the allocation proportions and a reduction in statistical power. However, in a multi‐armed context, simple modifications of the proposed CARA rule can be incorporated so that an ethical advantage can be offered without sacrificing power in comparison with balanced designs. [ABSTRACT FROM AUTHOR] Copyright of Biometrics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128572256&site=ehost-live
526,ERDO - a framework to select an appropriate randomization procedure for clinical trials.,William Rosenberger,BMC Medical Research Methodology,14712288,,12/4/17,17,,1,12,126974496,10.1186/s12874-017-0428-z,BioMed Central,journal article,"RANDOMIZED controlled trials; CLINICAL medicine research; CLINICAL trials; EXTENSIONS; PHARMACEUTICAL industry; ALGORITHMS; CASE studies; RESEARCH bias; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Drugs and Druggists' Sundries Merchant Wholesalers; Pharmaceuticals and pharmacy supplies merchant wholesalers; Pharmaceutical and medicine manufacturing; Pharmaceutical Preparation Manufacturing",Chronological bias; Design; Restricted randomization; Selection bias; Type I error probability,"<bold>Background: </bold>Randomization is considered to be a key feature to protect against bias in randomized clinical trials. Randomization induces comparability with respect to known and unknown covariates, mitigates selection bias, and provides a basis for inference. Although various randomization procedures have been proposed, no single procedure performs uniformly best. In the design phase of a clinical trial, the scientist has to decide which randomization procedure to use, taking into account the practical setting of the trial with respect to the potential of bias. Less emphasis has been placed on this important design decision than on analysis, and less support has been available to guide the scientist in making this decision.<bold>Methods: </bold>We propose a framework that weights the properties of the randomization procedure with respect to practical needs of the research question to be answered by the clinical trial. In particular, the framework assesses the impact of chronological and selection bias on the probability of a type I error. The framework is applied to a case study with a 2-arm parallel group, single center randomized clinical trial with continuous endpoint, with no-interim analysis, 1:1 allocation and no adaptation in the randomization process.<bold>Results: </bold>In so doing, we derive scientific arguments for the selection of an appropriate randomization procedure and develop a template which is illustrated in parallel by a case study. Possible extensions are discussed.<bold>Conclusion: </bold>The proposed ERDO framework guides the investigator through a template for the choice of a randomization procedure, and provides easy to use tools for the assessment. The barriers for the thorough reporting and assessment of randomization procedures could be further reduced in the future when regulators and pharmaceutical companies employ similar, standardized frameworks for the choice of a randomization procedure. [ABSTRACT FROM AUTHOR] Copyright of BMC Medical Research Methodology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126974496&site=ehost-live
527,Exact optimum coin bias in Efron's randomization procedure.,William Rosenberger,Statistics in Medicine,2776715,,Dec-15,34,28,3760,9,110589967,10.1002/sim.6576,"John Wiley & Sons, Inc.",journal article,,accidental bias; biased coin design; compound optimality; restricted randomization; selection bias,"Efron's biased coin design is a restricted randomization procedure that has very favorable balancing properties, yet it is fully randomized, in that subjects are always randomized to one of two treatments with a probability less than 1. The parameter of interest is the bias p of the coin, which can range from 0.5 to 1. In this note, we propose a compound optimization strategy that selects p based on a subjected weighting of the relative importance of the two fundamental criteria of interest for restricted randomization mechanisms, namely balance between the treatment assignments and allocation randomness. We use exact and asymptotic distributional properties of Efron's coin to find the optimal p under compound criteria involving imbalance variability, expected imbalance, selection bias, and accidental bias, for both small/moderate trials and large samples. [ABSTRACT FROM AUTHOR] Copyright of Statistics in Medicine is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=110589967&site=ehost-live
528,Implementing Optimal Allocation in Sequential Binary Response Experiments.,William Rosenberger,Journal of the American Statistical Association,1621459,,Mar-07,102,477,224,11,24253392,10.1198/016214506000000906,Taylor & Francis Ltd,Article,TECHNICAL specifications; SAMPLE size (Statistics); STATISTICAL sampling; MULTIVARIATE analysis; MATHEMATICAL optimization; HOMOGENEITY; Marketing Research and Public Opinion Polling,Clinical trials; Doubly adaptive biased coin design; Neyman allocation; Power; Response-adaptive randomization,"For sequential experiments with K treatments, we establish two formal optimization criteria to find optimal allocation strategies. Both criteria involve the sample sizes on each treatment and a concave noncentrality parameter from a multivariate test. We show that these two criteria are equivalent. We apply this result to specific questions: (1) How do we maximize power of a multivariate test of homogeneity with binary, response?, and (2) for fixed power, how do we minimize expected treatment failures? Because the solutions depend on unknown parameters, we describe a response-adaptive randomization procedure that ""targets"" the optimal allocation and provides increases in power along the lines of 2-4% over complete randomization for equal allocation. The increase in power contradicts the conclusions of other authors who have explored other randomization procedures for K = 2 and have found that the variability induced by randomization negates any benefit of targeting an optimal allocation. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Statistical Association is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24253392&site=ehost-live
529,Inference for Blocked Randomization under a Selection Bias Model.,William Rosenberger,Biometrics,0006341X,,Dec-15,71,4,979,6,112037769,10.1111/biom.12334,Wiley-Blackwell,Article,QUANTITATIVE research; RANDOMIZED controlled trials; CLINICAL medicine research; INVESTIGATIONAL therapies; CHI-squared test,Confidence intervals; Likelihood ratio test; Permuted block design; Restricted randomization,"We provide an asymptotic test to analyze randomized clinical trials that may be subject to selection bias. For normally distributed responses, and under permuted block randomization, we derive a likelihood ratio test of the treatment effect under a selection bias model. A likelihood ratio test of the presence of selection bias arises from the same formulation. We prove that the test is asymptotically chi-square on one degree of freedom. These results correlate well with the likelihood ratio test of Ivanova et al. (2005, Statistics in Medicine 24, 1537-1546) for binary responses, for which they established by simulation that the asymptotic distribution is chi-square. Simulations also show that the test is robust to departures from normality and under another randomization procedure. We illustrate the test by reanalyzing a clinical trial on retinal detachment. [ABSTRACT FROM AUTHOR] Copyright of Biometrics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=112037769&site=ehost-live
530,On asymptotic normality of the randomization-based logrank test.,William Rosenberger,Journal of Nonparametric Statistics,10485252,,Oct-05,17,7,833,7,19082909,10.1080/10485250500270826,Taylor & Francis Ltd,Article,NONPARAMETRIC statistics; STATISTICS; RANDOM variables; MATHEMATICAL statistics; ESTIMATION theory; MATHEMATICS,Censored data: Linear rank test; Permutation test; Survival analysis,"In this article, we explore conditions under which the nonparametric randomization-based version of the logrank test is asymptotically normal for a wide class of randomization procedures. [ABSTRACT FROM AUTHOR] Copyright of Journal of Nonparametric Statistics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=19082909&site=ehost-live
531,On the use of randomization tests following adaptive designs.,William Rosenberger,Journal of Biopharmaceutical Statistics,10543406,,2016,26,3,466,9,114928004,10.1080/10543406.2015.1052486,Taylor & Francis Ltd,Article,"RANDOMIZATION (Statistics); CLINICAL trials; MONTE Carlo method; PERMUTATIONS; ASYMPTOTIC distribution; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Clinical trials; Monte Carlo distribution; response-adaptive randomization; restricted randomization,"Randomization tests (sometimes referred to as “re-randomization” tests) are used in clinical trials, either as an assumption-free confirmation of parametric analyses, or as an independent analysis based on the principle of randomization-based inference. In the context of adaptive randomization, either restricted or response-adaptive procedures, it is unclear how accurate such Monte Carlo approximations are, or how many Monte Carlo sequences to generate. In this paper, we describe several randomization procedures for which there is a known exact or asymptotic distribution of the randomization test. For a special class of procedures, called, and binary responses, the exact test statistic has a simple closed form. For the limited subset of existing procedures with known exact and asymptotic distributions, we can use these as a benchmark for the accuracy of Monte Carlo randomization techniques. We conclude that Monte Carlo tests are very accurate, and require minimal computation time. For simple tests with binary response in the class ofprocedures, the exact distribution provides the best test, but Monte Carlo approximations can be used when the exact distribution is difficult to compute. [ABSTRACT FROM PUBLISHER] Copyright of Journal of Biopharmaceutical Statistics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114928004&site=ehost-live
532,Random norming aids analysis of non-linear regression models with sequential informative dose selection.,William Rosenberger,Journal of Statistical Planning & Inference,3783758,,May-20,206,,29,14,140090766,10.1016/j.jspi.2019.09.003,Elsevier B.V.,Article,REGRESSION analysis; RANDOM measures; NONLINEAR regression; INFORMATION measurement; STOCHASTIC processes; FISHER information,Adaptive optimal designs; Generalized Cramér–Slutsky theorem; Inference for stochastic processes; Random information measures; Stable convergence,"A two-stage adaptive optimal design is an attractive option for increasing the efficiency of clinical trials. In these designs, based on interim data, the locally optimal dose is chosen for further exploration, which induces dependencies between data from the two stages. When the maximum likelihood estimator (MLE) is used under nonlinear regression models with independent normal errors in a pilot study where the first stage sample size is fixed, and the second stage sample size is large, the Fisher information fails to normalize the estimator adequately asymptotically, because of dependencies. In this situation, three alternative random information measures are presented and are shown to provide better normalization of the MLE asymptotically. The performance of random information measures is investigated in simulation studies, and the results suggest that the observed information performs best when the sample size is small. [ABSTRACT FROM AUTHOR] Copyright of Journal of Statistical Planning & Inference is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140090766&site=ehost-live
533,Randomization tests for multiarmed randomized clinical trials.,William Rosenberger,Statistics in Medicine,2776715,,2/20/20,39,4,494,16,141152910,10.1002/sim.8418,"John Wiley & Sons, Inc.",journal article,"CLINICAL trials; FALSE positive error; MONTE Carlo method; STATISTICAL power analysis; ERROR rates; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",generalized randomization procedures; Monte Carlo rerandomization test; multiple treatment comparison; randomization‐based inference,"We examine the use of randomization-based inference for analyzing multiarmed randomized clinical trials, including the application of conditional randomization tests to multiple comparisons. The view is taken that the linkage of the statistical test to the experimental design (randomization procedure) should be recognized. A selected collection of randomization procedures generalized to multiarmed treatment allocation is summarized, and generalizations for two randomization procedures that heretofore were designed for only two treatments are developed. We explain the process of computing the randomization test and conditional randomization test via Monte Carlo simulation, developing an efficient algorithm that makes multiple comparisons possible that would not be possible using a standard algorithm, demonstrate the preservation of type I error rate, and explore the relationship of statistical power to the randomization procedure in the presence of a time trend and outliers. We distinguish between the interpretation of the p-value in the randomization test and in the population test and verify that the randomization test can be approximated by the population test on some occasions. Data from two multiarmed clinical trials from the literature are reanalyzed to illustrate the methodology. [ABSTRACT FROM AUTHOR] Copyright of Statistics in Medicine is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141152910&site=ehost-live
534,Randomization: The forgotten component of the randomized clinical trial.,William Rosenberger,Statistics in Medicine,2776715,,1/15/19,38,1,1,12,133464636,10.1002/sim.7901,"John Wiley & Sons, Inc.",journal article,"CLINICAL trials; STATISTICAL sampling; STATISTICS; DATA analysis; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Marketing Research and Public Opinion Polling",history of randomization; randomization as a basis for inference; randomization tests,"""…The customary test for an observed difference…is based on an enumeration of the probabilities, on the initial hypothesis that two treatments do not differ in their effects,…of all the various results which would occur if the trial were repeated indefinitely with different random samples of the same size as those actually used."" -Peter Armitage (""Sequential tests in prophylactic and therapeutic trials"" in Quarterly Journal of Medicine, 1954;23(91):255-274). Randomization has been the hallmark of the clinical trial since Sir Bradford Hill adopted it in the 1946 streptomycin trial. An exploration of the early literature yields three rationales, ie, (i) the incorporation of randomization provides unpredictability in treatment assignments, thereby mitigating selection bias; (ii) randomization tends to ensure similarity in the treatment groups on known and unknown confounders (at least asymptotically); and (iii) the act of randomization itself provides a basis for inference when random sampling is not conducted from a population model. Of these three, rationale (iii) is often forgotten, ignored, or left untaught. Today, randomization is a rote exercise, scarcely considered in protocols or medical journal articles. Yet, the literature of the last century is rich with statistical articles on randomization methods and their consequences, authored by some of the pioneers of the biostatistics and statistics world. In this paper, we review some of this literature and describe very simple methods to rectify some of the oversight. We describe how randomization-based inference can be used for virtually any outcome of interest in a clinical trial. Special mention is made of nonstandard clinical trials situations. [ABSTRACT FROM AUTHOR] Copyright of Statistics in Medicine is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133464636&site=ehost-live
535,Randomization-based interval estimation in randomized clinical trials.,William Rosenberger,Statistics in Medicine,2776715,,9/20/20,39,21,2843,12,145008411,10.1002/sim.8577,"John Wiley & Sons, Inc.",journal article,"MONTE Carlo method; CLINICAL trials; CONFIDENCE intervals; DEFINITIONS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",bisection method; interval estimation; Monte Carlo re‐randomization test; randomization‐based inference; Robbins‐Monro algorithm,"Randomization-based interval estimation takes into account the particular randomization procedure in the analysis and preserves the confidence level even in the presence of heterogeneity. It is distinguished from population-based confidence intervals with respect to three aspects: definition, computation, and interpretation. The article contributes to the discussion of how to construct a confidence interval for a treatment difference from randomization tests when analyzing data from randomized clinical trials. The discussion covers (i) the definition of a confidence interval for a treatment difference in randomization-based inference, (ii) computational algorithms for efficiently approximating the endpoints of an interval, and (iii) evaluation of statistical properties (ie, coverage probability and interval length) of randomization-based and population-based confidence intervals under a selected set of randomization procedures when assuming heterogeneity in patient outcomes. The method is illustrated with a case study. [ABSTRACT FROM AUTHOR] Copyright of Statistics in Medicine is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145008411&site=ehost-live
536,Rejoinder.,William Rosenberger,Statistics in Medicine,2776715,,1/15/19,38,1,27,4,133464650,10.1002/sim.7978,"John Wiley & Sons, Inc.",journal article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133464650&site=ehost-live
537,Response-Adaptive Randomization for Clinical Trials with Continuous Outcomes.,William Rosenberger,Biometrics,0006341X,,Jun-06,62,2,562,8,21194519,10.1111/j.1541-0420.2005.00496.x,Wiley-Blackwell,Article,"CLINICAL trials; ASYMPTOTIC expansions; CAUCHY problem; CLINICAL medicine research; MEDICAL research; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Delayed responses; Doubly adaptive biased coin design; Power,"We provide an explicit asymptotic method to evaluate the performance of different response-adaptive randomization procedures in clinical trials with continuous outcomes. We use this method to investigate four different response-adaptive randomization procedures. Their performance, especially in power and treatment assignment skewing to the better treatment, is thoroughly evaluated theoretically. These results are then verified by simulation. Our analysis concludes that the doubly adaptive biased coin design procedure targeting optimal allocation is the best one for practical use. We also consider the effect of delay in responses and nonstandard responses, for example, Cauchy distributed response. We illustrate our procedure by redesigning a real clinical trial. [ABSTRACT FROM AUTHOR] Copyright of Biometrics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=21194519&site=ehost-live
538,Sequential design and analysis in the randomized clinical trial: A historical perspective.,William Rosenberger,Sequential Analysis,7474946,,2020,39,3,295,12,147965378,10.1080/07474946.2020.1823190,Taylor & Francis Ltd,Article,"CLINICAL trials; SEQUENTIAL analysis; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",62L10; Group sequential monitoring; Primary 62-03; randomized clinical trials; Secondary 62L05; 62P10; sequential procedures,"Sequential analysis, as invented by Wald, was not targeted to clinical trials; in fact, the randomized clinical trial was being invented nearly simultaneously by Hill on the other side of the Atlantic. The connection to clinical trials was established by Bross and Armitage in the early 1950s and applied in a number of trials during that decade. Restrictions in its applicability in practice led to a dry spell until its resurgence with group sequential methods. This article reviews the historical context of the use of sequential analysis in actual randomized clinical trials. It does not review methodological developments, except as they relate to the historical and philosophical setting. [ABSTRACT FROM AUTHOR] Copyright of Sequential Analysis is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147965378&site=ehost-live
539,Sequential Monitoring of Conditional Randomization Tests: Generalized Biased Coin Designs.,William Rosenberger,Sequential Analysis,7474946,,Sep-08,27,3,234,20,33649849,10.1080/07474940802240969,Taylor & Francis Ltd,Article,"ASYMPTOTIC distribution; CLINICAL trials; DISTRIBUTION (Probability theory); RANDOM variables; STATISTICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Conditional information fraction; Joint asymptotic normality; Linear rank test; Spending functions,"For the generalized biased coin class of randomization procedures, Smythe (1988) proved asymptotic normality of the conditional linear rank test. Clinical trialists often undertake interim analysis to determine whether to stop the trial early for a substantial treatment effect. In this article, we will set up one interim analysis using a conditional randomization test. The joint asymptotic distribution of the interim test statistic and the final test statistic will be explored. We also define the concept of conditional information under a randomization model. [ABSTRACT FROM AUTHOR] Copyright of Sequential Analysis is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=33649849&site=ehost-live
540,Sequential Monitoring of Randomization Tests: Stratified Randomization.,William Rosenberger,Biometrics,0006341X,,Sep-07,63,3,865,8,26421003,10.1111/j.1541-0420.2006.00735.x,Wiley-Blackwell,Article,"BLOCK designs; CLINICAL trials; PERMUTATIONS; EXPERIMENTAL design; MATHEMATICAL sequences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Information fraction; Linear rank tests; Permutation tests; Permuted block design; Stratified block design; Stratified urn design,"In many clinical trials, it is desirable to establish a sequential monitoring plan, whereby the test statistic is computed at an interim point or points in the trial and a decision is made whether to stop early due to evidence of treatment efficacy. In this article, we will set up a sequential monitoring plan for randomization-based inference under the permuted block design, stratified block design, and stratified urn design. We will also propose a definition of information fraction in these settings and discuss its calculation under these different designs. [ABSTRACT FROM AUTHOR] Copyright of Biometrics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26421003&site=ehost-live
541,Sociodemographic disparities in corticolimbic structures.,William Rosenberger,PLoS ONE,19326203,,5/9/19,14,5,1,21,136361674,10.1371/journal.pone.0216338,Public Library of Science,Article,CINGULATE cortex; PREFRONTAL cortex; SOCIAL status; RACE; HEALTH equity; CENTRAL nervous system,Amygdala; Anatomy; Biology and life sciences; Brain; Central nervous system; Emotions; Health care; Hippocampus; Medicine and health sciences; Mental health and psychiatry; Nervous system; Prefrontal cortex; Psychological stress; Psychology; Public and occupational health; Research Article; Social sciences; Social stratification; Socioeconomic aspects of health; Sociology,"This study sought to examine the interactive relations of socioeconomic status and race to corticolimbic regions that may play a key role in translating stress to the poor health outcomes overrepresented among those of lower socioeconomic status and African American race. Participants were 200 community-dwelling, self-identified African American and White adults from the Healthy Aging in Neighborhoods of Diversity across the Life Span SCAN study. Brain volumes were derived using T1-weighted MP-RAGE images. Socioeconomic status by race interactions were observed for right medial prefrontal cortex (B = .26, p = .014), left medial prefrontal cortex (B = .26, p = .017), left orbital prefrontal cortex (B = .22, p = .037), and left anterior cingulate cortex (B = .27, p = .018), wherein higher socioeconomic status Whites had greater volumes than all other groups. Additionally, higher versus lower socioeconomic status persons had greater right and left hippocampal (B = -.15, p = .030; B = -.19, p = .004, respectively) and amygdalar (B = -.17, p = .015; B = -.21; p = .002, respectively) volumes. Whites had greater right and left hippocampal (B = -.17, p = .012; B = -.20, p = .003, respectively), right orbital prefrontal cortex (B = -.34, p < 0.001), and right anterior cingulate cortex (B = -.18, p = 0.011) volumes than African Americans. Among many factors, the higher levels of lifetime chronic stress associated with lower socioeconomic status and African American race may adversely affect corticolimbic circuitry. These relations may help explain race- and socioeconomic status-related disparities in adverse health outcomes. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136361674&site=ehost-live
542,Adaptive randomization for balancing over covariates.,William Rosenberger,WIREs: Computational Statistics,19395108,,Jul-14,6,4,288,16,96797938,10.1002/wics.1309,Wiley-Blackwell,Article,"CLINICAL trials; INDIVIDUALIZED medicine; RANDOMIZATION (Statistics); EUROPEAN Medicines Agency; INTERNATIONAL Conference on Harmonisation; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",clinical trials; covariate‐adaptive randomization; covariate-adaptive randomization; dynamic allocation; inference,"In controlled clinical trials, balanced allocation over covariates is often viewed as an essential component in ensuring valid treatment comparisons. Minimization, sometimes called 'dynamic allocation', or 'covariate-adaptive randomization' has an advantage over stratified randomization, in that it is able to achieve balance over a large number of covariates when the sample size is small to medium. Despite its effectiveness, minimization has been questioned by regulatory agencies, mainly because of its increased complexity in practice and its potential impact on subsequent analysis. In recent years, however, with developments in clinical trials information technology, as well as advances in statistical theory, the attitudes toward minimization have evolved. In its 2013 draft guidelines, the European Medicines Agency (EMA) provided instructive guidelines for the implementation of minimization. In this paper we review the broad class of methods that belong to minimization, including its original forms for balancing over covariate margins and its generalization to balancing over other subgroups of interest or over continuous covariates. Moreover, we review the theoretical development in recent years, including the large-sample properties of balance under minimization, the impact of minimization on inference for different data types, and on suitable randomization tests. WIREs Comput Stat 2014, 6:288-303. doi: 10.1002/wics.1309 For further resources related to this article, please visit the . Conflict of interest: The authors have declared no conflicts of interest for this article. [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96797938&site=ehost-live
543,Convergence properties of sequential Bayesian D-optimal designs,William Rosenberger,Journal of Statistical Planning & Inference,3783758,,Feb-09,139,2,425,16,35072266,10.1016/j.jspi.2008.04.025,Elsevier B.V.,Article,"STOCHASTIC convergence; BAYESIAN analysis; OPTIMAL designs (Statistics); ASYMPTOTES; ESTIMATION theory; CLINICAL trials; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Adaptive designs; Asymptotic normality; Discrete optimal design; Dose–response; Posterior convergence,"Abstract: We establish convergence properties of sequential Bayesian optimal designs. In particular, for sequential D-optimality under a general nonlinear location-scale model for binary experiments, we establish posterior consistency, consistency of the design measure, and the asymptotic normality of posterior following the design. We illustrate our results in the context of a particular application in the design of phase I clinical trials, namely a sequential design of Haines et al. [2003. Bayesian optimal designs for phase I clinical trials. Biometrics 59, 591–600] that incorporates an ethical constraint on overdosing. [Copyright &y& Elsevier] Copyright of Journal of Statistical Planning & Inference is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35072266&site=ehost-live
544,Design and analysis of stratified clinical trials in the presence of bias.,William Rosenberger,Statistical Methods in Medical Research,9622802,,Jun-20,29,6,1715,13,143590650,10.1177/0962280219846146,Sage Publications Inc.,journal article,"CLINICAL trials; ERROR probability; FALSE positive error; DISCRIMINATION (Sociology); Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Multi-center clinical trial; sample size; selection bias; stratified randomization; time-trend bias; type I error probability; weighted t test,"<bold>Background: </bold>Among various design aspects, the choice of randomization procedure have to be agreed on, when planning a clinical trial stratified by center. The aim of the paper is to present a methodological approach to evaluate whether a randomization procedure mitigates the impact of bias on the test decision in clinical trial stratified by center.<bold>Methods: </bold>We use the weighted t test to analyze the data from a clinical trial stratified by center with a two-arm parallel group design, an intended 1:1 allocation ratio, aiming to prove a superiority hypothesis with a continuous normal endpoint without interim analysis and no adaptation in the randomization process. The derivation is based on the weighted t test under misclassification, i.e. ignoring bias. An additive bias model combing selection bias and time-trend bias is linked to different stratified randomization procedures.<bold>Results: </bold>Various aspects to formulate stratified versions of randomization procedures are discussed. A formula for sample size calculation of the weighted t test is derived and used to specify the tolerated imbalance allowed by some randomization procedures. The distribution of the weighted t test under misclassification is deduced, taking the sequence of patient allocation to treatment, i.e. the randomization sequence into account. An additive bias model combining selection bias and time-trend bias at strata level linked to the applied randomization sequence is proposed. With these before mentioned components, the potential impact of bias on the type one error probability depending on the selected randomization sequence and thus the randomization procedure is formally derived and exemplarily calculated within a numerical evaluation study.<bold>Conclusion: </bold>The proposed biasing policy and test distribution are necessary to conduct an evaluation of the comparative performance of (stratified) randomization procedure in multi-center clinical trials with a two-arm parallel group design. It enables the choice of the best practice procedure. The evaluation stimulates the discussion about the level of evidence resulting in those kind of clinical trials. [ABSTRACT FROM AUTHOR] Copyright of Statistical Methods in Medical Research is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143590650&site=ehost-live
545,"Discussion on ""Second-Guessing Clinical Trial Designs"" by Jonathan J. Shuster and Myron N. Chang.",William Rosenberger,Sequential Analysis,7474946,,Jan-08,27,1,24,2,28805771,10.1080/07474940701801788,Taylor & Francis Ltd,Article,"CLINICAL trials; EXPERIMENTAL design; MEDICAL research; CLINICAL medicine; SEQUENTIAL analysis; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Data and safety monitoring board; Redesign; Sequential monitoring,"Redesigning previously conducted clinical trials is a useful tool for better determining the impact of an alternative design. Shuster and Chang suggest the redesign of clinical trials that do not use sequential monitoring by imposing a sequential monitoring structure and determining what would have happened had such a structure been employed. However, early stopping decisions are complex and involve are intimate understanding of ethical, logistical, and statistical issues. ""Second-guessing"" a data and safety monitoring board is dangerous without this intimate understanding. [ABSTRACT FROM AUTHOR] Copyright of Sequential Analysis is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=28805771&site=ehost-live
546,Fasting glucose and glucose tolerance as potential predictors of neurocognitive function among nondiabetic older adults.,William Rosenberger,Journal of Clinical & Experimental Neuropsychology,13803395,,Feb-15,37,1,49,12,101449455,10.1080/13803395.2014.985189,Taylor & Francis Ltd,Article,COGNITION disorders diagnosis; TYPE 2 diabetes; PHYSIOLOGICAL effects of glucose; DISEASE diagnosis in older people; GLUCOSE tolerance tests,Impaired fasting glucose; Impaired glucose tolerance; Neurocognitive function; Older adults; Type 2 diabetes,"Introduction: Significant evidence has demonstrated that Type 2 diabetes mellitus and related precursors are associated with diminished neurocognitive function and risk of dementia among older adults. However, very little research has examined relations of glucose regulation to neurocognitive function among older adults free of these conditions. The primary aim of this investigation was to examine associations among fasting glucose, glucose tolerance, and neurocognitive function among nondiabetic older adults. The secondary aim was to examine age, gender, and education as potential effect modifiers. Method: The study employed a cross-sectional, correlational study design. Participants were 172 older adults with a mean age of 64.43 years (SD = 13.09). The sample was 58% male and 87% White. Participants completed an oral glucose tolerance test as part of a larger study. Trained psychometricians administered neuropsychological tests that assessed performance in the domains of response inhibition, nonverbal memory, verbal memory, attention and working memory, visuoconstructional abilities, visuospatial abilities, psychomotor speed and executive function, and motor speed and manual dexterity. Linear multiple regressions were run to test study aims. Results: No significant main effects of fasting glucose and 2-hour glucose emerged for performance on any neurocognitive test; however, significant interactions were present. Higher fasting glucose was associated with poorer short-term verbal memory performance among men, but unexpectedly better response inhibition and long-term verbal memory performance for participants over age 70. Higher 2-hour glucose values were associated with reduced divided attention performance among participants with less than a high school education. Conclusions: Mixed findings suggest that glucose levels may be both beneficial and deleterious to neurocognition among nondiabetic older adults. Additional studies with healthy older adults are needed to confirm this unexpected pattern of associations; however, findings have implications for the importance of maintaining healthy glucose levels in older adulthood. [ABSTRACT FROM AUTHOR] Copyright of Journal of Clinical & Experimental Neuropsychology is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101449455&site=ehost-live
547,Statistical Concepts and Applications in Clinical Medicine.,William Rosenberger,Journal of the American Statistical Association,1621459,,Mar-06,101,473,404,0,20482020,10.1198/jasa.2006.s91,Taylor & Francis Ltd,Book Review,"CLINICAL medicine; NONFICTION; STATISTICAL Concepts & Applications in Clinical Medicine (Book); AITCHISON, John; KAY, Jim W.; LAUDER, J.",,"This article reviews the book ""Statistical Concepts and Applications in Clinical Medicine,"" by John Aitchison, Jim W. Kay and Ian J. Lauder.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20482020&site=ehost-live
548,The Temptation of Overgeneralizing Response-adaptive Randomization.,William Rosenberger,Clinical Infectious Diseases,10584838,,Aug-21,73,3,e842,1,151699006,10.1093/cid/ciaa1027,Oxford University Press / USA,Article,RESEARCH methodology; HEALTH outcome assessment; EXTRACORPOREAL membrane oxygenation; RANDOMIZED controlled trials,,The article highlights study by Proschan and Evans on the use of response-adaptive randomization (RAR) and its potential problems. It discusses that volatility in sample size distributions that can cause a nontrivial proportion of trials to assign more patients to an inferior arm; and mentions that Proschan and Evans use the infamous Extra Corporeal Membrane Oxygenation (ECMO) trial as the key example to discourage the use of RAR in practice.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151699006&site=ehost-live
549,Time-Frequency Analysis of Increases in Vaginal Blood Perfusion Elicited by Long-Duration Pudendal Neuromodulation in Anesthetized Rats.,Shani Ross,Neuromodulation,10947159,,Dec-17,20,8,807,8,126656100,10.1111/ner.12707,Wiley-Blackwell,Article,BLOOD flow; SEXUAL dysfunction; SEXUAL excitement; WOMEN'S sexual behavior; PUDENDAL nerve; LABORATORY rats; THERAPEUTICS; All Other Animal Production,blood flow; internal organ function; peripheral nerve stimulation; pudendal nerve stimulation; sexual function,"Objectives Female sexual dysfunction (FSD) affects a significant portion of the population. Although treatment options for FSD are limited, neuromodulation for bladder dysfunction has improved sexual function in some women. A few studies have investigated peripheral neuromodulation for eliciting changes in vaginal blood flow, as a proxy for modulating genital sexual arousal, however results are generally transient. Our central hypothesis is that repeated or extended-duration pudendal nerve stimulation can elicit maintained vaginal blood flow increases. Materials and Methods Under ketamine anesthesia, the pudendal nerve of 14 female rats was stimulated at varying frequencies (1-100 Hz) and durations (0.15-60 min). Vaginal blood perfusion was measured with a laser Doppler flowmetry probe. Changes in blood perfusion were determined through raw signal analysis and increases in the energy of neurogenic (0.076-0.200 Hz) and myogenic (0.200-0.740 Hz) frequency bands through wavelet analysis. Additionally, a convolution model was developed for a carry-over stimulation effect. Results Each experiment had significant increases in vaginal blood perfusion due to pudendal nerve stimulation. In addition, there were large concurrent increases in neurogenic and myogenic frequency-band energy in 11/14 experiments, with an average maximal response at 31.3 min after stimulation initiation. An effective stimulation model with a 30-min carry-over effect had a stronger correlation to blood perfusion than the stimulation period itself. Conclusions Repeated or extended-duration pudendal nerve stimulation can elicit maintained increases in vaginal blood perfusion. This work indicates the potential for pudendal neuromodulation as a method for increasing genital arousal as a potential treatment for FSD. [ABSTRACT FROM AUTHOR] Copyright of Neuromodulation is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126656100&site=ehost-live
550,GKMPAN: An Efficient Group Rekeying Scheme for Secure Multicast in Ad-Hoc Networks.,Sanjeev Setia,Journal of Computer Security,0926227X,,2006,14,4,301,25,22976131,10.3233/JCS-2006-14401,IOS Press,Article,COMPUTER networks; SENSOR networks; DETECTORS; NETWORK routers; PROBABILISTIC number theory; PROBABILISTIC automata; PHASE shift keying; COMPUTER network protocols; Computer Systems Design Services,ad hoc networks; Group key management; key updating; probabilistic key sharing; stateless,"We present GKMPAN, an efficient and scalable group rekeying protocol for secure multicast in ad hoc networks. Our protocol exploits the property of ad hoc networks that each member of a group is both a host and a router, and distributes the group key to member nodes via a secure hop-by-hop propagation scheme. A probabilistic scheme based on pre-deployed symmetric keys is used for implementing secure channels between members for group key distribution. GKMPAN also includes a novel distributed scheme for efficiently updating the pre-deployed keys. GKMPAN has three attractive properties. First, it is significantly more efficient than group rekeying schemes that were adapted from those proposed for wired networks. Second, GKMPAN has the property of partial statelessness; that is, a node can decode the current group key even if it has missed a certain number of previous group rekeying operations. This makes it very attractive for ad hoc networks where nodes may lose packets due to transmission link errors or temporary network partitions. Third, in GKMPAN the key server does not need any information about the topology of the ad hoc network or the geographic location of the members of the group. We study the security and performance of GKMPAN through detailed analysis and simulation; we have also implemented GKMPAN in a sensor network testbed. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computer Security is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22976131&site=ehost-live
551,A Data-Driven Evolutionary Algorithm for Mapping Multibasin Protein Energy Landscapes.,Amarda Shehu,Journal of Computational Biology,10665277,,Sep-15,22,9,844,17,109251698,10.1089/cmb.2015.0107,"Mary Ann Liebert, Inc.",Article,"EVOLUTIONARY algorithms; DIMENSION reduction (Statistics); PROTEIN structure; MULTISCALE modeling; COMPUTATIONAL biology; Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",dimensionality reduction; evolutionary algorithm; multibasin energy landscape; multiscale modeling; protein structure modeling,"Evidence is emerging that many proteins involved in proteinopathies are dynamic molecules switching between stable and semistable structures to modulate their function. A detailed understanding of the relationship between structure and function in such molecules demands a comprehensive characterization of their conformation space. Currently, only stochastic optimization methods are capable of exploring conformation spaces to obtain sample-based representations of associated energy surfaces. These methods have to address the fundamental but challenging issue of balancing computational resources between exploration (obtaining a broad view of the space) and exploitation (going deep in the energy surface). We propose a novel algorithm that strikes an effective balance by employing concepts from evolutionary computation. The algorithm leverages deposited crystal structures of wildtype and variant sequences of a protein to define a reduced, low-dimensional search space from where to rapidly draw samples. A multiscale technique maps samples to local minima of the all-atom energy surface of a protein under investigation. Several novel algorithmic strategies are employed to avoid premature convergence to particular minima and obtain a broad view of a possibly multibasin energy surface. Analysis of applications on different proteins demonstrates the broad utility of the algorithm to map multibasin energy landscapes and advance modeling of multibasin proteins. In particular, applications on wildtype and variant sequences of proteins involved in proteinopathies demonstrate that the algorithm makes an important first step toward understanding the impact of sequence mutations on misfunction by providing the energy landscape as the intermediate explanatory link between protein sequence and function. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251698&site=ehost-live
553,Balancing multiple objectives in conformation sampling to control decoy diversity in template-free protein structure prediction.,Amarda Shehu,BMC Bioinformatics,14712105,,4/25/19,20,1,1,17,136097840,10.1186/s12859-019-2794-5,BioMed Central,Article,PROTEIN structure; AMINO acids; ATOMIC interactions; PROTEIN conformation; ALGORITHMS,Protein energy landscape; Stochastic optimization; Structural dynamics,"Background: Computational approaches for the determination of biologically-active/native three-dimensional structures of proteins with novel sequences have to handle several challenges. The (conformation) space of possible three-dimensional spatial arrangements of the chain of amino acids that constitute a protein molecule is vast and high-dimensional. Exploration of the conformation spaces is performed in a sampling-based manner and is biased by the internal energy that sums atomic interactions. Even state-of-the-art energy functions that quantify such interactions are inherently inaccurate and associate with protein conformation spaces overly rugged energy surfaces riddled with artifact local minima. The response to these challenges in template-free protein structure prediction is to generate large numbers of low-energy conformations (also referred to as decoys) as a way of increasing the likelihood of having a diverse decoy dataset that covers a sufficient number of local minima possibly housing near-native conformations. Results: In this paper we pursue a complementary approach and propose to directly control the diversity of generated decoys. Inspired by hard optimization problems in high-dimensional and non-linear variable spaces, we propose that conformation sampling for decoy generation is more naturally framed as a multi-objective optimization problem. We demonstrate that mechanisms inherent to evolutionary search techniques facilitate such framing and allow balancing multiple objectives in protein conformation sampling. We showcase here an operationalization of this idea via a novel evolutionary algorithm that has high exploration capability and is also able to access lower-energy regions of the energy landscape of a given protein with similar or better proximity to the known native structure than several state-of-the-art decoy generation algorithms. Conclusions: The presented results constitute a promising research direction in improving decoy generation for template-free protein structure prediction with regards to balancing of multiple conflicting objectives under an optimization framework. Future work will consider additional optimization objectives and variants of improvement and selection operators to apportion a fixed computational budget. Of particular interest are directions of research that attenuate dependence on protein energy models. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136097840&site=ehost-live
554,Building maps of protein structure spaces in template-free protein structure prediction.,Amarda Shehu,Journal of Bioinformatics & Computational Biology,2197200,,Dec-19,17,6,N.PAG,17,141546480,10.1142/S0219720019400134,World Scientific Publishing Company,Article,PROTEIN structure; SPACE frame structures; ALGORITHMS; AMINO acid sequence; TERTIARY structure,decoy generation; decoy quality; evolutionary algorithm; map of protein structure space; Template-free protein structure prediction,"An important goal in template-free protein structure prediction is how to control the quality of computed tertiary structures of a target amino-acid sequence. Despite great advances in algorithmic research, given the size, dimensionality, and inherent characteristics of the protein structure space, this task remains exceptionally challenging. It is current practice to aim to generate as many structures as can be afforded so as to increase the likelihood that some of them will reside near the sought but unknown biologically-active/native structure. When operating within a given computational budget, this is impractical and uninformed by any metrics of interest. In this paper, we propose instead to equip algorithms that generate tertiary structures, also known as decoy generation algorithms, with memory of the protein structure space that they explore. Specifically, we propose an evolving, granularity-controllable map of the protein structure space that makes use of low-dimensional representations of protein structures. Evaluations on diverse target sequences that include recent hard CASP targets show that drastic reductions in storage can be made without sacrificing decoy quality. The presented results make the case that integrating a map of the protein structure space is a promising mechanism to enhance decoy generation algorithms in template-free protein structure prediction. [ABSTRACT FROM AUTHOR] Copyright of Journal of Bioinformatics & Computational Biology is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141546480&site=ehost-live
555,Computational Methods for Exploration and Analysis of Macromolecular Structure and Dynamics.,Amarda Shehu,PLoS Computational Biology,1553734X,,10/27/15,11,10,1,3,110560854,10.1371/journal.pcbi.1004585,Public Library of Science,Editorial,COMPUTATIONAL mechanics; MACROMOLECULAR dynamics; COMPUTER simulation; MACROMOLECULES; MICROSCOPIC kinetics,Editorial,"The article presents author's views regarding the use and development of computational methods for exploring and analyzing macromolecular structure and dynamics. Topics discussed include benefits of computations in presenting a complete description of the processes of life, computer simulations as a bridge between the microscopic length and time scales, and the macroscopic world of the laboratory and role of macromolecules in countless biological processes.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=110560854&site=ehost-live
556,Computing the Structural Dynamics of RVFV L Protein Domain in Aqueous Glycerol Solutions.,Amarda Shehu,Biomolecules (2218-273X),2218273X,,Oct-21,11,10,1427,1,153220770,10.3390/biom11101427,MDPI,Article,STRUCTURAL dynamics; PROTEIN domains; CONFORMATIONAL analysis; AQUEOUS solutions; RIFT Valley fever; RADIAL distribution function; GLYCERIN; Soap and cleaning compound manufacturing; Soap and Other Detergent Manufacturing,aqueous glycerol; molecular dynamics; RVFV; structural dynamics,"Many biological and biotechnological processes are controlled by protein–protein and protein–solvent interactions. In order to understand, predict, and optimize such processes, it is important to understand how solvents affect protein structure during protein–solvent interactions. In this study, all-atom molecular dynamics are used to investigate the structural dynamics and energetic properties of a C-terminal domain of the Rift Valley Fever Virus L protein solvated in glycerol and aqueous glycerol solutions in different concentrations by molecular weight. The Generalized Amber Force Field is modified by including restrained electrostatic potential atomic charges for the glycerol molecules. The peptide is considered in detail by monitoring properties like the root-mean-squared deviation, root-mean-squared fluctuation, radius of gyration, hydrodynamic radius, end-to-end distance, solvent-accessible surface area, intra-potential energy, and solvent–peptide interaction energies for hundreds of nanoseconds. Secondary structure analysis is also performed to examine the extent of conformational drift for the individual helices and sheets. We predict that the peptide helices and sheets are maintained only when the modeling strategy considers the solvent with lower glycerol concentration. We also find that the solvent-peptide becomes more cohesive with decreasing glycerol concentrations. The density and radial distribution function of glycerol solvent calculated when modeled with the modified atomic charges show a very good agreement with experimental results and other simulations at 298.15 K. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153220770&site=ehost-live
557,Data Size and Quality Matter: Generating Physically-Realistic Distance Maps of Protein Tertiary Structures.,Amarda Shehu,Biomolecules (2218-273X),2218273X,,Jul-22,12,7,N.PAG,22,158211289,10.3390/biom12070908,MDPI,Article,TERTIARY structure; PROTEIN structure; DATA quality; CYTOSKELETAL proteins; LATENT variables,disentanglement; generative model; multi-structure view; protein molecule; spatial pyramidal pooling; tertiary structure; training set configuration; variational autoencoder,"With the debut of AlphaFold2, we now can get a highly-accurate view of a reasonable equilibrium tertiary structure of a protein molecule. Yet, a single-structure view is insufficient and does not account for the high structural plasticity of protein molecules. Obtaining a multi-structure view of a protein molecule continues to be an outstanding challenge in computational structural biology. In tandem with methods formulated under the umbrella of stochastic optimization, we are now seeing rapid advances in the capabilities of methods based on deep learning. In recent work, we advance the capability of these models to learn from experimentally-available tertiary structures of protein molecules of varying lengths. In this work, we elucidate the important role of the composition of the training dataset on the neural network's ability to learn key local and distal patterns in tertiary structures. To make such patterns visible to the network, we utilize a contact map-based representation of protein tertiary structure. We show interesting relationships between data size, quality, and composition on the ability of latent variable models to learn key patterns of tertiary structure. In addition, we present a disentangled latent variable model which improves upon the state-of-the-art variable autoencoder-based model in key, physically-realistic structural patterns. We believe this work opens up further avenues of research on deep learning-based models for computing multi-structure views of protein molecules. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158211289&site=ehost-live
558,Editorial overview: Theory and simulation and their new friends.,Amarda Shehu,Current Opinion in Structural Biology,0959440X,,Apr-21,67,,iii,1,150069315,10.1016/j.sbi.2021.02.003,Elsevier B.V.,Article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150069315&site=ehost-live
559,Elucidating the ensemble of functionally-relevant transitions in protein systems with a robotics-inspired method.,Amarda Shehu,BMC Structural Biology,14726807,,2013Suppl 1,13,,1,22,131875276,10.1186/1472-6807-13-S1-S8,BioMed Central,Article,,,"Background: Many proteins tune their biological function by transitioning between different functional states, effectively acting as dynamic molecular machines. Detailed structural characterization of transition trajectories is central to understanding the relationship between protein dynamics and function. Computational approaches that build on the Molecular Dynamics framework are in principle able to model transition trajectories at great detail but also at considerable computational cost. Methods that delay consideration of dynamics and focus instead on elucidating energetically-credible conformational paths connecting two functionally-relevant structures provide a complementary approach. Effective sampling-based path planning methods originating in robotics have been recently proposed to produce conformational paths. These methods largely model short peptides or address large proteins by simplifying conformational space. Methods: We propose a robotics-inspired method that connects two given structures of a protein by sampling conformational paths. The method focuses on small- to medium-size proteins, efficiently modeling structural deformations through the use of the molecular fragment replacement technique. In particular, the method grows a tree in conformational space rooted at the start structure, steering the tree to a goal region defined around the goal structure. We investigate various bias schemes over a progress coordinate for balance between coverage of conformational space and progress towards the goal. A geometric projection layer promotes path diversity. A reactive temperature scheme allows sampling of rare paths that cross energy barriers. Results and conclusions: Experiments are conducted on small- to medium-size proteins of length up to 214 amino acids and with multiple known functionally-relevant states, some of which are more than 13Å apart of each-other. Analysis reveals that the method effectively obtains conformational paths connecting structural states that are significantly different. A detailed analysis on the depth and breadth of the tree suggests that a soft global bias over the progress coordinate enhances sampling and results in higher path diversity. The explicit geometric projection layer that biases the exploration away from over-sampled regions further increases coverage, often improving proximity to the goal by forcing the exploration to find new paths. The reactive temperature scheme is shown effective in increasing path diversity, particularly in difficult structural transitions with known high-energy barriers. [ABSTRACT FROM AUTHOR] Copyright of BMC Structural Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131875276&site=ehost-live
560,Evolutionary-inspired probabilistic search for enhancing sampling of local minima in the protein energy surface.,Amarda Shehu,Proteome Science,14775956,,2012,10,Suppl 1,1,16,77940817,10.1186/1477-5956-10-S1-S5,BioMed Central,Article,"PROBABILITY theory; COMPUTATIONAL biology; CONFORMATIONAL analysis; MOLECULAR conformation; PROTEIN structure; PROTEIN-protein interactions; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",,"Background: Despite computational challenges, elucidating conformations that a protein system assumes under physiologic conditions for the purpose of biological activity is a central problem in computational structural biology. While these conformations are associated with low energies in the energy surface that underlies the protein conformational space, few existing conformational search algorithms focus on explicitly sampling lowenergy local minima in the protein energy surface. Methods: This work proposes a novel probabilistic search framework, PLOW, that explicitly samples low-energy local minima in the protein energy surface. The framework combines algorithmic ingredients from evolutionary computation and computational structural biology to effectively explore the subspace of local minima. A greedy local search maps a conformation sampled in conformational space to a nearby local minimum. A perturbation move jumps out of a local minimum to obtain a new starting conformation for the greedy local search. The process repeats in an iterative fashion, resulting in a trajectory-based exploration of the subspace of local minima. Results and conclusions: The analysis of PLOW's performance shows that, by navigating only the subspace of local minima, PLOW is able to sample conformations near a protein's native structure, either more effectively or as well as state-of-the-art methods that focus on reproducing the native structure for a protein system. Analysis of the actual subspace of local minima shows that PLOW samples this subspace more effectively that a naive sampling approach. Additional theoretical analysis reveals that the perturbation function employed by PLOW is key to its ability to sample a diverse set of low-energy conformations. This analysis also suggests directions for further research and novel applications for the proposed framework. [ABSTRACT FROM AUTHOR] Copyright of Proteome Science is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77940817&site=ehost-live
561,"Fewer Dimensions, More Structures for Improved Discrete Models of Dynamics of Free versus Antigen-Bound Antibody.",Amarda Shehu,Biomolecules (2218-273X),2218273X,,Jul-22,12,7,N.PAG,19,158211392,10.3390/biom12071011,MDPI,Article,MOLECULAR dynamics; MARKOV processes; IMMUNOGLOBULINS,antibody; antigen binding; Markov State Model; molecular dynamics; structure,"Over the past decade, Markov State Models (MSM) have emerged as powerful methodologies to build discrete models of dynamics over structures obtained from Molecular Dynamics trajectories. The identification of macrostates for the MSM is a central decision that impacts the quality of the MSM but depends on both the selected representation of a structure and the clustering algorithm utilized over the featurized structures. Motivated by a large molecular system in its free and bound state, this paper investigates two directions of research, further reducing the representation dimensionality in a non-parametric, data-driven manner and including more structures in the computation. Rigorous evaluation of the quality of obtained MSMs via various statistical tests in a comparative setting firmly shows that fewer dimensions and more structures result in a better MSM. Many interesting findings emerge from the best MSM, advancing our understanding of the relationship between antibody dynamics and antibody–antigen recognition. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158211392&site=ehost-live
562,From Extraction of Local Structures of Protein Energy Landscapes to Improved Decoy Selection in Template-Free Protein Structure Prediction.,Amarda Shehu,Molecules,14203049,,Jan-18,23,1,216,20,127754426,10.3390/molecules23010216,MDPI,Article,"PROTEIN structure; STRUCTURAL proteomics; PROTEIN domains; COMPUTATIONAL biology; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",basins; conformational space; decoy selection; energy landscape; Pareto optimality; template-free protein structure prediction,"Due to the essential role that the three-dimensional conformation of a protein plays in regulating interactions with molecular partners, wet and dry laboratories seek biologically-active conformations of a protein to decode its function. Computational approaches are gaining prominence due to the labor and cost demands of wet laboratory investigations. Template-free methods can now compute thousands of conformations known as decoys, but selecting native conformations from the generated decoys remains challenging. Repeatedly, research has shown that the protein energy functions whose minima are sought in the generation of decoys are unreliable indicators of nativeness. The prevalent approach ignores energy altogether and clusters decoys by conformational similarity. Complementary recent efforts design protein-specific scoring functions or train machine learning models on labeled decoys. In this paper, we show that an informative consideration of energy can be carried out under the energy landscape view. Specifically, we leverage local structures known as basins in the energy landscape probed by a template-free method. We propose and compare various strategies of basin-based decoy selection that we demonstrate are superior to clustering-based strategies. The presented results point to further directions of research for improving decoy selection, including the ability to properly consider the multiplicity of native conformations of proteins. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127754426&site=ehost-live
563,Generative deep learning for macromolecular structure and dynamics.,Amarda Shehu,Current Opinion in Structural Biology,0959440X,,Apr-21,67,,170,8,150069307,10.1016/j.sbi.2020.11.012,Elsevier B.V.,Article,"MACROMOLECULAR dynamics; COMPUTATIONAL biology; MOLECULAR biology; DYNAMICAL systems; STRUCTURAL dynamics; DEEP learning; LEARNING communities; Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"Much scientific enquiry across disciplines is founded upon a mechanistic treatment of dynamic systems that ties form to function. A highly visible instance of this is in molecular biology, where characterizing macromolecular structure and dynamics is central to a detailed, molecular-level understanding of biological processes in the living cell. The current computational paradigm utilizes optimization as the generative process for modeling both structure and structural dynamics. Computational biology researchers are now attempting to wield generative models employing deep neural networks as an alternative computational paradigm. In this review, we summarize such efforts. We highlight progress and shortcomings. More importantly, we expose challenges that macromolecular structure poses to deep generative models and take this opportunity to introduce the structural biology community to several recent advances in the deep learning community that promise a way forward. [ABSTRACT FROM AUTHOR] Copyright of Current Opinion in Structural Biology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150069307&site=ehost-live
564,Guiding the Search for Native-like Protein Conformations with an Ab-initio Tree-based Exploration.,Amarda Shehu,International Journal of Robotics Research,2783649,,Jul-10,29,8,1106,22,60833518,10.1177/0278364910371527,"Sage Publications, Ltd.",Article,PROTEIN conformation; ROBOTICS; STATISTICAL sampling; AMINO acid sequence; MONTE Carlo method; DATA structures; PROTEIN engineering; Research and Development in Biotechnology; Marketing Research and Public Opinion Polling,discretization layers; energy landscape; guided exploration; native-like protein conformations; probabilistic sampling; projection space; robotics-inspired; tree-based search,"In this paper we propose a robotics-inspired method to enhance sampling of native-like conformations when employing only aminoacid sequence information for a protein at hand. Computing such conformations, essential to associating structural and functional information with gene sequences, is challenging due to the high-dimensionality and the rugged energy surface of the protein conformational space. The contribution of this paper is a novel two-layered method to enhance the sampling of geometrically distinct low-energy conformations at a coarse-grained level of detail. The method grows a tree in conformational space reconciling two goals: (i) guiding the tree towards lower energies; and (ii) not oversampling geometrically similar conformations. Discretizations of the energy surface and a low-dimensional projection space are employed to select more often for expansion low-energy conformations in under-explored regions of the conformational space. The tree is expanded with low-energy conformations through a Metropolis Monte Carlo framework that uses a move set of physical fragment configurations. Testing on sequences of eight small-to-medium structurally diverse proteins shows that the method rapidly samples native-like conformations in a few hours on a single CPU. Analysis shows that computed conformations are good candidates for further detailed energetic refinements by larger studies in protein engineering and design. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Robotics Research is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60833518&site=ehost-live
565,Modeling Structures and Motions of Loops in Protein Molecules.,Amarda Shehu,Entropy,10994300,,Jun-12,14,2,252,39,72325506,10.3390/e14020252,MDPI,Article,"PROTEIN structure; LIGAND binding; COMPUTATIONAL biology; PROTEIN stability; CONFORMATIONAL analysis; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",conformational ensemble; equilibrium fluctuations; loop modeling; native state; structural analysis of proteins; structural bioinformatics,"Unlike the secondary structure elements that connect in protein structures, loop fragments in protein chains are often highly mobile even in generally stable proteins. The structural variability of loops is often at the center of a protein's stability, folding, and even biological function. Loops are found to mediate important biological processes, such as signaling, protein-ligand binding, and protein-protein interactions. Modeling conformations of a loop under physiological conditions remains an open problem in computational biology. This article reviews computational research in loop modeling, highlighting progress and challenges. Important insight is obtained on potential directions for future research. [ABSTRACT FROM AUTHOR] Copyright of Entropy is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=72325506&site=ehost-live
566,Small molecule generation via disentangled representation learning.,Amarda Shehu,Bioinformatics,13674803,,Jun-22,38,12,3200,9,157413582,10.1093/bioinformatics/btac296,Oxford University Press / USA,Article,SMALL molecules; MATERIALS science; PROTEIN-protein interactions; ATOMIC interactions; DRUG discovery; CHEMICAL structure,,"Motivation Expanding our knowledge of small molecules beyond what is known in nature or designed in wet laboratories promises to significantly advance cheminformatics, drug discovery, biotechnology and material science. In silico molecular design remains challenging, primarily due to the complexity of the chemical space and the non-trivial relationship between chemical structures and biological properties. Deep generative models that learn directly from data are intriguing, but they have yet to demonstrate interpretability in the learned representation, so we can learn more about the relationship between the chemical and biological space. In this article, we advance research on disentangled representation learning for small molecule generation. We build on recent work by us and others on deep graph generative frameworks, which capture atomic interactions via a graph-based representation of a small molecule. The methodological novelty is how we leverage the concept of disentanglement in the graph variational autoencoder framework both to generate biologically relevant small molecules and to enhance model interpretability. Results Extensive qualitative and quantitative experimental evaluation in comparison with state-of-the-art models demonstrate the superiority of our disentanglement framework. We believe this work is an important step to address key challenges in small molecule generation with deep generative frameworks. Availability and implementation Training and generated data are made available at https://ieee-dataport.org/documents/dataset-disentangled-representation-learning-interpretable-molecule-generation. All code is made available at https://anonymous.4open.science/r/D-MolVAE-2799/. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157413582&site=ehost-live
568,The 6th Computational Structural Bioinformatics Workshop.,Amarda Shehu,BMC Structural Biology,14726807,,2013Suppl 1,13,,1,2,131873982,10.1186/1472-6807-13-S1-I1,BioMed Central,Article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131873982&site=ehost-live
569,Unsupervised multi-instance learning for protein structure determination.,Amarda Shehu,Journal of Bioinformatics & Computational Biology,2197200,,Feb-21,19,1,N.PAG,20,149011628,10.1142/S0219720021400023,World Scientific Publishing Company,Article,PROTEIN structure; LEARNING; PATIENT selection,decoy quality; decoy selection; multi-instance learning; Protein structure determination; unsupervised learning,"Many regions of the protein universe remain inaccessible by wet-laboratory or computational structure determination methods. A significant challenge in elucidating these dark regions in silico relates to the ability to discriminate relevant structure(s) among many structures/decoys computed for a protein of interest, a problem known as decoy selection. Clustering decoys based on geometric similarity remains popular. However, it is unclear how exactly to exploit the groups of decoys revealed via clustering to select individual structures for prediction. In this paper, we provide an intuitive formulation of the decoy selection problem as an instance of unsupervised multi-instance learning. We address the problem in three stages, first organizing given decoys of a protein molecule into bags, then identifying relevant bags, and finally drawing individual instances from these bags to offer as prediction. We propose both non-parametric and parametric algorithms for drawing individual instances. Our evaluation utilizes two datasets, one benchmark dataset of ensembles of decoys for a varied list of protein molecules, and a dataset of decoy ensembles for targets drawn from recent CASP competitions. A comparative analysis with state-of-the-art methods reveals that the proposed approach outperforms existing methods, thus warranting further investigation of multi-instance learning to advance our treatment of decoy selection. [ABSTRACT FROM AUTHOR] Copyright of Journal of Bioinformatics & Computational Biology is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149011628&site=ehost-live
570,A population-based evolutionary search approach to the multiple minima problem in de novo protein structure prediction.,Amarda Shehu,BMC Structural Biology,14726807,,2013Suppl 1,13,,1,19,131874027,10.1186/1472-6807-13-S1-S4,BioMed Central,Article,,,"Background: Elucidating the native structure of a protein molecule from its sequence of amino acids, a problem known as de novo structure prediction, is a long standing challenge in computational structural biology. Difficulties in silico arise due to the high dimensionality of the protein conformational space and the ruggedness of the associated energy surface. The issue of multiple minima is a particularly troublesome hallmark of energy surfaces probed with current energy functions. In contrast to the true energy surface, these surfaces are weakly-funneled and rich in comparably deep minima populated by non-native structures. For this reason, many algorithms seek to be inclusive and obtain a broad view of the low-energy regions through an ensemble of low-energy (decoy) conformations. Conformational diversity in this ensemble is key to increasing the likelihood that the native structure has been captured. Methods: We propose an evolutionary search approach to address the multiple-minima problem in decoy sampling for de novo structure prediction. Two population-based evolutionary search algorithms are presented that follow the basic approach of treating conformations as individuals in an evolving population. Coarse graining and molecular fragment replacement are used to efficiently obtain protein-like child conformations from parents. Potential energy is used both to bias parent selection and determine which subset of parents and children will be retained in the evolving population. The effect on the decoy ensemble of sampling minima directly is measured by additionally mapping a conformation to its nearest local minimum before considering it for retainment. The resulting memetic algorithm thus evolves not just a population of conformations but a population of local minima. Results and conclusions: Results show that both algorithms are effective in terms of sampling conformations in proximity of the known native structure. The additional minimization is shown to be key to enhancing sampling capability and obtaining a diverse ensemble of decoy conformations, circumventing premature convergence to sub-optimal regions in the conformational space, and approaching the native structure with proximity that is comparable to state-of-the-art decoy sampling methods. The results are shown to be robust and valid when using two representative state-of-the-art coarse-grained energy functions. [ABSTRACT FROM AUTHOR] Copyright of BMC Structural Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131874027&site=ehost-live
571,A stochastic roadmap method to model protein structural transitions.,Amarda Shehu,Robotica,2635747,,Aug-16,34,8,1705,29,116719274,10.1017/S0263574715001058,Cambridge University Press,Article,ROBOT motion; PROTEIN structure; GENETIC mutation; DISEASES; MARKOV processes,Conformational switching; Energy landscape; First-step analysis; Human disorders; Markov state model; Path smoothing; Protein dynamics; SOD1 and Ras variants; Stochastic roadmap simulation; Structural transitions,"Evidence is emerging that the role of protein structure in disease needs to be rethought. Sequence mutations in proteins are often found to affect the rate at which a protein switches between structures. Modeling structural transitions in wildtype and variant proteins is central to understanding the molecular basis of disease. This paper investigates an efficient algorithmic realization of the stochastic roadmap simulation framework to model structural transitions in wildtype and variants of proteins implicated in human disorders. Our results indicate that the algorithm is able to extract useful information on the impact of mutations on protein structure and function. [ABSTRACT FROM PUBLISHER] Copyright of Robotica is the property of Cambridge University Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116719274&site=ehost-live
572,Attenuating dependence on structural data in computing protein energy landscapes.,Amarda Shehu,BMC Bioinformatics,14712105,,6/6/2019 Supplement 11,20,11,1,10,136827411,10.1186/s12859-019-2822-5,BioMed Central,Article,ATTENUATION (Physics); PROTEINS; HUMAN biology; LANDSCAPES; STOCHASTIC analysis,Protein energy landscape; Stochastic optimization; Structural dynamics,"Background: Nearly all cellular processes involve proteins structurally rearranging to accommodate molecular partners. The energy landscape underscores the inherent nature of proteins as dynamic molecules interconverting between structures with varying energies. In principle, reconstructing a protein's energy landscape holds the key to characterizing the structural dynamics and its regulation of protein function. In practice, the disparate spatio-temporal scales spanned by the slow dynamics challenge both wet and dry laboratories. However, the growing number of deposited structures for proteins central to human biology presents an opportunity to infer the relevant dynamics via exploitation of the information encoded in such structures about equilibrium dynamics. Results: Recent computational efforts using extrinsic modes of motion as variables have successfully reconstructed detailed energy landscapes of several medium-size proteins. Here we investigate the extent to which one can reconstruct the energy landscape of a protein in the absence of sufficient, wet-laboratory structural data. We do so by integrating intrinsic modes of motion extracted off a single structure in a stochastic optimization framework that supports the plug-and-play of different variable selection strategies. We demonstrate that, while knowledge of more wet-laboratory structures yields better-reconstructed landscapes, precious information can be obtained even when only one structural model is available. Conclusions: The presented work shows that it is possible to reconstruct the energy landscape of a protein with reasonable detail and accuracy even when the structural information about the protein is limited to one structure. By attenuating the dependence on structural data of methods designed to compute protein energy landscapes, the work opens up interesting venues of research on structure-based inference of dynamics. Of particular interest are directions of research that will extend such inference to proteins with no experimentally-characterized structures. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136827411&site=ehost-live
573,"Computational Structural Biology: Successes, Future Directions, and Challenges.",Amarda Shehu,Molecules,14203049,,Feb-19,24,3,637,1,134795891,10.3390/molecules24030637,MDPI,Article,CHROMATIN; GENOMES; PUBLIC health; BIG data; SUPRAMOLECULAR chemistry; GENETIC mutation; Health and Welfare Funds,big data; bioinformatics; biological modeling; free-energy landscape; machine intelligence; mutations,"Computational biology has made powerful advances. Among these, trends in human health have been uncovered through heterogeneous 'big data' integration, and disease-associated genes were identified and classified. Along a different front, the dynamic organization of chromatin is being elucidated to gain insight into the fundamental question of genome regulation. Powerful conformational sampling methods have also been developed to yield a detailed molecular view of cellular processes. when combining these methods with the advancements in the modeling of supramolecular assemblies, including those at the membrane, we are finally able to get a glimpse into how cells' actions are regulated. Perhaps most intriguingly, a major thrust is on to decipher the mystery of how the brain is coded. Here, we aim to provide a broad, yet concise, sketch of modern aspects of computational biology, with a special focus on computational structural biology. We attempt to forecast the areas that computational structural biology will embrace in the future and the challenges that it may face. We skirt details, highlight successes, note failures, and map directions. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134795891&site=ehost-live
574,Computing energy landscape maps and structural excursions of proteins.,Amarda Shehu,BMC Genomics,14712164,,8/18/16,17,,433,24,117594572,10.1186/s12864-016-2798-8,BioMed Central,Article,PROTEINS; BIOMOLECULES; ORGANIC compounds; BIOSYNTHESIS; PROTEOMICS; Other basic organic chemical manufacturing; All Other Basic Organic Chemical Manufacturing,Energy landscape map; Evolutionary algorithm; Low-cost paths; Mechanical work; Multi-basin energy landscape; Multi-state protein; Nearest-neighbor graph; Protein equilibrium dynamics; Sample-based representation; Structural excursion,"Background: Structural excursions of a protein at equilibrium are key to biomolecular recognition and function modulation. Protein modeling research is driven by the need to aid wet laboratories in characterizing equilibrium protein dynamics. In principle, structural excursions of a protein can be directly observed via simulation of its dynamics, but the disparate temporal scales involved in such excursions make this approach computationally impractical. On the other hand, an informative representation of the structure space available to a protein at equilibrium can be obtained efficiently via stochastic optimization, but this approach does not directly yield information on equilibrium dynamics. Methods: We present here a novel methodology that first builds a multi-dimensional map of the energy landscape that underlies the structure space of a given protein and then queries the computed map for energetically-feasible excursions between structures of interest. An evolutionary algorithm builds such maps with a practical computational budget. Graphical techniques analyze a computed multi-dimensional map and expose interesting features of an energy landscape, such as basins and barriers. A path searching algorithm then queries a nearest-neighbor graph representation of a computed map for energetically-feasible basin-to-basin excursions. Results: Evaluation is conducted on intrinsically-dynamic proteins of importance in human biology and disease. Visual statistical analysis of the maps of energy landscapes computed by the proposed methodology reveals features already captured in the wet laboratory, as well as new features indicative of interesting, unknown thermodynamically-stable and semi-stable regions of the equilibrium structure space. Comparison of maps and structural excursions computed by the proposed methodology on sequence variants of a protein sheds light on the role of equilibrium structure and dynamics in the sequence-function relationship. Conclusions: Applications show that the proposed methodology is effective at locating basins in complex energy landscapes and computing basin-basin excursions of a protein with a practical computational budget. While the actual temporal scales spanned by a structural excursion cannot be directly obtained due to the foregoing of simulation of dynamics, hypotheses can be formulated regarding the impact of sequence mutations on protein function. These hypotheses are valuable in instigating further research in wet laboratories. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117594572&site=ehost-live
575,Decoy selection for protein structure prediction via extreme gradient boosting and ranking.,Amarda Shehu,BMC Bioinformatics,14712105,,12/9/20,21,1,1,21,147500708,10.1186/s12859-020-3523-9,BioMed Central,Article,"FORECASTING; SPACE probes; MACHINE learning; COMPUTATIONAL biology; PROTEIN structure; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology",Basins; Decoy selection; Energy landscape; Machine learning; Purity; Ranking; XGBoost,"Background: Identifying one or more biologically-active/native decoys from millions of non-native decoys is one of the major challenges in computational structural biology. The extreme lack of balance in positive and negative samples (native and non-native decoys) in a decoy set makes the problem even more complicated. Consensus methods show varied success in handling the challenge of decoy selection despite some issues associated with clustering large decoy sets and decoy sets that do not show much structural similarity. Recent investigations into energy landscape-based decoy selection approaches show promises. However, lack of generalization over varied test cases remains a bottleneck for these methods. Results: We propose a novel decoy selection method, ML-Select, a machine learning framework that exploits the energy landscape associated with the structure space probed through a template-free decoy generation. The proposed method outperforms both clustering and energy ranking-based methods, all the while consistently offering better performance on varied test-cases. Moreover, ML-Select shows promising results even for the decoy sets consisting of mostly low-quality decoys. Conclusions: ML-Select is a useful method for decoy selection. This work suggests further research in finding more effective ways to adopt machine learning frameworks in achieving robust performance for decoy selection in template-free protein structure prediction. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147500708&site=ehost-live
576,Effective Automated Feature Construction and Selection for Classification of Biological Sequences.,Amarda Shehu,PLoS ONE,19326203,,Jul-14,9,7,1,14,97360147,10.1371/journal.pone.0099982,Public Library of Science,Article,"BIOINFORMATICS; NUCLEOTIDE sequence; EXONS (Genetics); GENETIC engineering; MACHINE learning; EVOLUTIONARY algorithms; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",Algorithms; Applied mathematics; Biochemistry; Biology and life sciences; Biophysics; Cell biology; Computational biology; Computer and information sciences; Computerized simulations; DNA; DNA amplification; DNA modification; DNA transcription; Epigenetics; Gene expression; Genetics; Mathematics; Molecular cell biology; Nucleic acids; Physical sciences; Physics; Research Article,"Background: Many open problems in bioinformatics involve elucidating underlying functional signals in biological sequences. DNA sequences, in particular, are characterized by rich architectures in which functional signals are increasingly found to combine local and distal interactions at the nucleotide level. Problems of interest include detection of regulatory regions, splice sites, exons, hypersensitive sites, and more. These problems naturally lend themselves to formulation as classification problems in machine learning. When classification is based on features extracted from the sequences under investigation, success is critically dependent on the chosen set of features. Methodology: We present an algorithmic framework (EFFECT) for automated detection of functional signals in biological sequences. We focus here on classification problems involving DNA sequences which state-of-the-art work in machine learning shows to be challenging and involve complex combinations of local and distal features. EFFECT uses a two-stage process to first construct a set of candidate sequence-based features and then select a most effective subset for the classification task at hand. Both stages make heavy use of evolutionary algorithms to efficiently guide the search towards informative features capable of discriminating between sequences that contain a particular functional signal and those that do not. Results: To demonstrate its generality, EFFECT is applied to three separate problems of importance in DNA research: the recognition of hypersensitive sites, splice sites, and ALU sites. Comparisons with state-of-the-art algorithms show that the framework is both general and powerful. In addition, a detailed analysis of the constructed features shows that they contain valuable biological information about DNA architecture, allowing biologists and other researchers to directly inspect the features and potentially use the insights obtained to assist wet-laboratory studies on retainment or modification of a specific signal. Code, documentation, and all data for the applications presented here are provided for the community at http://www.cs.gmu.edu/~ashehu/?q=OurTools. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=97360147&site=ehost-live
577,Evaluating Autoencoder-Based Featurization and Supervised Learning for Protein Decoy Selection.,Amarda Shehu,Molecules,14203049,,Mar-20,25,5,1146,1,142478940,10.3390/molecules25051146,MDPI,Article,"TERTIARY structure; MORPHOLOGY; PROTEIN structure; MOLECULAR structure; LIBRARY information networks; WEB servers; Data Processing, Hosting, and Related Services",autoencoder; decoy selection; featurization; protein modeling; tertiary structure,"Rapid growth in molecular structure data is renewing interest in featurizing structure. Featurizations that retain information on biological activity are particularly sought for protein molecules, where decades of research have shown that indeed structure encodes function. Research on featurization of protein structure is active, but here we assess the promise of autoencoders. Motivated by rapid progress in neural network research, we investigate and evaluate autoencoders on yielding linear and nonlinear featurizations of protein tertiary structures. An additional reason we focus on autoencoders as the engine to obtain featurizations is the versatility of their architectures and the ease with which changes to architecture yield linear versus nonlinear features. While open-source neural network libraries, such as Keras, which we employ here, greatly facilitate constructing, training, and evaluating autoencoder architectures and conducting model search, autoencoders have not yet gained popularity in the structure biology community. Here we demonstrate their utility in a practical context. Employing autoencoder-based featurizations, we address the classic problem of decoy selection in protein structure prediction. Utilizing off-the-shelf supervised learning methods, we demonstrate that the featurizations are indeed meaningful and allow detecting active tertiary structures, thus opening the way for further avenues of research. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142478940&site=ehost-live
578,Exploring representations of protein structure for automated remote homology detection and mapping of protein structure space.,Amarda Shehu,BMC Bioinformatics,14712105,,2014,15,Suppl 8,1,14,131680689,10.1186/1471-2105-15-S8-S4,BioMed Central,Article,,,"Background: Due to rapid sequencing of genomes, there are now millions of deposited protein sequences with no known function. Fast sequence-based comparisons allow detecting close homologs for a protein of interest to transfer functional information from the homologs to the given protein. Sequence-based comparison cannot detect remote homologs, in which evolution has adjusted the sequence while largely preserving structure. Structure-based comparisons can detect remote homologs but most methods for doing so are too expensive to apply at a large scale over structural databases of proteins. Recently, fragment-based structural representations have been proposed that allow fast detection of remote homologs with reasonable accuracy. These representations have also been used to obtain linearly-reducible maps of protein structure space. It has been shown, as additionally supported from analysis in this paper that such maps preserve functional co-localization of the protein structure space. Methods: Inspired by a recent application of the Latent Dirichlet Allocation (LDA) model for conducting structural comparisons of proteins, we propose higher-order LDA-obtained topic-based representations of protein structures to provide an alternative route for remote homology detection and organization of the protein structure space in few dimensions. Various techniques based on natural language processing are proposed and employed to aid the analysis of topics in the protein structure domain. Results: We show that a topic-based representation is just as effective as a fragment-based one at automated detection of remote homologs and organization of protein structure space. We conduct a detailed analysis of the information content in the topic-based representation, showing that topics have semantic meaning. The fragmentbased and topic-based representations are also shown to allow prediction of superfamily membership. Conclusions: This work opens exciting venues in designing novel representations to extract information about protein structures, as well as organizing and mining protein structure space with mature text mining tools. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131680689&site=ehost-live
579,From molecular energy landscapes to equilibrium dynamics via landscape analysis and markov state models.,Amarda Shehu,Journal of Bioinformatics & Computational Biology,2197200,,Dec-19,17,6,N.PAG,21,141546481,10.1142/S0219720019400146,World Scientific Publishing Company,Article,MARKOV processes; NANOTECHNOLOGY; STRUCTURAL dynamics; MOLECULAR dynamics; Research and Development in Biotechnology,basins; energy landscape; Equilibrium dynamics; graph embeddings; markov state models; molecular dynamics simulation,"Molecular dynamics (MD) simulation software allows probing the equilibrium structural dynamics of a molecule of interest, revealing how a molecule navigates its structure space one structure at a time. To obtain a broader view of dynamics, typically one needs to launch many such simulations, obtaining many trajectories. A summarization of the equilibrium dynamics requires integrating the information in the various trajectories, and Markov State Models (MSM) are increasingly being used for this task. At its core, the task involves organizing the structures accessed in simulation into structural states, and then constructing a transition probability matrix revealing the transitions between states. While now considered a mature technology and widely used to summarize equilibrium dynamics, the underlying computational process in the construction of an MSM ignores energetics even though the transition of a molecule between two nearby structures in an MD trajectory is governed by the corresponding energies. In this paper, we connect theory with simulation and analysis of equilibrium dynamics. A molecule navigates the energy landscape underlying the structure space. The structural states that are identified via off-the-shelf clustering algorithms need to be connected to thermodynamically-stable and semi-stable (macro)states among which transitions can then be quantified. Leveraging recent developments in the analysis of energy landscapes that identify basins in the landscape, we evaluate the hypothesis that basins, directly tied to stable and semi-stable states, lead to better models of dynamics. Our analysis indicates that basins lead to MSMs of better quality and thus can be useful to further advance this widely-used technology for summarization of molecular equilibrium dynamics. [ABSTRACT FROM AUTHOR] Copyright of Journal of Bioinformatics & Computational Biology is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141546481&site=ehost-live
580,Generative Adversarial Learning of Protein Tertiary Structures.,Amarda Shehu,Molecules,14203049,,Mar-21,26,5,1209,1,149272646,10.3390/molecules26051209,MDPI,Article,,deep learning; generative adversarial learning; protein modeling; tertiary structure,"Protein molecules are inherently dynamic and modulate their interactions with different molecular partners by accessing different tertiary structures under physiological conditions. Elucidating such structures remains challenging. Current momentum in deep learning and the powerful performance of generative adversarial networks (GANs) in complex domains, such as computer vision, inspires us to investigate GANs on their ability to generate physically-realistic protein tertiary structures. The analysis presented here shows that several GAN models fail to capture complex, distal structural patterns present in protein tertiary structures. The study additionally reveals that mechanisms touted as effective in stabilizing the training of a GAN model are not all effective, and that performance based on loss alone may be orthogonal to performance based on the quality of generated datasets. A novel contribution in this study is the demonstration that Wasserstein GAN strikes a good balance and manages to capture both local and distal patterns, thus presenting a first step towards more powerful deep generative models for exploring a possibly very diverse set of structures supporting diverse activities of a protein molecule in the cell. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149272646&site=ehost-live
582,"Mapping the Conformation Space of Wildtype and Mutant H-Ras with a Memetic, Cellular, and Multiscale Evolutionary Algorithm.",Amarda Shehu,PLoS Computational Biology,1553734X,,9/1/15,11,9,1,26,109205244,10.1371/journal.pcbi.1004470,Public Library of Science,Article,MOLECULAR biology; EVOLUTIONARY algorithms; PROTEIN structure; CONFORMATIONAL analysis; MEMETICS; GENETIC mutation,Research Article,"An important goal in molecular biology is to understand functional changes upon single-point mutations in proteins. Doing so through a detailed characterization of structure spaces and underlying energy landscapes is desirable but continues to challenge methods based on Molecular Dynamics. In this paper we propose a novel algorithm, SIfTER, which is based instead on stochastic optimization to circumvent the computational challenge of exploring the breadth of a protein’s structure space. SIfTER is a data-driven evolutionary algorithm, leveraging experimentally-available structures of wildtype and variant sequences of a protein to define a reduced search space from where to efficiently draw samples corresponding to novel structures not directly observed in the wet laboratory. The main advantage of SIfTER is its ability to rapidly generate conformational ensembles, thus allowing mapping and juxtaposing landscapes of variant sequences and relating observed differences to functional changes. We apply SIfTER to variant sequences of the H-Ras catalytic domain, due to the prominent role of the Ras protein in signaling pathways that control cell proliferation, its well-studied conformational switching, and abundance of documented mutations in several human tumors. Many Ras mutations are oncogenic, but detailed energy landscapes have not been reported until now. Analysis of SIfTER-computed energy landscapes for the wildtype and two oncogenic variants, G12V and Q61L, suggests that these mutations cause constitutive activation through two different mechanisms. G12V directly affects binding specificity while leaving the energy landscape largely unchanged, whereas Q61L has pronounced, starker effects on the landscape. An implementation of SIfTER is made available at . We believe SIfTER is useful to the community to answer the question of how sequence mutations affect the function of a protein, when there is an abundance of experimental structures that can be exploited to reconstruct an energy landscape that would be computationally impractical to do via Molecular Dynamics. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109205244&site=ehost-live
583,Menthol Binding and Inhibition of α7-Nicotinic Acetylcholine Receptors.,Amarda Shehu,PLoS ONE,19326203,,Jul-13,8,7,1,12,89626840,10.1371/journal.pone.0067674,Public Library of Science,Article,MENTHOL; NICOTINIC acetylcholine receptors; CIGARETTES; BIOCHEMISTRY; XENOPUS laevis; NEUROSCIENCES; BRAIN imaging; ANIMAL models in research; Tobacco and Tobacco Product Merchant Wholesalers; All other miscellaneous store retailers (except beer and wine-making supplies stores); Cigarette and tobacco product merchant wholesalers; Tobacco Manufacturing; Tobacco Stores; Tobacco product manufacturing,Animal models; Biochemistry; Biology; Calcium imaging; Chemistry; Drugs and devices; Medicine; Methanol; Model organisms; Neuroimaging; Neuropharmacology; Neuroscience; Nicotinic acetylcholine receptors; Organic chemistry; Organic compounds; Proteins; Research Article; Terpenes; Xenopus laevis,"Menthol is a common compound in pharmaceutical and commercial products and a popular additive to cigarettes. The molecular targets of menthol remain poorly defined. In this study we show an effect of menthol on the α7 subunit of the nicotinic acetylcholine (nACh) receptor function. Using a two-electrode voltage-clamp technique, menthol was found to reversibly inhibit α7-nACh receptors heterologously expressed in Xenopus oocytes. Inhibition by menthol was not dependent on the membrane potential and did not involve endogenous Ca2+-dependent Cl− channels, since menthol inhibition remained unchanged by intracellular injection of the Ca2+ chelator BAPTA and perfusion with Ca2+-free bathing solution containing Ba2+. Furthermore, increasing ACh concentrations did not reverse menthol inhibition and the specific binding of [125I] α-bungarotoxin was not attenuated by menthol. Studies of α7- nACh receptors endogenously expressed in neural cells demonstrate that menthol attenuates α7 mediated Ca2+ transients in the cell body and neurite. In conclusion, our results suggest that menthol inhibits α7-nACh receptors in a noncompetitive manner. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89626840&site=ehost-live
584,Principles and Overview of Sampling Methods for Modeling Macromolecular Structure and Dynamics.,Amarda Shehu,PLoS Computational Biology,1553734X,,4/28/16,12,4,1,70,114917157,10.1371/journal.pcbi.1004619,Public Library of Science,Article,MACROMOLECULAR dynamics; COMPUTATIONAL mechanics; NUCLEIC acid isolation methods; FUNCTIONAL genomics; X-ray crystallography; NUCLEAR magnetic resonance,Algorithms; Applied mathematics; Biochemical simulations; Biochemistry; Biology and life sciences; Biophysical simulations; Biophysics; Chemistry; Computational biology; Free energy; Macromolecular structure analysis; Macromolecules; Mathematics; Molecular biology; Physical sciences; Physics; Polymer chemistry; Protein structure; Protein structure prediction; Proteins; Research and analysis methods; Review; Simulation and modeling; Thermodynamics,"Investigation of macromolecular structure and dynamics is fundamental to understanding how macromolecules carry out their functions in the cell. Significant advances have been made toward this end in silico, with a growing number of computational methods proposed yearly to study and simulate various aspects of macromolecular structure and dynamics. This review aims to provide an overview of recent advances, focusing primarily on methods proposed for exploring the structure space of macromolecules in isolation and in assemblies for the purpose of characterizing equilibrium structure and dynamics. In addition to surveying recent applications that showcase current capabilities of computational methods, this review highlights state-of-the-art algorithmic techniques proposed to overcome challenges posed in silico by the disparate spatial and time scales accessed by dynamic macromolecules. This review is not meant to be exhaustive, as such an endeavor is impossible, but rather aims to balance breadth and depth of strategies for modeling macromolecular structure and dynamics for a broad audience of novices and experts. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114917157&site=ehost-live
585,Restriction versus guidance in protein structure prediction.,Amarda Shehu,Proceedings of the National Academy of Sciences of the United States of America,278424,,9/8/09,106,36,15302,6,44288999,10.1073/pnas.0907002106,National Academy of Sciences,Article,CONFORMATIONAL analysis; PROTEIN conformation; HAMILTONIAN systems; MOLECULE-molecule collisions; MOLECULAR dynamics; FORCE & energy,annealing; associative memory Hamiltonian; fragment assembly; molecular dynamics; protein folding,"Conformational restriction by fragment assembly and guidance in molecular dynamics are alternate conformational search strategies in protein structure prediction. We examine both approaches using a version of the associative memory Hamiltonian that incorporates the influence of water-mediated interactions (AMW). For short proteins (<70 residues), fragment assembly, while searching a restricted space, compares well to molecular dynamics and is often sufficient to fold such proteins to near-native conformations (4A) via simulated annealing. Longer proteins encounter kinetic sampling limitations in fragment assembly not seen in molecular dynamics which generally samples more native-like conformations. We also present a fragment enriched version of the standard AMW energy function, AMW-FME, which incorporates the local sequence alignment derived fragment libraries from fragment assembly directly into the energy function. This energy function, in which fragment information acts as a guide not a restriction, is found by molecular dynamics to improve on both previous approaches. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the National Academy of Sciences of the United States of America is the property of National Academy of Sciences and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=44288999&site=ehost-live
586,Sample-Based Models of Protein Energy Landscapes and Slow Structural Rearrangements.,Amarda Shehu,Journal of Computational Biology,10665277,,Jan-18,25,1,33,18,127185401,10.1089/cmb.2017.0158,"Mary Ann Liebert, Inc.",Article,PROTEIN structure; STRUCTURAL dynamics; PROTEIN models; SIMULATION methods & models; ALGORITHMS,energy landscape; protein modeling; sample-based model; sampling capability; structural rearrangements,"Proteins often undergo slow structural rearrangements that involve several angstroms and surpass the nanosecond timescale. These spatiotemporal scales challenge physics-based simulations and open the way to sample-based models of structural dynamics. This article improves an understanding of current capabilities and limitations of sample-based models of dynamics. Borrowing from widely used concepts in evolutionary computation, this article introduces two conflicting aspects of sampling capability and quantifies them via statistical (and graphical) analysis tools. This allows not only conducting a principled comparison of different sample-based algorithms but also understanding which algorithmic ingredients to use as knobs via which to control sampling and, in turn, the accuracy and detail of modeled structural rearrangements. We demonstrate the latter by proposing two powerful variants of a recently published sample-based algorithm. We believe that this work will advance the adoption of sample-based models as reliable tools for modeling slow protein structural rearrangements. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127185401&site=ehost-live
587,The 7th Computational Structural Bioinformatics Workshop.,Amarda Shehu,Journal of Computational Biology,10665277,,Sep-15,22,9,785,2,109251701,10.1089/cmb.2015.28999.jh,"Mary Ann Liebert, Inc.",Article,MOLECULAR docking; CONFORMATIONAL analysis; PROTEINS,,"An introduction is presented in which the editor discusses various reports within the issue on topics including evaluation of decoy databases to improve its quality, exploring conformational space of proteins by introducing crystallographic structures and, an algorithm for protein docking.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251701&site=ehost-live
588,Are nicotinic acetylcholine receptors coupled to G proteins?,Amarda Shehu,BioEssays,2659247,,Dec-13,35,12,1025,10,92005592,10.1002/bies.201300082,"John Wiley & Sons, Inc.",Article,NICOTINIC acetylcholine receptors; G proteins; PROTEOMICS; ION channels; CONSERVED sequences (Genetics); CELLULAR signal transduction,acetylcholine; G protein coupling; intracellular loop; ligand‐gated ion channel; ligand-gated ion channel; loop modeling; protein interaction; signal transduction,"It was, until recently, accepted that the two classes of acetylcholine (ACh) receptors are distinct in an important sense: muscarinic ACh receptors signal via heterotrimeric GTP binding proteins (G proteins), whereas nicotinic ACh receptors (nAChRs) open to allow flux of Na+, Ca2+, and K+ ions into the cell after activation. Here we present evidence of direct coupling between G proteins and nAChRs in neurons. Based on proteomic, biophysical, and functional evidence, we hypothesize that binding to G proteins modulates the activity and signaling of nAChRs in cells. It is important to note that while this hypothesis is new for the nAChR, it is consistent with known interactions between G proteins and structurally related ligand-gated ion channels. Therefore, it underscores an evolutionarily conserved metabotropic mechanism of G protein signaling via nAChR channels. Also watch the Video Abstract. [ABSTRACT FROM AUTHOR] Copyright of BioEssays is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92005592&site=ehost-live
589,Foreword on special issue on robotics methods for structural and dynamic modeling of molecular systems.,Amarda Shehu,Robotica,2635747,,Aug-16,34,8,1677,2,116719278,10.1017/S0263574716000370,Cambridge University Press,Article,ROBOTICS; MOLECULAR biology; ROBOT motion; EVOLUTIONARY computation; CONSTRAINT programming; DATA mining,,"Molecular biological systems can be seen as extremely complex mobile systems. The development of methods for modeling the structure and the motion of such systems is essential to better understand their physiochemical properties and biological functions. In recent years, many computer scientists in robotics and artificial intelligence have made significant contributions to modeling biological systems. Research expertise in planning, search, learning, evolutionary computation, constraint programming, and data mining is being used to make great progress on molecular motion, structure prediction, and design. [ABSTRACT FROM PUBLISHER] Copyright of Robotica is the property of Cambridge University Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116719278&site=ehost-live
590,Graph-Based Community Detection for Decoy Selection in Template-Free Protein Structure Prediction.,Amarda Shehu,Molecules,14203049,,Mar-19,24,5,854,1,135380903,10.3390/molecules24050854,MDPI,Article,PROTEIN structure; TERTIARY structure; PROTEIN analysis; AMINO acid sequence; PROTEIN folding,community detection; decoy selection; nearest-neighbor graph; protein structure space; template-free protein structure prediction,"Significant efforts in wet and dry laboratories are devoted to resolving molecular structures. In particular, computational methods can now compute thousands of tertiary structures that populate the structure space of a protein molecule of interest. These advances are now allowing us to turn our attention to analysis methodologies that are able to organize the computed structures in order to highlight functionally relevant structural states. In this paper, we propose a methodology that leverages community detection methods, designed originally to detect communities in social networks, to organize computationally probed protein structure spaces. We report a principled comparison of such methods along several metrics on proteins of diverse folds and lengths. We present a rigorous evaluation in the context of decoy selection in template-free protein structure prediction. The results make the case that network-based community detection methods warrant further investigation to advance analysis of protein structure spaces for automated selection of functionally relevant structures. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135380903&site=ehost-live
592,Modeling the Tertiary Structure of the Rift Valley Fever Virus L Protein.,Amarda Shehu,Molecules,14203049,,May-19,24,9,1768,1,136449552,10.3390/molecules24091768,MDPI,Article,RIFT Valley fever; GENE expression; MOLECULAR dynamics; AMINO acids; GENOMES,computational structure determination; multidomain protein; Rift Valley fever virus; tertiary structure,"A tertiary structure governs, to a great extent, the biological activity of a protein in the living cell and is consequently a central focus of numerous studies aiming to shed light on cellular processes central to human health. Here, we aim to elucidate the structure of the Rift Valley fever virus (RVFV) L protein using a combination of in silico techniques. Due to its large size and multiple domains, elucidation of the tertiary structure of the L protein has so far challenged both dry and wet laboratories. In this work, we leverage complementary perspectives and tools from the computational-molecular-biology and bioinformatics domains for constructing, refining, and evaluating several atomistic structural models of the L protein that are physically realistic. All computed models have very flexible termini of about 200 amino acids each, and a high proportion of helical regions. Properties such as potential energy, radius of gyration, hydrodynamics radius, flexibility coefficient, and solvent-accessible surface are reported. Structural characterization of the L protein enables our laboratories to better understand viral replication and transcription via further studies of L protein-mediated protein–protein interactions. While results presented a focus on the RVFV L protein, the following workflow is a more general modeling protocol for discovering the tertiary structure of multidomain proteins consisting of thousands of amino acids. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136449552&site=ehost-live
593,Unsupervised and Supervised Learning over the Energy Landscape for Protein Decoy Selection.,Amarda Shehu,Biomolecules (2218-273X),2218273X,,Oct-19,9,10,607,1,139415532,10.3390/biom9100607,MDPI,Article,"STRUCTURAL dynamics; MOLECULAR dynamics; PROTEIN structure; COMPUTATIONAL biology; MOLECULAR structure; LANDSCAPE assessment; SYSTEMS biology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology",basin; decoy selection; energy landscape; machine learning; model quality assessment; purity,"The energy landscape that organizes microstates of a molecular system and governs the underlying molecular dynamics exposes the relationship between molecular form/structure, changes to form, and biological activity or function in the cell. However, several challenges stand in the way of leveraging energy landscapes for relating structure and structural dynamics to function. Energy landscapes are high-dimensional, multi-modal, and often overly-rugged. Deep wells or basins in them do not always correspond to stable structural states but are instead the result of inherent inaccuracies in semi-empirical molecular energy functions. Due to these challenges, energetics is typically ignored in computational approaches addressing long-standing central questions in computational biology, such as protein decoy selection. In the latter, the goal is to determine over a possibly large number of computationally-generated three-dimensional structures of a protein those structures that are biologically-active/native. In recent work, we have recast our attention on the protein energy landscape and its role in helping us to advance decoy selection. Here, we summarize some of our successes so far in this direction via unsupervised learning. More importantly, we further advance the argument that the energy landscape holds valuable information to aid and advance the state of protein decoy selection via novel machine learning methodologies that leverage supervised learning. Our focus in this article is on decoy selection for the purpose of a rigorous, quantitative evaluation of how leveraging protein energy landscapes advances an important problem in protein modeling. However, the ideas and concepts presented here are generally useful to make discoveries in studies aiming to relate molecular structure and structural dynamics to function. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139415532&site=ehost-live
594,Analysis of performance and equity in ground delay programs,Lance Sherry,Transportation Research: Part C,0968090X,,Dec-10,18,6,910,11,53303750,10.1016/j.trc.2010.03.009,Elsevier B.V.,Article,FLIGHT delays & cancellations (Airlines); AIRPORTS; RATIONING; CONJOINT analysis; AIR travelers; STAKEHOLDERS; AIRPLANE fuel consumption; AIRLINE industry; Scheduled air transportation; Scheduled Passenger Air Transportation; Other Airport Operations,Equity; Fuel burn; Ground delay program; Passenger delay; Performance; Utility,"Abstract: The discrepancy between the projected demand for arrival slots at an airport and the projected available arrival slots on a given day is resolved by the Ground Delay Program (GDP). The current GDP rationing rule, Ration-by-Schedule, allocates the available arrival slots at the affected airport by scheduled arrival time of the flights with some adjustments to balance the equity between airlines. This rule does not take into account passenger flow and fuel flow performance in the rationing assignment tradeoff. This paper examines the trade-off between passenger delays and excess surface fuel burn as well as airline equity and passenger equity in GDP slot allocation using different rationing rules. A GDP Rationing Rule Simulator (GDP-RRS) is developed to calculate performance and equity metrics for all stakeholders using six alternate rules. The results show that there is a trade-off between GDP performance and GDP equity. Ration-by-Passengers (a rule which maximizes the passenger throughput) decreased total passenger delay by 22% and decreased total excess fuel burn by 57% with no change in total flight delay compared to the traditional Ration-by-Schedule. However, when the airline and passenger equity are primary concerns, the Ration-by-Schedule is preferred. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303750&site=ehost-live
595,Automation for task analysis of next generation air traffic management systems,Lance Sherry,Transportation Research: Part C,0968090X,,Dec-10,18,6,921,9,53303751,10.1016/j.trc.2010.03.006,Elsevier B.V.,Article,AIR traffic control; AUTOMATION; TASK analysis; ERROR rates; JOB analysis; USER-centered system design; HUMAN-computer interaction; INDUSTRIAL engineering; Air Traffic Control,Human–computer interaction; Probability of failure-to-complete a task; Task analysis; Trials-to-mastery; Usability analysis,"Abstract: The increasing span of control of Air Traffic Control enterprise automation (e.g. Flight Schedule Monitor, Departure Flow Management), along with lean-processes and pay-for-performance business models, has placed increased emphasis on operator training time and error rates. There are two traditional approaches to the design of human–computer interaction (HCI) to minimize training time and reduce error rates: (1) experimental user testing provides the most accurate assessment of training time and error rates, but occurs too late in the development cycle and is cost prohibitive, (2) manual review methods (e.g. cognitive walkthrough) can be used earlier in the development cycle, but suffer from poor accuracy and poor inter-rater reliability. Recent development of “affordable” human performance models provide the basis for the automation of task analysis and HCI design to obtain low cost, accurate, estimates of training time and error rates early in the development cycle. This paper describes a usability/HCI analysis tool that this intended for use by design engineers in the course of their software engineering duties. The tool computes estimates of trials-to-mastery (i.e. time to competence for training) and the probability of failure-to-complete for each task. The HCI required to complete a task on the automation under development is entered into the web-based tool via a form. Assessments of the salience of visual cues to prompt operator actions for the proposed design are used to compute training time and error rates. The web-based tool enables designers in multiple locations to review and contribute to the design. An example analysis is provided along with a discussion of the limitations of the tool and directions for future research. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303751&site=ehost-live
596,Improving the accuracy of airport emissions inventories using disparate datasets.,Lance Sherry,IIE Transactions,0740817X,,Jun-15,47,6,577,9,101893084,10.1080/0740817X.2014.938845,Taylor & Francis Ltd,Article,AIRPORTS; DATA analysis; ENVIRONMENTAL regulations; AIR quality; CARBON oxides; Other Airport Operations; Administration of Air and Water Resource and Solid Waste Management Programs,aircraft performance models; Airport emissions inventories; radar surveillance track data; takeoff thrust setting,"Environmental regulations require airports to report air quality emissions inventories (i.e., tons emitted) for aircraft emissions such as carbon oxides (COx) and nitrogen oxides (NOx). Traditional methods for emission inventory calculations yield over-estimated inventories due an assumption of the use of maximum takeoff thrust settings for all departures. To reduce costs, airlines use “reduced” thrust settings (such as derated or flex temperature thrust settings) that can be up to 25% lower than the maximum takeoff thrust setting. Thrust data for each flight operation are not readily available to those responsible for the emission inventory. This article describes an approach to estimate the actual takeoff thrust for each flight operation using algorithms that combine radar surveillance track data, weather data, and standardized aircraft performance models. A case study for flights from Chicago's O’Hare airport exhibited an average takeoff thrust of 86% of maximum takeoff thrust (within 4% of the average for actual takeoff thrust settings). The implications and limitations of this method are discussed. [ABSTRACT FROM PUBLISHER] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101893084&site=ehost-live
597,Methodology and Case Study for Validation of Aircraft-Induced Clouds from Hyperspectral Imagery.,Lance Sherry,Atmosphere,20734433,,Aug-22,13,8,N.PAG,11,158731404,10.3390/atmos13081257,MDPI,Article,WEATHER; JET engines; REMOTE-sensing images; MATCHED filters; TERRESTRIAL radiation; WEST Virginia,aviation; climate change; contrails; in-scene spectra; remote sensing,"Aircraft-Induced Clouds (AICs), colloquially called contrails, form from the emission of soot from jet engines during cruise flight in favorable atmospheric conditions. AICs absorb, scatter, and reflect shortwave and longwave radiation. This radiative transfer has a cooling effect during the day; however, the night experiences an overwhelming warming effect, which leads to an overall warming effect on Earth, contributing to anthropogenically propelled climate change. Reducing AICs significantly mitigates aviation's contribution to climate change by reducing the disruption in Earth's radiation budget. Researchers have proposed AIC Abatement Programs (AAPs) to increase cruise flight levels without additional fuel burn. In order to effectively implement AAPs, it is crucial to be able to accurately identify AICs from publicly available aerial and satellite imagery. This study aims at the identification of AICs from hyperspectral imagery to help the effective implementation of an AAP and to mitigate climate change. This paper describes a method for the hyperspectral analysis of aerial images in order to accurately identify AICs through a case study based in West Virginia. The results show that both the Adaptive Coherence Estimator and the Matched Filter algorithms based on unique in-scene spectra were successful in the isolation of the AICs from other cloud types and the background. It is found that AICs can be identified with 84% confidence in this case study. The method, a case study, and future works are provided. [ABSTRACT FROM AUTHOR] Copyright of Atmosphere is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158731404&site=ehost-live
598,Methodology for collision risk assessment of an airspace flow corridor concept.,John Shortle,Reliability Engineering & System Safety,9518320,,Oct-15,142,,444,12,108552086,10.1016/j.ress.2015.05.015,Elsevier B.V.,Article,TRAFFIC accidents; RISK assessment; PARAMETER estimation; UNITED States. Federal Aviation Administration; INTERNATIONAL Civil Aviation Organization; Regulation and Administration of Transportation Programs,ADS-B Automatic Dependent Surveillance Broadcast; AFM Autonomous Flight Management; Air traffic management; Autonomous Flight Management; CD&R conflict detection and resolution; Dynamic event tree; FAA Federal Aviation Administration; Fault tree; Flow corridor concept; ICAO international civil aviation organization; LOL loss of locatability; Monte Carlo simulation; NextGen Next Generation Air Transportation System; NMAC Near Mid-air Collision; RNP required navigation performance; SICDR Strategic Intent-based CD&R Function; TCAS Traffic Collision Avoidance System; TICDR tactical intent-based CD&R function; TIS-B Traffic Information Service Broadcast; TSCDR tactical state-based CD&R function,"This paper presents a methodology to estimate the collision risk associated with a future air-transportation concept called the flow corridor . This concept is designed to reduce congestion and increase throughput in en-route airspace by creating dedicated flight corridors across the continent. The methodology is a hybrid collision-risk methodology combining Monte Carlo simulation and dynamic event trees. Monte Carlo simulation is used to model the movement of aircraft within the corridor and to identify potential trajectories that might lead to a collision. Dynamic event trees are used to evaluate the effectiveness of subsequent safety layers that protect against collisions. The overall risk assessment captures the unique characteristics of the flow corridor concept, including self-separation within the corridor, lane change maneuvers, speed adjustments, and the automated separation assurance system. A tradeoff between safety and throughput is conducted, and a sensitivity analysis identifies the most critical parameters in the model. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108552086&site=ehost-live
598,Methodology for collision risk assessment of an airspace flow corridor concept.,Lance Sherry,Reliability Engineering & System Safety,9518320,,Oct-15,142,,444,12,108552086,10.1016/j.ress.2015.05.015,Elsevier B.V.,Article,TRAFFIC accidents; RISK assessment; PARAMETER estimation; UNITED States. Federal Aviation Administration; INTERNATIONAL Civil Aviation Organization; Regulation and Administration of Transportation Programs,ADS-B Automatic Dependent Surveillance Broadcast; AFM Autonomous Flight Management; Air traffic management; Autonomous Flight Management; CD&R conflict detection and resolution; Dynamic event tree; FAA Federal Aviation Administration; Fault tree; Flow corridor concept; ICAO international civil aviation organization; LOL loss of locatability; Monte Carlo simulation; NextGen Next Generation Air Transportation System; NMAC Near Mid-air Collision; RNP required navigation performance; SICDR Strategic Intent-based CD&R Function; TCAS Traffic Collision Avoidance System; TICDR tactical intent-based CD&R function; TIS-B Traffic Information Service Broadcast; TSCDR tactical state-based CD&R function,"This paper presents a methodology to estimate the collision risk associated with a future air-transportation concept called the flow corridor . This concept is designed to reduce congestion and increase throughput in en-route airspace by creating dedicated flight corridors across the continent. The methodology is a hybrid collision-risk methodology combining Monte Carlo simulation and dynamic event trees. Monte Carlo simulation is used to model the movement of aircraft within the corridor and to identify potential trajectories that might lead to a collision. Dynamic event trees are used to evaluate the effectiveness of subsequent safety layers that protect against collisions. The overall risk assessment captures the unique characteristics of the flow corridor concept, including self-separation within the corridor, lane change maneuvers, speed adjustments, and the automated separation assurance system. A tradeoff between safety and throughput is conducted, and a sensitivity analysis identifies the most critical parameters in the model. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108552086&site=ehost-live
599,Troublesome trends in U. S. air transportation.,Lance Sherry,Aerospace America,0740722X,,Nov-12,50,10,20,5,83301866,,American Institute of Aeronautics & Astronautics,Article,COMMERCIAL aeronautics; TRAVELERS; AIRLINE rates; TECHNOLOGICAL innovations; UNITED States; Nonscheduled Chartered Passenger Air Transportation; Non-scheduled specialty flying services; Scheduled Freight Air Transportation; Scheduled Passenger Air Transportation; Scheduled air transportation; All Other Traveler Accommodation; UNITED States economy,,"The article reports that trends in the air transportation industry are changing its role as a prime mover of the U.S. economy. Studies showed that since the 1990s, U.S. domestic airlines are serving fewer, higher paying passengers. Increased airfare and reduced accessibility lead to higher transportation costs for businesses, affecting expansion and productivity. To maintain affordable levels, the authors recommend a two pronged approach, namely regulatory incentives and technical innovation.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=83301866&site=ehost-live
600,Resilient Consensus in Robot Swarms With Periodic Motion and Intermittent Communication.,Daigo Shishika,IEEE Transactions on Robotics,15523098,,Feb-22,38,1,110,16,155186448,10.1109/TRO.2021.3088765,IEEE,Article,PERIODIC motion; ROBOTS; CONSENSUS (Social sciences); MULTIAGENT systems; TASK analysis,Communication networks; Consensus; multiagent systems; Network topology; resilience systems; Robots; Robustness; Shape; Task analysis; Topology,"In this article, we propose an approach to construct a time-varying communication topology with a resilient consensus performance for robot swarms with limited communication ranges. The robots are deployed to explore a large task space and achieve consensus despite the existence of a finite number of noncooperative members in the team. Existing methods encouraged robots to stay close to each other to achieve certain robustness requirements on the connectivity of the communication topology. We leverage on the robots’ mobility to design a time-varying integrated topology composed of several subgroups of robots deployed on nonoverlapping closed-loop paths. Robots are spread out and move along the paths, forming periodic communication links within or across groups. We analyze the time-varying topology synthesized and provide sufficient conditions for individual subgroups and the interconnection between them. We show designs satisfying the conditions with simulated examples in a lattice space, as well as in a task space with predefined paths. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Robotics is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155186448&site=ehost-live
601,A Generalized Framework for Probabilistic Design With Application to Rotors.,John Shortle,IEEE Transactions on Reliability,189529,,Mar-06,55,1,44,9,20926423,10.1109/TR.2005.863816,IEEE,Article,PROBABILITY theory; QUALITY; INERTIA (Mechanics); ROTORS; RELIABILITY in engineering; MULTIVARIATE analysis; ENGINEERING systems,Multivariate distributions; probabilistic design; rotational transforms; rotor dynamics; rotor imbalance; tensors,"This paper generalizes an existing method for deriving probability models of manufacturing quality metrics, We specifically consider the problem of deriving probability models for the inertia tensor of a rotor. The inertia tensor is a 3 × 3 matrix that determines various dynamical properties of the rotor as it spins, affecting its reliability. The key contribution of this paper is that the quality metric of interest is a matrix or a second-order tensor, and the various manufacturing imperfections that cause deviations in the inertia tensor may be vectors. Existing methods, by contrast, assume that the quality metric, as well as the manufacturing imperfections, are scalar quantities, By using rotational properties of matrices & vectors, we show that the relationship between the inertia tensor, and the manufacturing errors must have a specific form, when the errors are small. This structure significantly restricts the class of allowable distributions for the inertia tensor. For example, we show that the multivariate s-normal distribution is not a physically appropriate distribution for the inertia tensor, The results in this paper, while applied specifically to the inertia tensor, are general, and depend only on transformational properties of vectors & matrices. Thus, the framework is applicable to modeling other engineering systems involving second-order teamsters, such as the stress tensor, or strain tensor. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Reliability is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20926423&site=ehost-live
602,An Algorithm to Compute the Waiting Time Distribution for the M/G/1 Queue.,John Shortle,INFORMS Journal on Computing,10919856,,Spring2004,16,2,152,10,13258405,10.1287/ijoc.1030.0045,INFORMS: Institute for Operations Research,Article,QUEUING theory; INTERNET; DISTRIBUTION (Probability theory); PROBABILITY theory; GAME theory; MANAGEMENT science; Internet Publishing and Broadcasting and Web Search Portals; Wired Telecommunications Carriers,algorithms; communications.; queues,"In many modern applications of queueing theory, the classical assumption of exponentially decaying service distributions does not apply. Often, appropriate distributions for modeling in these areas are heavy-tailed. Such distributions decay more slowly than any exponential function, giving a nontrivial probability of an extremely large event. Two areas where heavy-tailed distributions play a significant role are in Internet and financial applications. Internet traffic statistics have suggested that many relevant quantities—for example, file sizes, packet lengths, interarrival times and connection times—should be modeled with heavy-tailed distributions.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=13258405&site=ehost-live
603,Efficient simulation of blackout probabilities using splitting,John Shortle,International Journal of Electrical Power & Energy Systems,1420615,,Jan-13,44,1,743,9,83297892,10.1016/j.ijepes.2012.07.055,Elsevier B.V.,Article,ENERGY consumption; SIMULATION methods & models; ELECTRIC power failures; MONTE Carlo method; STOCHASTIC models; PARAMETER estimation; GRID computing; COMPUTER networks; Computer Systems Design Services,Cascading blackouts; Rare-event simulation; Splitting,"Abstract: Standard Monte-Carlo simulation may be computationally intractable when the events of interest are extremely rare. This paper applies the rare-event simulation technique of splitting to the problem of estimating large-scale blackout probabilities. First, a stochastic model of cascading line failures is developed. Then, a simple network is presented and an analytical solution is derived for the simple network. Exploration of the analytical solution provides some guidance for setting splitting parameters in more complicated networks. In particular, geometrically increasing levels typically give an improvement over equally-spaced levels, due to the cascading nature of blackouts. The splitting methodology is applied to several different network topologies of varying complexity – a mesh network, a grid network, and the IEEE 118-bus network. Numerical results indicate that splitting has the potential to be effective on problems for which standard simulation may be infeasible. [Copyright &y& Elsevier] Copyright of International Journal of Electrical Power & Energy Systems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=83297892&site=ehost-live
604,PIECEWISE POLYNOMIAL APPROXIMATIONS FOR HEAVY-TAILED DISTRIBUTIONS IN QUEUEING ANALYSIS.,John Shortle,Stochastic Models,15326349,,Feb-05,21,1,215,20,16432817,10.1081/STM-200046520,Taylor & Francis Ltd,Article,QUEUING theory; STOCHASTIC processes; LAPLACE transformation; OPERATIONAL calculus; DIFFERENTIAL equations; CHEBYSHEV polynomials,Heavy-tailed distributions; Numerical methods in queueing,"A basic difficulty in dealing with heavy-tailed distributions is that they may not have explicit Laplace transforms. This makes numerical methods that use the Laplace transform more challenging. This paper generalizes an existing method for approximating heavy-tailed distributions, for use in queueing analysis. The generalization, involves fitting Chebyshev polynomials to a probability density function g(t) at specified points t(1), t(2),...,t(N). By choosing points t(1, which rapidly get far out in the tail, it is possible to capture the tail behavior with relatively few points, and to control the relative error in the approximation. We give numerical examples to evaluate the performance of the method in simple queueing problem. [ABSTRACT FROM AUTHOR] Copyright of Stochastic Models is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=16432817&site=ehost-live
605,Safety Comparison of Centralized and Distributed Aircraft Separation Assurance Concepts.,John Shortle,IEEE Transactions on Reliability,189529,,Mar-14,63,1,259,11,94763939,10.1109/TR.2014.2299193,IEEE,Article,AIRCRAFT separation; TRAFFIC safety; PROPULSION systems; AERONAUTICAL safety measures; DECENTRALIZATION in management; PROBABILITY theory,Aircraft; Aircraft propulsion; Atmospheric modeling; Aviation safety; case study; centralized versus distributed; common cause failure; near mid-air collisions; Numerical models; Probability; Safety; separation assurance; Trajectory,"This paper presents several models to compare centralized and distributed automated separation assurance concepts in aviation. In a centralized system, safety-related functions are implemented by common equipment on the ground. In a distributed system, safety-related functions are implemented by equipment on each aircraft. Failures of the safety-related functions can increase the risk of near mid-air collisions. Intuitively, failures on the ground are worse than failures in the air because the ground failures simultaneously affect multiple aircraft. This paper evaluates the degree to which this belief is true. Using region-wide models to account for dependencies between aircraft pairs, we derive the region-wide expectation and variance of the number of separation losses for both centralized and distributed concepts. This derivation is done first for a basic scenario involving a single component and function. We show that the variance of the number of separation losses is always higher for the centralized system, holding the expectations equal. However, numerical examples show that the difference is negligible when the events of interest are rare. Results are extended to a hybrid centralized-distributed scenario involving multiple components and functions on the ground and in the air. In this case, the variance of the centralized system may actually be less than that of the distributed system. The overall implication is that the common-cause failure of the ground function does not seriously weaken the overall case for using a centralized concept versus a distributed concept. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Reliability is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94763939&site=ehost-live
606,Spatial and Temporal Modeling of IED Emplacements against Dismounted Patrols.,John Shortle,Military Operations Research Journal,10825983,,2015,20,3,5,20,110365812,10.5711/1082598320305,Military Operations Research Society,Article,IMPROVISED explosive devices; GLOBAL Positioning System; MIDDLE East-United States relations,,"Improvised explosive device (IED) activity has been a concern for US and coalition troops in the Middle East for more than a decade. In particular, commanders still desire credible estimates of where and when IEDs are emplaced by enemy forces. This paper describes a data collection effort in Afghanistan where IED event data and dismounted friendly force patrol movements were obtained. The IED event data is analyzed in time and space with no clear resemblance to a Poisson process in either domain. Consequently, a spatial clustering model is developed to model the collected data with high fidelity and few input parameters. Next, an IED emplacement model is developed to estimate emplacement times based on the interaction between the time and spatial dimensions of the friendly force data and the IED encounter data. Finally, simulated data is used to test the sensitivity of the model to a range of input parameters. From this we suggest improvements to the fidelity of the input data for the most accurate output results. [ABSTRACT FROM AUTHOR] Copyright of Military Operations Research Journal is the property of Military Operations Research Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=110365812&site=ehost-live
607,Transmission-Capacity Expansion for Minimizing Blackout Probabilities.,John Shortle,IEEE Transactions on Power Systems,8858950,,Jan-14,29,1,43,10,93281036,10.1109/TPWRS.2013.2279508,IEEE,Article,ELECTRIC power distribution grids; ELECTRIC power failures; MATHEMATICAL optimization; SIMULATION methods & models; MONTE Carlo method,Cascading blackouts; Computational modeling; Linear programming; Load modeling; Mathematical model; Optimization; Power system faults; Power system protection; rare-event simulation; simulation optimization; splitting; transmission expansion,"The objective of this paper is to determine an optimal plan for expanding the capacity of a power grid in order to minimize the likelihood of a large cascading blackout. Capacity-expansion decisions considered in this paper include the addition of new transmission lines and the addition of capacity to existing lines. We embody these interacting considerations in a simulation optimization model, where the objective is to minimize the probability of a large blackout subject to a budget constraint. The probability of a large-scale blackout is estimated via Monte Carlo simulation of a probabilistic cascading blackout model. Because the events of interest are rare, standard simulation is often intractable from a computational perspective. We apply a variance-reduction technique within the simulation to provide results in a reasonable time frame. Numerical results are given for some small test networks including an IEEE 14-bus test network. A key conclusion is that the different expansion strategies lead to different shapes of the tails of the blackout distributions. In other words, there is a tradeoff between reducing the frequency of small-scale blackouts versus reducing the frequency of large-scale blackouts. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Power Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=93281036&site=ehost-live
608,Waiting-Time Distribution of M/DN/1 Queues Through Numerical Laplace Inversion.,John Shortle,INFORMS Journal on Computing,10919856,,Winter2007,19,1,112,9,24775526,10.1287/ijoc.1050.0148,INFORMS: Institute for Operations Research,Article,"QUEUING theory; STOCHASTIC processes; OPERATIONS research; LAPLACE transformation; OPERATIONAL calculus; DIFFERENTIAL equations; MATHEMATICAL transformations; Process, Physical Distribution, and Logistics Consulting Services",Laplace transform inversion; M/Discrete/ 1 queues; numerical methods,"This paper considers an M/G/1 queue where the service time for each customer is a discrete random variable taking one of N values. We call this an M/DN/1 queue. There are potential numerical problems inverting Laplace transforms associated with this queue because the service distribution is discontinuous. The purpose of this paper is to investigate the performance of numerical transform-inversion methods in analyzing this queue. We first derive continuity properties of the steady-state distribution of wait in the M/DN/1 queue. Then, we show analytically how continuity properties affect the performance of the Fourier method for inverting transforms. In particular, continuity is not required in all derivatives for best performance of the method. We also present a new inversion method specifically for the M/DN/1 queue. Finally, we give numerical experiments comparing these and four other inversion methods. Although no method clearly dominates, the recursion method performs well in most examples. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24775526&site=ehost-live
609,Biomechanics of Walking in Healthy Adults at Different Gait Speeds: 121 Board #2 May 30 9:30 AM - 11:30 AM.,Siddhartha Sikdar,Medicine & Science in Sports & Exercise,1959131,,2018 Supplement 1,50,,10,1,133521716,10.1249/01.mss.0000535118.61829.e2,Lippincott Williams & Wilkins,Abstract,CONFERENCES & conventions; BIOMECHANICS; WALKING; WALKING speed; ADULTS; MINNESOTA; AMERICAN College of Sports Medicine; Convention and Trade Show Organizers,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133521716&site=ehost-live
610,Knee joint angular velocities and accelerations during the patellar tendon jerk,Siddhartha Sikdar,Journal of Neuroscience Methods,1650270,,Jun-11,198,2,255,5,61172286,10.1016/j.jneumeth.2011.04.018,Elsevier B.V.,Article,KNEE; TENDON reflex; STRETCH reflex; ACCELERATION (Mechanics); MAGNETIC resonance imaging; ACTIVITIES of daily living; Diagnostic Imaging Centers,"activities of daily living ( ADL ); coefficients of linear function y(x) with equation y =Ax +B ( A, B ); estimated angular knee extension acceleration due to tendon tap (rads−2) ( α ); estimated angular knee extension velocity due to tendon tap (rads−1) ( ω ); first component of stretch reflex ( M1 ); Human; Imaging; magnetic resonance imaging ( MRI ); maximum knee angular velocity during passive knee flexion, following reflexive knee extension caused by TJ (rads−1) ( ω p ); maximum longitudinal elongation acceleration during tendon tap (cms−2) ( a ); maximum longitudinal elongation velocity of the muscle after TJ (cms−1) ( v el ); maximum longitudinal elongation velocity of the muscle during passive knee flexion following reflexive knee extension caused by TJ (cms−1) ( v p ); Monosynaptic reflex; number of subjects in the study ( N ); Rectus femoris; Stretch reflex; stretch reflex ( SR ); tendon jerk, patellar tendon tap ( TJ ); Ultrasound; vector tissue Doppler imaging ( TDI )","Abstract: Tendon jerk (TJ) is one of the most commonly used clinical tests in differential diagnosis of human motor disorders. There remains some ambiguity in the physiological interpretation of the test, especially with respect to its association to the functional status of patients. The TJ test inputs a non-physiological stimuli, but it is unclear to what degree the kinematics generated during the TJ test exceed the ranges that muscles encounter in activities of daily living (ADLs). The aim of our pilot study was to determine the range of angular knee kinematics (angular velocities and accelerations) corresponding to the muscle stretch elicited by TJ. We measured the longitudinal kinematics (velocities and accelerations) of the rectus femoris muscle in vivo using vector tissue Doppler imaging, an ultrasound-based method, and measured the angular kinematics of the knee in response to tendon taps with an electrogoniometer. We concluded that muscle longitudinal elongation accelerations elicited during the standard TJ test exceed angular accelerations (104.40–4534.20rads−2) encountered in typical ADLs, but the velocities (0.82–6.21rads−1) elicited do not exceed those elicited by ADLs. [Copyright &y& Elsevier] Copyright of Journal of Neuroscience Methods is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=61172286&site=ehost-live
611,Proprioceptive Sonomyographic Control: A novel method for intuitive and proportional control of multiple degrees-of-freedom for individuals with upper extremity limb loss.,Siddhartha Sikdar,Scientific Reports,20452322,,7/1/19,9,1,N.PAG,1,137275945,10.1038/s41598-019-45459-7,Springer Nature,Article,,,"Technological advances in multi-articulated prosthetic hands have outpaced the development of methods to intuitively control these devices. In fact, prosthetic users often cite ""difficulty of use"" as a key contributing factor for abandoning their prostheses. To overcome the limitations of the currently pervasive myoelectric control strategies, namely unintuitive proportional control of multiple degrees-of-freedom, we propose a novel approach: proprioceptive sonomyographiccontrol. Unlike myoelectric control strategies which measure electrical activation of muscles and use the extracted signals to determine the velocity of an end-effector; our sonomyography-based strategy measures mechanical muscle deformation directly with ultrasound and uses the extracted signals to proportionally control the position of an end-effector. Therefore, our sonomyography-based control is congruent with a prosthetic user's innate proprioception of muscle deformation in the residual limb. In this work, we evaluated proprioceptive sonomyographic control with 5 prosthetic users and 5 able-bodied participants in a virtual target achievement and holding task for 5 different hand motions. We observed that with limited training, the performance of prosthetic users was comparable to that of able-bodied participants and thus conclude that proprioceptive sonomyographic control is a robust and intuitive prosthetic control strategy. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137275945&site=ehost-live
612,Semiautomatic segmentation of atherosclerotic carotid artery wall volume using 3D ultrasound imaging.,Siddhartha Sikdar,Medical Physics,942405,,Apr-15,42,4,2029,15,101903559,10.1118/1.4915925,"John Wiley & Sons, Inc.",Article,ATHEROSCLEROTIC plaque; ULTRASONIC imaging; THREE-dimensional imaging; STROKE; CAROTID artery abnormalities,3D US imaging; adventitial wall boundary; Bifurcations; Biological material; Biomedical instrumentation and transducers; biomedical transducers; biomedical ultrasonics; blood vessels; carotid segmentation; Diagnosis using ultrasonic; Digital computing or data processing equipment or methods; e.g. blood; Image data processing or generation; image reconstruction; image segmentation; Image sensors; in general; including micro‐electro‐mechanical systems (MEMS); level set method; Logic and set theory; lumen intima boundary; Magnetohydrodynamics; media adventitia boundary; medical image processing; Medical image segmentation; Processes or apparatus for generating mechanical vibrations of infrasonic; Reconstruction; Segmentation; set theory; sonic or infrasonic waves; sonic or ultrasonic frequency; specially adapted for specific applications; Speckle; stopping criteria; stroke; Three dimensional image processing; Transducers; ultrasonic transducers; Ultrasonographic imaging; Ultrasonography; urine; Haemocytometers; Vascular system; vessel wall volume,"Purpose: Rupture of atherosclerotic plaques in the carotid artery has been implicated in 20% of strokes. 3D ultrasound (US) imaging is emerging as an attractive method to quantify plaque burden and track changes in plaque longitudinally over time. However, plaque segmentation from US images is challenging because of poor boundary contrast and shadowing. The objective of this study is to develop and evaluate a semiautomatic segmentation algorithm with a novel stopping criterion for segmenting outer wall boundary (OWB) and lumen intima boundary (LIB) of common, internal, and external carotid artery from 3D US images for quantifying the vessel wall volume (VWV). Methods: 3D US image volumes were acquired from ten subjects with asymptomatic carotid stenoses. Volumes were acquired using a mechanically scanned linear probe, and the reconstructed volume consisted of 21 slices acquired at an interslice distance of 1 mm. The authors used distance regularized level set method with edge-based energy, region-based energy, smoothness energy, and a novel stopping criterion to segment the LIB and OWB of carotid artery. The algorithm was initialized by six user-selected points on the LIB and OWB in seven 2D cross-sectional slices in each volume. An ellipse fitting and a stopping boundary-based energy is proposed to smooth the OWB contour and to stop leaking of the evolving contour, respectively. The algorithm was compared against ground truth boundaries generated from manual segmentations. The dice similarity coefficient (DSC), Hausdorff distance (HD), and modified HD (MHD) were used as error metrics. Results: The authors' proposed stopping boundary energy-based stopping criterion was compared with percentage change of area and change of the MHD between evolving contours at successive iterations stopping criteria. The performance of the proposed algorithm was better than other two stopping criteria and yielded mean of LIBDSC = 88.78%, OWBDSC = 94.81%, LIBMHD = 0.26 mm, OWBMHD = 0.25 mm, LIBHD = 0.74 mm, and OWBHD = 0.80 mm. The Bland-Altman plot and correlation coefficient (r = 0.99) indicated a high agreement between ground truth and algorithm-generated boundaries. The coefficient of variation (COV) and minimum detectable change of the VWV are 5.2% and 57.2 mm³ (5.18% of mean VWV), calculated from repeated measurements of the VWV by algorithm. The mean absolute distance between corresponding points of the algorithm-generated and the ground truth boundaries was 0.25 mm. Conclusions: The authors have developed a semiautomatic segmentation algorithm for measuring the VWV of the carotid artery using 3D US images with reduced operator interaction and computational time and higher reproducibility using a commercially available 3D US transducer. Their method is a step forward toward routine longitudinal monitoring of 3D plaque progression. [ABSTRACT FROM AUTHOR] Copyright of Medical Physics is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101903559&site=ehost-live
613,A Pseudo-Likelihood Approach to Linear Regression With Partially Shuffled Data.,Martin Slawski,Journal of Computational & Graphical Statistics,10618600,,Oct-Dec 2021,30,4,991,13,154362940,10.1080/10618600.2020.1870482,Taylor & Francis Ltd,Article,ACQUISITION of data; CONFIDENCE intervals; PARAMETER estimation; EXPECTATION-maximization algorithms; DATA integration,Broken sample problem; Expectation-maximization algorithm; Mixture models; Pseudo-likelihood; Record linkage,"Recently, there has been significant interest in linear regression in the situation where predictors and responses are not observed in matching pairs corresponding to the same statistical unit as a consequence of separate data collection and uncertainty in data integration. Mismatched pairs can considerably impact the model fit and disrupt the estimation of regression parameters. In this article, we present a method to adjust for such mismatches under ""partial shuffling"" in which a sufficiently large fraction of (predictors, response)-pairs are observed in their correct correspondence. The proposed approach is based on a pseudo-likelihood in which each term takes the form of a two-component mixture density. expectation-maximization schemes are proposed for optimization, which (i) scale favorably in the number of samples, and (ii) achieve excellent statistical performance relative to an oracle that has access to the correct pairings as certified by simulations and case studies. In particular, the proposed approach can tolerate considerably larger fraction of mismatches than existing approaches, and enables estimation of the noise level as well as the fraction of mismatches. Inference for the resulting estimator (standard errors, confidence intervals) can be based on established theory for composite likelihood estimation. Along the way, we also propose a statistical test for the presence of mismatches and establish its consistency under suitable conditions. Supplemental files for this article are available online. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational & Graphical Statistics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154362940&site=ehost-live
614,Large-scale Cost-Aware Classification Using Feature Computational Dependency Graph.,Martin Slawski,IEEE Transactions on Knowledge & Data Engineering,10414347,,May-21,33,5,2029,16,149773599,10.1109/TKDE.2019.2948607,IEEE,Article,FEATURE selection; DATA mining; MACHINE learning; LOSSLESS data compression; MATHEMATICAL optimization; CLASSIFICATION,Computational modeling; cost-sensitive learning; Data models; Feature computational dependency; Feature extraction; Machine learning; Optimization; Runtime; Standards,"With the rapid growth of real-time machine learning applications, the process of feature selection and model optimization requires to integrate with the constraints on computational budgets. A specific computational resource in this regard is the time needed for evaluating predictions on test instances. The joint optimization problem of prediction accuracy and prediction-time efficiency draws more and more attention in the data mining and machine learning communities. The runtime cost is dominated by the feature generation process that contains significantly redundant computations across different features that sharing the same computational component in practice. Eliminating such redundancies would obviously reduce the time costs in the feature generation process. Our previous Cost-aware classification using Feature computational dependencies heterogeneous Hypergraph (CAFH) model has achieved excellent performance on the effectiveness. In the big data era, the high dimensionality caused by the heterogeneous data sources leads to the difficulty in fitting the entire hypergraph into the main memory and the high computational cost during the optimization process. Simply partitioning the features into batches cannot give the optimal solution since it will lose some feature dependencies across the batches. To improve the high memory and computational costs in the CAFH model, we propose an equivalent Accelerated CAFH (ACAFH) model based on the lossless heterogeneous hypergraph decomposition. An efficient and effective nonconvex optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed to optimize the ACAFH model. The time and space complexities of the optimization algorithm for the ACAFH model are three and one polynomial degrees less than our previous algorithm for the CAFH model, respectively. Extensive experiments demonstrate the proposed ACAFH model achieves competitive performance on the effectiveness and much better performance on the efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149773599&site=ehost-live
614,Large-scale Cost-Aware Classification Using Feature Computational Dependency Graph.,Kai Zeng,IEEE Transactions on Knowledge & Data Engineering,10414347,,May-21,33,5,2029,16,149773599,10.1109/TKDE.2019.2948607,IEEE,Article,FEATURE selection; DATA mining; MACHINE learning; LOSSLESS data compression; MATHEMATICAL optimization; CLASSIFICATION,Computational modeling; cost-sensitive learning; Data models; Feature computational dependency; Feature extraction; Machine learning; Optimization; Runtime; Standards,"With the rapid growth of real-time machine learning applications, the process of feature selection and model optimization requires to integrate with the constraints on computational budgets. A specific computational resource in this regard is the time needed for evaluating predictions on test instances. The joint optimization problem of prediction accuracy and prediction-time efficiency draws more and more attention in the data mining and machine learning communities. The runtime cost is dominated by the feature generation process that contains significantly redundant computations across different features that sharing the same computational component in practice. Eliminating such redundancies would obviously reduce the time costs in the feature generation process. Our previous Cost-aware classification using Feature computational dependencies heterogeneous Hypergraph (CAFH) model has achieved excellent performance on the effectiveness. In the big data era, the high dimensionality caused by the heterogeneous data sources leads to the difficulty in fitting the entire hypergraph into the main memory and the high computational cost during the optimization process. Simply partitioning the features into batches cannot give the optimal solution since it will lose some feature dependencies across the batches. To improve the high memory and computational costs in the CAFH model, we propose an equivalent Accelerated CAFH (ACAFH) model based on the lossless heterogeneous hypergraph decomposition. An efficient and effective nonconvex optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed to optimize the ACAFH model. The time and space complexities of the optimization algorithm for the ACAFH model are three and one polynomial degrees less than our previous algorithm for the CAFH model, respectively. Extensive experiments demonstrate the proposed ACAFH model achieves competitive performance on the effectiveness and much better performance on the efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149773599&site=ehost-live
615,On the Trade-Off Between Bit Depth and Number of Samples for a Basic Approach to Structured Signal Recovery From $b$ -Bit Quantized Linear Measurements.,Martin Slawski,IEEE Transactions on Information Theory,189448,,Jun-18,64,6,4159,20,129840965,10.1109/TIT.2018.2826459,IEEE,Article,QUANTUM groups; LINEAR statistical models; GAUSSIAN measures; QUANTIZATION (Physics); COMPRESSED sensing,Additives; Compressed sensing; Gaussian noise; Gaussian width; low-complexity signals; Noise level; Noise measurement; quantization; Quantization (signal); Signal to noise ratio,"We consider the problem of recovering a high-dimensional structured signal from independent Gaussian linear measurements each of which is quantized to $b$ bits. The focus is on a specific method of signal recovery that extends a procedure originally proposed by Plan and Vershynin for one-bit quantization to a multi-bit setting. At the heart of this paper is a characterization of the optimal trade-off between the number of measurements m$ and the bit depth per measurement b$ given a total budget of B = m \cdot b -error in estimating the signal. It turns out that the choice b = 1$ is optimal for estimating the unit vector (direction) corresponding to the signal for any level of additive Gaussian noise before quantization as well as for a specific model of adversarial noise, whereas in a noiseless setting the choice b = 2$ is optimal for estimating the direction and the norm (scale) of the signal. Moreover, Lloyd–Max quantization is shown to be an optimal quantization scheme with respect to -estimation error. Our analysis is corroborated by the numerical experiments showing nearly perfect agreement with our theoretical predictions. This paper is complemented by an empirical comparison to alternative methods of signal recovery. The results of that comparison point to a regime change depending on the noise level: in a low-noise setting, the approach under study falls short of more sophisticated competitors while being competitive in moderate- and high-noise settings. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Information Theory is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129840965&site=ehost-live
616,Order-Constrained ROC Regression With Application to Facial Recognition.,Martin Slawski,Technometrics,401706,,Aug-21,63,3,343,11,151763386,10.1080/00401706.2020.1785549,Taylor & Francis Ltd,Article,,Area under the ROC curve; Biometric matching; Facial recognition; Order constraint; ROC regression; Stochastic ordering,"The receiver operating characteristic (ROC) curve is widely used to assess discriminative accuracy of two groups based on a continuous score. In a variety of applications, the distributions of such scores across the two groups exhibit a stochastic ordering. Specific examples include calibrated biomarkers in medical diagnostics or the output of matching algorithms in biometric recognition. Incorporating stochastic ordering as an additional constraint into estimation can improve statistical efficiency. In this article, we consider modeling of ROC curves using both the order constraint and covariates associated with each score given that the latter (e.g., demographic characteristics of the underlying subjects) often have a substantial impact on discriminative accuracy. The proposed method is based on the indirect ROC regression approach using a location-scale model, and quadratic optimization is used to implement the order constraint. The statistical properties of the proposed order-constrained least squares estimator are studied. Based on the theoretical results developed herein, we deduce that the proposed estimator can achieve substantial reductions in mean squared error relative to its unconstrained counterpart. Simulation studies corroborate the superior performance of the proposed approach. Its practical usefulness is demonstrated in an application to face recognition data from the ""Good, Bad, and Ugly"" face challenge, a domain in which accounting for covariates has hardly been studied. [ABSTRACT FROM AUTHOR] Copyright of Technometrics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151763386&site=ehost-live
617,Regression with linked datasets subject to linkage error.,Martin Slawski,WIREs: Computational Statistics,19395108,,Jul-22,14,4,1,19,157874736,10.1002/wics.1570,Wiley-Blackwell,Article,REGRESSION analysis; DATA analysis; BAYESIAN analysis; DATA integration; EMPIRICAL research,Bayesian analysis; data integration; linkage error; mixture models; record linkage; regression,"Data are often collected from multiple heterogeneous sources and are combined subsequently. In combing data, record linkage is an essential task for linking records in datasets that refer to the same entity. Record linkage is generally not error‐free; there is a possibility that records belonging to different entities are linked or that records belonging to the same entity are missed. It is not advisable to simply ignore such errors because they can lead to data contamination and introduce bias in sample selection or estimation, which, in return, can lead to misleading statistical results and conclusions. For a long while, this problem was not properly recognized, but in recent years a growing number of researchers have developed methodology for dealing with linkage errors in regression analysis with linked datasets. The main goal of this overview is to give an account of those developments, with an emphasis on recent approaches and their connection to the so‐called ""Broken Sample"" problem. We also provide a short empirical study that illustrates the efficacy of corrective methods in different scenarios. This article is categorized under:Statistical Models > Model SelectionStatistical and Graphical Methods of Data Analysis > Robust MethodsStatistical and Graphical Methods of Data Analysis > Multivariate Analysis [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157874736&site=ehost-live
618,Two-Stage Approach to Multivariate Linear Regression with Sparsely Mismatched Data.,Martin Slawski,Journal of Machine Learning Research,15324435,,2020,21,189-216,1,42,146744451,,Microtome Publishing,Article,SIGNAL-to-noise ratio; PERMUTATIONS; DATA logging,,"A tacit assumption in linear regression is that (response, predictor)-pairs correspond to identical observational units. A series of recent works have studied scenarios in which this assumption is violated under terms such as \Unlabeled Sensing and \Regression with Unknown Permutation"". In this paper, we study the setup of multiple response variables and a notion of mismatches that generalizes permutations in order to allow for missing matches as well as for one-to-many matches. A two-stage method is proposed under the assumption that most pairs are correctly matched. In the first stage, the regression parameter is estimated by handling mismatches as contaminations, and subsequently the generalized permutation is estimated by a basic variant of matching. The approach is both computationally convenient and equipped with favorable statistical guarantees. Specifically, it is shown that the conditions for permutation recovery become considerably less stringent as the number of responses m per observation increase. Particularly, for m = (log n), the required signal-to-noise ratio no longer depends on the sample size n. Numerical results on synthetic and real data are presented to support the main findings of our analysis. [ABSTRACT FROM AUTHOR] Copyright of Journal of Machine Learning Research is the property of Microtome Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146744451&site=ehost-live
619,Development and evaluation of optimal prostate biopsy protocols for individual groups of patients,Ariela Sofer,International Congress Series,5315131,,Jul-04,1268,,803,6,13327498,10.1016/j.ics.2004.03.347,Elsevier B.V.,Article,PROSTATE cancer; BIOPSY; MALE reproductive organs; PATIENTS,Age-specific; Biopsy simulation; Needle biopsy protocols; Prostate cancer; Race-specific; Volume-specific; Optimization,"We have developed a number of optimal biopsy protocols for the detection of prostate cancer for both general and specific groups of patients. Age, race and prostate volume were considered in the development and optimal protocols were developed for individual patients based on these factors. Experimental results showed that optimal 6-needle protocols outperformed traditional sextant protocol for all age, race and prostate volume groups of patients, and the difference is statistically significant. We did not find significant improvements for developing protocols for age-specific, race-specific or prostate volume-specific patients in this preliminary study. Our results also indicated that more biopsy cores should be used in older men to achieve a targeted detection rate. [Copyright &y& Elsevier] Copyright of International Congress Series is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=13327498&site=ehost-live
620,Preface.,Ariela Sofer,Annals of Operations Research,2545330,,Nov-06,148,1,1,3,23302529,10.1007/s10479-006-0088-6,Springer Nature,Article,PREFACES & forewords; SMALLPOX,,The article discusses various reports within the issue including a dynamic epidemic-intervention model that captures key features of mass vaccination for smallpox by Kress and a real-time decision support system that incorporates efficient optimization technology seamlessly with a simulation module by Lee et al.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23302529&site=ehost-live
621,Bayesian regularization: From Tikhonov to horseshoe.,Vadim Sokolov,WIREs: Computational Statistics,19395108,,Jul-19,11,4,N.PAG,1,136997958,10.1002/wics.1463,Wiley-Blackwell,Article,TIKHONOV regularization; STATISTICAL learning; MATHEMATICAL regularization; MACHINE learning; DATA analysis; LITERATURE reviews,Bayesian regression; horseshoe; lasso; regularization,"Bayesian regularization is a central tool in modern‐day statistical and machine learning methods. Many applications involve high‐dimensional sparse signal recovery problems. The goal of our paper is to provide a review of the literature on penalty‐based regularization approaches, from Tikhonov (Ridge, Lasso) to horseshoe regularization. This article is categorized under:Statistical and Graphical Methods of Data Analysis > Robust MethodsStatistical Models > Linear ModelsStatistical Models > Bayesian Models [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136997958&site=ehost-live
622,Deep learning for short-term traffic flow prediction.,Vadim Sokolov,Transportation Research: Part C,0968090X,,Jun-17,79,,1,17,122700316,10.1016/j.trc.2017.02.024,Elsevier B.V.,Article,TRAFFIC flow; DEEP learning; TRAFFIC estimation; REGULARIZATION parameter; NONLINEAR statistical models,Deep Learning; Sparse linear models; Traffic Flows; Trend filtering,"We develop a deep learning model to predict traffic flows. The main contribution is development of an architecture that combines a linear model that is fitted using ℓ 1 regularization and a sequence of tanh layers. The challenge of predicting traffic flows are the sharp nonlinearities due to transitions between free flow, breakdown, recovery and congestion. We show that deep learning architectures can capture these nonlinear spatio-temporal effects. The first layer identifies spatio-temporal relations among predictors and other layers model nonlinear relations. We illustrate our methodology on road sensor data from Interstate I-55 and predict traffic flows during two special events; a Chicago Bears football game and an extreme snowstorm event. Both cases have sharp traffic flow regime changes, occurring very suddenly, and we show how deep learning provides precise short term traffic flow predictions. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122700316&site=ehost-live
623,Deep learning: Computational aspects.,Vadim Sokolov,WIREs: Computational Statistics,19395108,,Sep/Oct2020,12,5,1,17,145036839,10.1002/wics.1500,Wiley-Blackwell,Article,STATISTICAL learning; DEEP learning; LINEAR algebra; LATENT variables; DATA science,deep learning; linear algebra; stochastic gradient descent,"In this article, we review computational aspects of deep learning (DL). DL uses network architectures consisting of hierarchical layers of latent variables to construct predictors for high‐dimensional input–output models. Training a DL architecture is computationally intensive, and efficient linear algebra library is the key for training and inference. Stochastic gradient descent (SGD) optimization and batch sampling are used to learn from massive datasets. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Deep Learning Statistical Learning and Exploratory Methods of the Data Sciences > Modeling Methods Statistical Learning and Exploratory Methods of the Data Sciences > Neural Networks [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145036839&site=ehost-live
624,Building Security into Off-the-Shelf Smartphones.,Angelos Stavrou,Computer (00189162),189162,,Feb-12,45,2,82,0,73617961,10.1109/MC.2012.44,IEEE,Article,"MOBILE communication systems; MALWARE; COMPUTER software; APPLICATION software; COMPUTER security; Custom Computer Programming Services; Software publishers (except video game publishers); Software Publishers; Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; Wireless Telecommunications Carriers (except Satellite)",Androids; application testing; Computer security; Malware; Mobile communication; mobile computing; NIST; security; Smart phones,"Quantifying mobile application functionality and enforcing a finer-grained permission model would identify and thus thwart a wide range of malware. [ABSTRACT FROM PUBLISHER] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=73617961&site=ehost-live
625,DDoS in the IoT: Mirai and Other Botnets.,Angelos Stavrou,Computer (00189162),189162,,Jul-17,50,7,80,5,124027610,10.1109/MC.2017.201,IEEE,Article,"INTERNET of things; INTERNET security; BOTNETS; DENIAL of service attacks; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Internet Publishing and Broadcasting and Web Search Portals; Wired Telecommunications Carriers",botnet; BrickerBot; Computer crime; Computer security; cybersecurity; Cybertrust; DDoS; distributed denial of service; Distributed processing; Hajime; Internet; Internet of Things; IoT; IP networks; malware; Mirai; Persirai; Ports (Computers); Risk management; security; Servers,"The Mirai botnet and its variants and imitators are a wake-up call to the industry to better secure Internet of Things devices or risk exposing the Internet infrastructure to increasingly disruptive distributed denial-of-service attacks. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124027610&site=ehost-live
626,On the Move: Evading Distributed Denial-of-Service Attacks.,Angelos Stavrou,Computer (00189162),189162,,Mar-16,49,3,104,4,113872710,10.1109/MC.2016.85,IEEE,Article,"DENIAL of service attacks; CLIENT/SERVER computing; COMPUTERS; COMPUTER networks; CYBERTERRORISM; Computer Systems Design Services; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Electronic Computer Manufacturing; Computer and peripheral equipment manufacturing; Electronics Stores",Computer crime; Computer security; Computers; Cybertrust; DDoS; distributed denial-of-service; Internet; IP networks; MOTAG; moving-target defense; Productivity; security; Servers; Service agreements,"A proposed moving-target defense against DDoS attacks repeatedly shuffles client-to-server assignments to identify and eventually quarantine malicious clients. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=113872710&site=ehost-live
627,"Securely Making ""Things"" Right.",Angelos Stavrou,Computer (00189162),189162,,Sep-15,48,9,84,5,109993862,10.1109/MC.2015.258,IEEE,Article,INTERNET of things; INFORMATION technology security; COMPUTER security; CYBERINFRASTRUCTURE; INTERNET; COMPUTER worms; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,Communication system security; Computer security; Cryptography; cybersecurity; Internet of things; IoT; IoT security; Privacy; Security; Wireless communication,"The Internet of Things (IoT) promises to seamlessly bind the physical world to cyberinfrastructure, but the Internet's insecure design principles could lead to life-threatening consequences. It's time to make security an integral IoT design tenet. [ABSTRACT FROM PUBLISHER] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109993862&site=ehost-live
628,Verified Time.,Angelos Stavrou,Computer (00189162),189162,,Mar-17,50,3,78,5,122302090,10.1109/MC.2017.63,IEEE,Article,"TIMESTAMPS; TIME measurements; DATA loggers; DIGITAL media; BLOCKCHAINS; DISTRIBUTED computing; Internet Publishing and Broadcasting and Web Search Portals; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables",blockchains; Internet of Things; IoT; security; timestamping; timestamping authority; TSA,"A provably secure timestamping system could be achieved by combining the trustworthiness and accuracy that comes from having a set of trusted centralized timestamping authorities with the open, decentralized nature of blockchain protocols. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122302090&site=ehost-live
629,Virtual Extension The Ephemeral Legion: Producing an Expert Cyber-Security Work Force from Thin Air.,Angelos Stavrou,Communications of the ACM,10782,,Jan-11,54,1,129,3,56676261,10.1145/1866739.1866764,Association for Computing Machinery,Opinion,INTERNET security; EMPLOYEE training; EMPLOYEE education; CERTIFICATION; UNITED States; Professional and Management Development Training; Internet Publishing and Broadcasting and Web Search Portals; Wired Telecommunications Carriers,,"The article presents the authors' views on developing a cyber-security work force in the United States. The strategy of training new information security workers through education rather than mass certification of the existing cyber-security work force is discussed. Government policy recommendations include funding cyber-security education, developing high-school pilot programs, providing guidance to ethical hackers, and expanding security expertise at universities.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=56676261&site=ehost-live
630,Development of Predictive Informatics Tool Using Electronic Health Records to Inform Personalized Evidence-Based Pressure Injury Management for Veterans with Spinal Cord Injury.,Jiayang Sun,Military Medicine,264075,,2021 Supplement,186,,651,8,148660258,10.1093/milmed/usaa469,Oxford University Press / USA,journal article,,,"<bold>Background: </bold>Pressure injuries (PrI) are serious complications for many with spinal cord injury (SCI), significantly burdening health care systems, in particular the Veterans Health Administration. Clinical practice guidelines (CPG) provide recommendations. However, many risk factors span multiple domains. Effective prioritization of CPG recommendations has been identified as a need. Bioinformatics facilitates clinical decision support for complex challenges. The Veteran's Administration Informatics and Computing Infrastructure provides access to electronic health record (EHR) data for all Veterans Health Administration health care encounters. The overall study objective was to expand our prototype structural model of environmental, social, and clinical factors and develop the foundation for resource which will provide weighted systemic insight into PrI risk in veterans with SCI.<bold>Methods: </bold>The SCI PrI Resource (SCI-PIR) includes three integrated modules: (1) the SCIPUDSphere multidomain database of veterans' EHR data extracted from October 2010 to September 2015 for ICD-9-CM coding consistency together with tissue health profiles, (2) the Spinal Cord Injury Pressure Ulcer and Deep Tissue Injury Ontology (SCIPUDO) developed from the cohort's free text clinical note (Text Integration Utility) notes, and (3) the clinical user interface for direct SCI-PIR query.<bold>Results: </bold>The SCI-PIR contains relevant EHR data for a study cohort of 36,626 veterans with SCI, representing 10% to 14% of the U.S. population with SCI. Extracted datasets include SCI diagnostics, demographics, comorbidities, rurality, medications, and laboratory tests. Many terminology variations for non-coded input data were found. SCIPUDO facilitates robust information extraction from over six million Text Integration Utility notes annually for the study cohort. Visual widgets in the clinical user interface can be directly populated with SCIPUDO terms, allowing patient-specific query construction.<bold>Conclusion: </bold>The SCI-PIR contains valuable clinical data based on CPG-identified risk factors, providing a basis for personalized PrI risk management following SCI. Understanding the relative impact of risk factors supports PrI management for veterans with SCI. Personalized interactive programs can enhance best practices by decreasing both initial PrI formation and readmission rates due to PrI recurrence for veterans with SCI. [ABSTRACT FROM AUTHOR] Copyright of Military Medicine is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148660258&site=ehost-live
631,"Subsampling from features in large regression to find ""winning features"".",Jiayang Sun,Statistical Analysis & Data Mining,19321864,,Apr-21,14,2,168,17,149246460,10.1002/sam.11499,Wiley-Blackwell,Article,FEATURE selection; RANDOM forest algorithms; OVARIAN cancer; GENES; DATA science,feature selection; high dimensions; subsampling winner algorithm (SWA); variable selection,"Feature (or variable) selection from a large number of p features continuously challenges data science, especially for ever‐enlarging data and in discovering scientifically important features in a regression setting. For example, to develop valid drug targets for ovarian cancer, we must control the false‐discovery rate (FDR) of a selection procedure. The popular approach to feature selection in large‐p regression uses a penalized likelihood or a shrinkage estimation, such as a LASSO, SCAD, Elastic Net, or MCP procedure. We present a different approach called the Subsampling Winner algorithm (SWA), which subsamples from p features. The idea of SWA is analogous to selecting US national merit scholars' that selects semifinalists based on student's performance in tests done at local schools (a.k.a. subsample analyses), and then determine the finalists (a.k.a. winning features) from the semifinalists. Due to its subsampling nature, SWA can scale to data of any dimension. SWA also has the best‐controlled FDR compared to the penalized and Random Forest procedures while having a competitive true‐feature discovery rate. Our application of SWA to an ovarian cancer data revealed functionally important genes and pathways. [ABSTRACT FROM AUTHOR] Copyright of Statistical Analysis & Data Mining is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149246460&site=ehost-live
632,Computer-Intensive Methods for Tests About the Mean of an Asymmetrical Distribution.,Clifton Sutton,Journal of the American Statistical Association,1621459,,Sep-93,88,423,802,9,9312090723,10.1080/01621459.1993.10476345,Taylor & Francis Ltd,Article,T-test (Statistics); DISTRIBUTION (Probability theory); PROBABILITY theory; ERRORS; STATISTICAL bootstrapping; RESAMPLING (Statistics); MONTE Carlo method,Bootstrap resampling; Johnson's modified t test; Population skewness; Robust hypothesis test,"For one-sided tests about the mean of a skewed distribution, the t test is asymptotically robust for validity, however, it can be quite inaccurate and inefficient with small sample sizes. Results presented here confirm that a procedure due to Johnson should be preferred to the t test when the parent distribution is asymmetrical, because it reduces the probability of type I error in cases where the t test has an inflated type I error rate and it is more powerful in other situations. But if the skewness is severe and the sample size is small, then Johnson's test can also be appreciably inaccurate. For such situations, computer-intensive test procedures using bootstrap resampling are proposed, and with an extensive Monte Carlo study it is shown that these procedures are remarkably robust and can result in reduced probabilities of type I and type II errors compared to Johnson's test. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Statistical Association is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=9312090723&site=ehost-live
633,Nearest-neighbor methods.,Clifton Sutton,WIREs: Computational Statistics,19395108,,May/Jun2012,4,3,307,0,74303888,10.1002/wics.1195,Wiley-Blackwell,Article,MATHEMATICAL statistics; STATISTICS; NEAREST neighbor analysis (Statistics); SPATIAL analysis (Statistics); PROBABILITY theory,classification; distance; local; majority,"Nearest-neighbor classification is a simple and popular method for supervised classification. The basic method is to classify a query point as being of a certain class if of the k-nearest neighbors of the query point, more of them belong to this class than to any other class. Variations of the basic method include weighted nearest-neighbor methods and adaptive nearest-neighbor methods (which allow some variables to have greater influence than other variables). WIREs Comput Stat 2012, 4:307-309. doi: 10.1002/wics.1195 For further resources related to this article, please visit the [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=74303888&site=ehost-live
634,The effect of oral irrigation with a magnetic water treatment device on plaque and calculus.,Clifton Sutton,Journal of Clinical Periodontology,3036979,,May-93,20,5,314,4,13444318,10.1111/j.1600-051X.1993.tb00366.x,Wiley-Blackwell,Article,DENTISTRY; PATIENTS; TEETH; MAGNETIC devices; WATER utilities; HISTOLOGY; Water Supply and Irrigation Systems,adherence supragingival calculus reduction; divalent cations.; irrigation; magnetic water treatment device; stern layer,"Calculus formation on tooth surfaces is analogous to the formation of lime and scale deposits in plumbing. Magnetic water devices have been shown to significantly reduce scale deposits in industry; therefore an oral irrigator with a magnetic water device may have a similar effect on calculus. To test this hypothesis, a double-blind clinical study was established using 64 irrigators, 30 of which had their magnetic devices removed. 54 patients with heavy supragingival calculus were given irrigators at random after prophylaxis. Instructions were given to irrigate twice a day, particularly the lower 6 anterior teeth. The patients were also told not to floss these 6 teeth which were to be the study teeth. They were examined after 3 months and measurements were taken of the accretions adhering to the study teeth. No attempt was made to determine whether the adhering material was hard or soft so it must be assumed that at least some of the measured material was also plaque. The measurements of the group using an irrigator with a magnetic device showed a 44% greater reduction in calculus volume (<em>p</em> <0.0005) and a 42% greater reduction in area (<em>p</em> <0.0001) over the group using an unmagnetized irrigator. There appears to be a statistically significant difference in supragingival accretion volumes between conventional irrigation and using an irrigator with a magnetic water treatment, device. [ABSTRACT FROM AUTHOR] Copyright of Journal of Clinical Periodontology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=13444318&site=ehost-live
635,A parametric study to improve suitability of micro-deval test to assess unbound base course aggregates.,Burak Tanyu,Construction & Building Materials,9500618,,Aug-17,147,,328,11,123373702,10.1016/j.conbuildmat.2017.04.173,Elsevier B.V.,Article,CONCRETE durability; ASPHALT concrete; STRENGTH of materials; ABRASION resistance,Aggregate; Asphalt concrete; Base; Durability; Micro-deval; Performance; Resistance; Unbound,"Micro-deval test provides a measure of resistance and durability of aggregates submerged in water through the actions of aggregate particles and abrasion from steel balls. The presence of water in the test chamber provides a more realistic assessment of field conditions when compared with Los Angeles (LA) abrasion test. Currently there is only one testing procedure in the United States (U.S.) to evaluate the coarse aggregates using micro-deval tests. Previous studies indicate that there has been significant interest in utilizing micro-deval test to assess the suitability coarse aggregates to be used in asphalt concrete and their field performance. However the studies related to evaluating coarse aggregates with micro-deval test for base course applications are very limited. This study attempts to address how the micro-deval testing procedure may be revised to better assess the performance of aggregates considered for the base course. Aggregates with seven different geological compositions were used in the study. Total of 250 micro-deval tests were performed and the results confirmed the repeatability and the suitability of the proposed method to all of the tested aggregates. As part of this study, a threshold is determined for the optimum amount of abrasive charge that should be used in the tests to obtain maximum material loss regardless of the geological make-up of the aggregate. If this approach is followed, a strong relationship between the number of revolutions and percent material loss is achieved. This relationship provides an opportunity to shorten the micro-deval tests and also an approach to assess durability of aggregates at number of revolutions significantly larger than what is prescribed in the existing micro-deval testing procedure. This ability may be used in the future to more realistically relate the long-term durability prediction of aggregates as it relates to field performance of base course. [ABSTRACT FROM AUTHOR] Copyright of Construction & Building Materials is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123373702&site=ehost-live
636,Laboratory evaluation of governing mechanism of frictionally connected MSEW face and implications on design.,Burak Tanyu,Geotextiles & Geomembranes,2661144,,Oct-14,42,5,468,11,98601103,10.1016/j.geotexmem.2014.07.006,Elsevier B.V.,Article,FRICTION; REINFORCED soils; GEOTEXTILES; SHEAR (Mechanics); CONCRETE; GEOMEMBRANES; Ready-Mix Concrete Manufacturing; All Other Miscellaneous Nonmetallic Mineral Product Manufacturing,"Concrete facing block; Connection strength; Geotextile; Laboratory testing; Pullout tests, and direct shear tests; Service and strength limit states","This paper presents a laboratory evaluation of purely frictionally connected geotextile and concrete facing block of Mechanically Stabilized Earth Wall (MSEW) systems. The study focuses on investigating the governing failure mechanism along the wall face, as determined from the pullout of reinforcement in between the facing blocks (herein referred as pullout mechanism) and sliding of the blocks over the geotextile, where the reinforcement stays stationery (herein referred as direct shear mechanism). A total of seventy-two tests were performed to investigate the effect of laboratory specimen size, difference in geotextile reinforcement, and repeatability of the test results. Overall, the results showed that at lower normal loads, sliding of the blocks over the geotextile reinforcement along the wall face is more likely to occur before the pullout of the geotextile in between the blocks. At higher normal loads, this order is reversed and pullout of the geotextile appears to occur first. The test results also indicated that the size of the specimen tested in the laboratory frictional connection evaluation has an effect on the measured connection strength. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98601103&site=ehost-live
637,"Landslide susceptibility analyses using Random Forest, C4.5, and C5.0 with balanced and unbalanced datasets.",Burak Tanyu,CATENA,3418162,,Aug-21,203,,N.PAG,1,150444585,10.1016/j.catena.2021.105355,Elsevier B.V.,Article,RANDOM forest algorithms; LANDSLIDE hazard analysis; VECTOR data; FEATURE selection; DECISION trees; CLIMATE change,Decision tree learning; Geotechnical properties; Landslide; Natural hazard; Susceptibility analysis,"• C4.5 and C5.0 outperform Random Forest in landslide susceptibility analyses. • Use of balance vector data improves the performance of Random Forest. • Including all conditioning factors improve landslide susceptibility results. • Difference in C4.5 and C5.0 results for raster and vector data is marginal. The effects of landslides have been exponentially increasing due to the rapid growth of urbanization and global climate change. The information gained from predictive models and landslide susceptibility analyses can be used to develop warning systems and mitigation measures. A comparative study was conducted to evaluate the effectiveness of landslide susceptibility analyses in a given area using three decision tree algorithms including Random Forest (RF), C4.5, and C5.0. Two sets of imagery datasets (raster and vector) were used and three combinations of 13 conditioning factors (including seven geotechnical properties of the soil) were determined by Information Gain, Gain Ratio, Chi-Squared Test, and Random Forest Importance. Datasets for the landslide conditioning factors were created based on the outcomes from the feature selection methods, in three different scenarios. In Scenario 1 the least important factors/features (as identified by information gain, chi-square, and gain ratio measures) were eliminated. In Scenario 2 only the most important factors (as identified by RF feature selection method evaluation) were kept. In Scenario 3, no factor was eliminated, using the data directly obtained from the sources without applying any feature selection method. The performances of the models were evaluated using statistical verification scores. C4.5 was found to have the highest performance when all 13 conditioning parameters (Scenario 3) were used for both the raster and vector data set. The RF model was the least effective in predicting the landslides in all three scenarios. However, the use of the balance vector dataset significantly increased the performance of the RF model. C4.5 and C5.0 had significantly better performance in handling extremely unbalance data in comparison to RF. Density, silt and clay content, and Atterberg's limits (LL and PI) were the most important geotechnical conditioning factors in the performed landslide susceptibility analyses. [ABSTRACT FROM AUTHOR] Copyright of CATENA is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150444585&site=ehost-live
637,"Landslide susceptibility analyses using Random Forest, C4.5, and C5.0 with balanced and unbalanced datasets.",Gheorghe Tecuci,CATENA,3418162,,Aug-21,203,,N.PAG,1,150444585,10.1016/j.catena.2021.105355,Elsevier B.V.,Article,RANDOM forest algorithms; LANDSLIDE hazard analysis; VECTOR data; FEATURE selection; DECISION trees; CLIMATE change,Decision tree learning; Geotechnical properties; Landslide; Natural hazard; Susceptibility analysis,"• C4.5 and C5.0 outperform Random Forest in landslide susceptibility analyses. • Use of balance vector data improves the performance of Random Forest. • Including all conditioning factors improve landslide susceptibility results. • Difference in C4.5 and C5.0 results for raster and vector data is marginal. The effects of landslides have been exponentially increasing due to the rapid growth of urbanization and global climate change. The information gained from predictive models and landslide susceptibility analyses can be used to develop warning systems and mitigation measures. A comparative study was conducted to evaluate the effectiveness of landslide susceptibility analyses in a given area using three decision tree algorithms including Random Forest (RF), C4.5, and C5.0. Two sets of imagery datasets (raster and vector) were used and three combinations of 13 conditioning factors (including seven geotechnical properties of the soil) were determined by Information Gain, Gain Ratio, Chi-Squared Test, and Random Forest Importance. Datasets for the landslide conditioning factors were created based on the outcomes from the feature selection methods, in three different scenarios. In Scenario 1 the least important factors/features (as identified by information gain, chi-square, and gain ratio measures) were eliminated. In Scenario 2 only the most important factors (as identified by RF feature selection method evaluation) were kept. In Scenario 3, no factor was eliminated, using the data directly obtained from the sources without applying any feature selection method. The performances of the models were evaluated using statistical verification scores. C4.5 was found to have the highest performance when all 13 conditioning parameters (Scenario 3) were used for both the raster and vector data set. The RF model was the least effective in predicting the landslides in all three scenarios. However, the use of the balance vector dataset significantly increased the performance of the RF model. C4.5 and C5.0 had significantly better performance in handling extremely unbalance data in comparison to RF. Density, silt and clay content, and Atterberg's limits (LL and PI) were the most important geotechnical conditioning factors in the performed landslide susceptibility analyses. [ABSTRACT FROM AUTHOR] Copyright of CATENA is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150444585&site=ehost-live
638,Methodology to develop design guidelines to construct unbound base course with reclaimed asphalt pavement (RAP).,Burak Tanyu,Construction & Building Materials,9500618,,Oct-19,223,,463,14,138458647,10.1016/j.conbuildmat.2019.06.196,Elsevier B.V.,Article,ASPHALT pavements; GUIDELINES; ASPHALT; VIRGINIA; Asphalt Shingle and Coating Materials Manufacturing; Petroleum Refineries; All Other Specialty Trade Contractors; Asphalt Paving Mixture and Block Manufacturing,Performance; Permanent deformation; RAP; Reclaimed asphalt pavement; Resilient modulus; Sustainability; Unbound base aggregate,"Reclaimed asphalt pavement (RAP) has been considered as one of the sources to create unbound base aggregate (UBA) for pavement structures for several years. There are number of previous research studies that focuses on testing the modulus and deformation characteristics of the RAP and RAP blended with virgin aggregate (VA), however results vary significantly from one study to another and there are no clear-cut guidelines on how much RAP should be blended with VA for the best outcome. In this study, the focus of the research was to develop a methodology that will allow the Owner's to develop design guidelines to create UBA with RAP-VA blends where the created blend will have similar performance criteria as the VA used in that region. To achieve this goal, factors affecting the performance of RAP-VA blends have been investigated and a guideline to select and how to meet the target modulus and deformation thresholds are presented. RAP obtained from 14 different asphalt plants and virgin aggregate collected from a source in Virginia have been investigated. The results showed that although there are differences in RAP from one source to another, following the proposed methodology in this article, it is possible to recycle RAP to create UBA that will have similar performance criteria as the VA by itself. [ABSTRACT FROM AUTHOR] Copyright of Construction & Building Materials is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138458647&site=ehost-live
639,Numerical analysis of instrumented mechanically stabilized gabion walls with large vertical reinforcement spacing.,Burak Tanyu,Geotextiles & Geomembranes,2661144,,Aug-17,45,4,294,13,123574063,10.1016/j.geotexmem.2017.04.002,Elsevier B.V.,Article,"GABIONS; BUILDING reinforcement; TENSILE tests; GEOGRIDS; WIRE netting; Nonferrous Metal (except Copper and Aluminum) Rolling, Drawing, and Extruding; Non-ferrous metal (except copper and aluminum) rolling, drawing, extruding and alloying; Copper Rolling, Drawing, Extruding, and Alloying; Other fabricated wire product manufacturing; Iron and Steel Mills and Ferroalloy Manufacturing; Other Fabricated Wire Product Manufacturing; Other Aluminum Rolling, Drawing, and Extruding; Steel Wire Drawing; Other Heavy and Civil Engineering Construction",Gabion facing; Geosynthetics; Instrumented field walls; Numerical modelling; Primary reinforcement; Secondary reinforcement,"The paper describes numerical models that were developed to simulate the performance of two instrumented mechanically stabilized earth walls constructed in Izmir, Turkey. These walls were constructed with gabion facing, hybrid reinforcement layers, and fill on a rigid foundation. The hybrid reinforcement layers comprised primary reinforcement (geogrid) and secondary reinforcement (wire mesh). The vertical spacing between the primary reinforcement changed from 1 m to 2 m in two walls while other properties were kept the same. The responses of the field walls at the end of construction were simulated and compared with the numerical results. The results calculated from the numerical models showed generally good agreement with the measured wall facing displacements, horizontal fill displacements, and tensile forces in the geogrid and in the wire mesh. The maximum calculated facing displacements for the walls with 1 m and 2 m reinforcement spacing were 30.7 and 36.4 mm, respectively. The maximum tensile forces in the geogrid layers were increased by 1.5 times in the 2 m spacing wall as compared with the 1 m spacing wall due to the increase of primary reinforcement spacing. However, the spacing change did not have an obvious effect on the increase of tensile forces in the secondary reinforcement (the wire mesh). The calculated results were also compared with theoretical results relating to the earth pressure distributions and the location of the maximum tensile strains in the primary reinforcement. The horizontal earth pressures against the wall facing were close to the active earth pressures for both walls. The maximum tensile strain line of the reinforcement was close to the Rankine's failure line. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123574063&site=ehost-live
640,Responses of geosynthetic-reinforced soil (GRS) abutments under bridge slab loading: Numerical investigation.,Burak Tanyu,Computers & Geotechnics,0266352X,,Jul-20,123,,N.PAG,1,143365342,10.1016/j.compgeo.2020.103566,Elsevier B.V.,Article,"BRIDGE abutments; SLABS; EARTH pressure; FINITE differences; SOILS; JOB stress; Highway, Street, and Bridge Construction; Iron and Steel Mills and Ferroalloy Manufacturing",Bridge abutment; Geosynthetic; Geosynthetic-reinforced soil; Numerical; Surcharge,"This study evaluated the responses of geosynthetic-reinforced soil (GRS) abutments subjected to bridge slab loading under working stress conditions using two-dimensional finite difference numerical software. A parametric study was conducted to investigate the effects of different combinations of reinforcement spacing S v and reinforcement stiffness J , beam seat width b , and setback distance a b on the responses of the GRS abutments in terms of additional vertical stresses under the beam seat centerline Δσ v induced by the bridge slab load, additional lateral earth pressures behind the abutment facing Δσ h-facing and under the beam seat centerline Δσ h-cetner induced by the bridge slab load, and maximum tension in the reinforcement T max. Numerical analyses evaluated trapezoidal and uniform reinforcement layouts and showed that both reinforcement layouts generated similar responses of the GRS abutments. Under the same ratio of J / S v , different combinations of S v and J generated similar distributions of Δσ v , Δσ h-facing and Δσ h-center. The maximum of T max with depth decreased almost proportionally with the decrease of S v. Larger b and a b caused lower Δσ v , Δσ h-facing , Δσ h-center , and smaller T max in the upper reinforcement layers. The truncated 2 to 1 distribution method, which considers the effects of abutment facing on the Δσ v distribution, could reasonably predict the T max in the reinforcement. [ABSTRACT FROM AUTHOR] Copyright of Computers & Geotechnics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143365342&site=ehost-live
641,"Tufa precipitation from Recycled Concrete Aggregate (RCA) over geotextile: Mechanism, composition, and affecting parameters.",Burak Tanyu,Construction & Building Materials,9500618,,Jan-19,196,,317,13,133781245,10.1016/j.conbuildmat.2018.10.146,Elsevier B.V.,Article,TUFAS; CONCRETE; GEOTEXTILES; CARBONATE rocks; MINERAL aggregates; Ready-Mix Concrete Manufacturing; All Other Miscellaneous Nonmetallic Mineral Product Manufacturing; Dimension Stone Mining and Quarrying; Limestone mining and quarrying; Crushed and Broken Limestone Mining and Quarrying,Geochemical modeling; Induced RCA tufa precipitation; Precipitation mechanism; Tufa mineralogy,"Highlights • Plummer et al. set-up is suitable to study the induced RCA tufa precipitation. • Garrels-Mackenzie model predicts the composition and mechanism of RCA tufa formation. • Calcium sulfate compounds are dominant in evaporative RCA Tufa. • Calcium carbonate compounds are dominant in artificially CO 2 saturated environments. • Formation of evaporative tufa is more favorable for using RCA in construction. Abstract One of the limiting factors of using RCA as a construction material has previously been related to the generated tufa despite its acceptable mechanical properties. Studies conducted in the mid-90s seem to overestimate the potential of such formations; whereas, more recent studies tend to underestimate such depositions from RCA. However, a comprehensive study to understand the chemical mechanisms and resulting chemical composition of pure RCA tufa had not been carried out. This paper discusses the results of an analytical study that combines both the findings from experimental and theoretical geochemistry to demonstrate the conditions that result in tufa precipitation, the associated mineral compositions, and what these differences in mineral compositions may mean for the construction. The precipitation experiments were conducted using Plummer et al. reaction vessel. Based on the observations from these experiments, the findings showed that the chemical reactions and the formation of tufa depend on different precipitation mechanisms. The theoretical evaluations showed that Garrels-Mackenzie (progressive evaporation) model can be successfully used to interpret observations from the experiments. The results of this analytical study confirm that in most construction conditions calcium sulfate minerals are more likely to precipitate from RCA than calcium carbonate minerals. This finding would mean that the precipitated deposition from RCA may be seasonal due to the higher solubility constant of calcium sulfate. For practical implications in construction, a one-point leach test and geochemical analysis method is suggested to estimate the characteristics of potential tufa formations from different RCA sources. [ABSTRACT FROM AUTHOR] Copyright of Construction & Building Materials is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133781245&site=ehost-live
642,Two and three-dimensional numerical analyses of geosynthetic-reinforced soil (GRS) piers.,Burak Tanyu,Geotextiles & Geomembranes,2661144,,Jun-19,47,3,352,17,135686834,10.1016/j.geotexmem.2019.01.010,Elsevier B.V.,Article,BEARING capacity of soils; SOIL testing; NUMERICAL analysis; PIERS; EARTH pressure; ELASTIC modulus; Other Heavy and Civil Engineering Construction; Testing Laboratories; Site Preparation Contractors,Finite difference analysis; Geosynthetic-reinforced soil; Geosynthetics; Pier; Reinforcement spacing; Tension; Three-dimensional; Two-dimensional,"Abstract In this study, both two-dimensional (2D) and three-dimensional (3D) numerical analyses were carried out to evaluate the performance of geosynthetic-reinforced soil (GRS) piers. The numerical models were first calibrated and verified against test results available in the literature. A parametric study was then conducted under both 2D and 3D conditions to investigate the influences of reinforcement tensile stiffness, reinforcement vertical spacing, and a combination of reinforcement stiffness and spacing on the performance of GRS piers under vertical loading. Numerical results indicated that the effect of reinforcement spacing was more significant than that of reinforcement stiffness. The use of closely – spaced reinforcement layers resulted in higher global elastic modulus of the GRS pier, smaller lateral displacements of pier facing and volumetric change of the GRS pier, lower and more uniformly-distributed tension in the reinforcement, and larger normalized coefficients of lateral earth pressure. This study concluded that a 2D numerical model gave more conservative results than a 3D model. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135686834&site=ehost-live
643,Utilization of sepiolite materials as a bottom liner material in solid waste landfills.,Burak Tanyu,Waste Management,0956053X,,Jan-14,34,1,112,13,92716307,10.1016/j.wasman.2013.10.008,Elsevier B.V.,Article,MEERSCHAUM; SANITARY landfill linings; SOLID waste; ADSORPTION; CHEMICAL processes; All other non-metallic mineral mining and quarrying; All Other Nonmetallic Mineral Mining; Solid Waste Landfill,Adsorption; Bottom liner; Heavy metals; Landfills; Sepiolite; Solid wastes; Zeolite,"Highlights: [•] Experimental evaluation of the suitability of sepiolite as a landfill liner. [•] Sepiolite increases the mechanical performance (q u, swelling) of landfill liners. [•] Sepiolite increases the metal adsorption capacities of the landfill liners. [•] Sepiolite decreases the required liner thickness of the landfill liners. [•] Sepiolite provides viable cost alternative to regular clay used as landfill liner. [ABSTRACT FROM AUTHOR] Copyright of Waste Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92716307&site=ehost-live
644,Effect of ammonium on the hydraulic conductivity of geosynthetic clay liners.,Kuo Tian,Geotextiles & Geomembranes,2661144,,Dec-17,45,6,665,9,125419449,10.1016/j.geotexmem.2017.08.008,Elsevier B.V.,Article,"HYDRAULIC conductivity; GEOSYNTHETIC clay liners; SODIUM; BENTONITE; BIOREACTORS; Clay and Ceramic and Refractory Minerals Mining; Shale, clay and refractory mineral mining and quarrying",Ammonium solution; Bioreactor leachate; Exchange complex; Geosynthetic clay liner; Geosynthetics; Hydraulic conductivity; Recirculation landfill leachate; Swelling,"Hydraulic conductivity and swell index tests were conducted on a conventional geosynthetic clay liner (GCL) containing sodium-bentonite (Na-B) using 5, 50, 100, 500, and 1000 mM ammonium acetate (NH 4 OAc) solutions to investigate how NH 4 + accumulation in leachates in bioreactor and recirculation landfills may affect GCLs. Control tests were conducted with deionized (DI) water. Swell index of the Na-B was 27.7 mL/2 g in 5 mM NH 4 + solution and decreased to 5.0 mL/2 g in 1000 mM NH 4 + solution, whereas the swell index of Na-B in DI water was 28.0 mL/2 g. Hydraulic conductivity of the Na-B GCL to 5, 50, and 100 mM NH 4 + was low, ranging from 1.6–5.9 × 10 −11 m/s, which is comparable to the hydraulic conductivity to DI water (2.1 × 10 −11 m/s). Hydraulic conductivities of the Na-B GCL permeated with 500 and 1000 mM NH 4 + solutions were much higher (e.g., 1.6–5.2 × 10 −6 m/s) due to suppression of osmotic swelling. NH 4 + replaced native Na + , K + , Ca 2+ , and Mg 2+ in the exchange complex of the Na-B during permeation with all NH 4 + solutions, with the NH 4 + fraction in the exchange complex increasing from 0.24 to 0.83 as the NH 4 + concentration increased from 5 to 1000 mM. A Na-B GCL specimen permeated with 1000 mM NH 4 + solution to chemical equilibrium was subsequently permeated with DI water. Permeation with the NH 4 + converted the Na-B to “NH 4 -bentonite” with more than 80% of the exchange complex occupied by NH 4 + . Hydraulic conductivity of this GCL specimen decreased from 5.9 × 10 −6 m/s to 2.9 × 10 −11 m/s during permeation with DI water, indicating that “NH 4 -bentonite” can swell and have low hydraulic conductivity, and that the impact of more concentrated NH 4 + solutions on swelling and hydraulic conductivity is reversible. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125419449&site=ehost-live
645,Effect of incineration ash leachates on the hydraulic conductivity of bentonite-polymer composite geosynthetic clay liners.,Kuo Tian,Waste Management,0956053X,,Feb-22,139,,25,14,154857550,10.1016/j.wasman.2021.12.011,Elsevier B.V.,Article,GEOSYNTHETIC clay liners; HYDRAULIC conductivity; LINEAR polymers; INCINERATION; CROSSLINKED polymers; LEACHATE; POLYMER blends; Waste treatment and disposal; Solid Waste Combustors and Incinerators,"Crosslinked polymer; Incinerator ash; Landfill liner; Linear polymer; Municipal solid waste incineration; Polymer elution; Waste-to-energy, Polymer modified bentonite","• Bentonite-polymer composite geosynthetic clay liners (BPC GCLs) may contain water-soluble (linear) or water-insoluble (crosslinked) polymers. • Generally, BPC GCLs with high initial polymer loading had relatively lower hydraulic conductivity than those with low initial polymer loading. • For BPC GCLs containing linear polymer, polymer elution occurred when permeated with water or leachate. • There was no correlation between the percentage of polymer retained and final hydraulic conductivity of the BPC GCLs containing linear polymer. A study was conducted to evaluate the hydraulic conductivity (k) of six bentonite-polymer composite (BPC) geosynthetic clay liners (GCLs) using five synthetic municipal solid waste incineration ash (IA) leachates with ionic strength (I) ranging from 174 to1978 mM. The BPC GCLs contained a dry blend of bentonite and proprietary polymers and had polymer loading ranging from 0.5 to 5.5%. The polymers used in the BPC GCLs were classified as linear polymer (LP) or crosslinked polymer (CP) based on the swelling characteristics of specimens extracted from the GCLs. Comparable hydraulic conductivity tests were also performed on two conventional bentonite (CB) GCLs as controls. The BPC GCLs had k of 2.6 – 6.7 × 10-11 m/s when permeated with IA leachate with I = 174 mM, whereas the CB GCLs had k > 5.0 × 10-8 m/s when permeated with the same leachate. However, k of the BPC GCLs ranged from the order of 10-10 to 10-7 m/s when permeated with IA leachates with I > 600 mM. BPC GCLs with high polymer loading generally had lower k compared to those with lower polymer loading when permeated with the same IA leachate, regardless of the polymer type. Polymer eluted from the BPC GCLs containing LP during permeation with DI water or IA leachate. Unlike CPs, LPs are water-soluble, therefore, they seem to easily migrate during permeation. There was no correlation between the percentage of polymer retained and the final hydraulic conductivity of the LPB GCLs used in this study. [ABSTRACT FROM AUTHOR] Copyright of Waste Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154857550&site=ehost-live
646,Effect of specimen preparation on the swell index of bentonite-polymer GCLs.,Kuo Tian,Geotextiles & Geomembranes,2661144,,Dec-20,48,6,875,11,147117108,10.1016/j.geotexmem.2020.06.006,Elsevier B.V.,Article,INCINERATION; HYDRAULIC conductivity; GEOSYNTHETIC clay liners; SOLID waste; MUNICIPAL water supply; Waste treatment and disposal; Solid Waste Combustors and Incinerators,Crosslinked polymer; Geosynthetic clay liners; Hydraulic conductivity; Linear polymer; Loss on ignition; Swell index,"Experiments were conducted to investigate how specimen preparation (crushing and sieving) affects the swell index (SI) of bentonite-polymer (B–P) composites and the relationship between SI and hydraulic conductivity of B–P GCLs. Seven B–P and one Na–B GCLs were used in this study. Tests were conducted using DI water and synthetic municipal solid waste incineration ash leachates. Specimens were prepared using the ASTM D5890 and two alternative methods prior to SI testing. For both Na–B and B–P composites, <100% of the specimen passed through the #100 sieve regardless of the amount of crushing performed using a mortar and pestle. SIs and loss on ignitions (LOI) of the portion of the B–P composites passing #100 sieve were comparable to the Na–B, whereas the B–P specimen retained on #100 sieve had very high SIs and LOIs. These observations indicate that crushing and sieving of the B–P composites lead to segregation of polymer. A stronger correlation (R2 = 0.90) was observed between SI and hydraulic conductivity, only when SI tests were conducted with B–P without any crushing and sieving, suggesting that SI tests should conduct with B–P composites retrieved from the GCLs without sieving to provide a better prediction of hydraulic compatibility. • There are limitations in the ASTM D5890 specimen preparation procedure. • < 100% of the Na–B and B–P specimens passed through the #100 sieve after crushing. • The portion of the B–P specimens retained on the #100 sieve mainly comprised polymer. • SIs of the uncrushed B–P specimens were higher than that of the ASTM D5890 specimens. • SI of uncrushed B–P specimens correlates well with hydraulic conductivity of B–P GCLs. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147117108&site=ehost-live
647,Hydraulic conductivity of bentonite-polymer geosynthetic clay liners to coal combustion product leachates.,Kuo Tian,Geotextiles & Geomembranes,2661144,,Oct-21,49,5,1129,10,151632293,10.1016/j.geotexmem.2021.03.007,Elsevier B.V.,Article,HYDRAULIC conductivity; GEOSYNTHETIC clay liners; COMBUSTION products; LEACHATE; COAL products; COAL combustion; STRAINS & stresses (Mechanics); IONIC strength; Other petroleum and coal product manufacturing; All Other Petroleum and Coal Products Manufacturing,Bentonite-polymer (B–P); CCP leachates; Geosynthetic clay liners (GCLs),"Hydraulic conductivity of seven geosynthetic clay liners (GCLs) to synthetic coal combustion product (CCP) leachates were evaluated in this study. The leachates are chemically representative of typical and worst scenarios observed in CCP landfills. The ionic strength (I) of the synthetic CCP leachates ranged from 50 mM to 4676 mM (TCCP-50, LRMD-96, TFGDS-473, LR-2577, HI-3179 and HR-4676). One of the GCLs contained conventional sodium bentonite (Na–B) and the other six contained bentonite-polymer (B–P) mixture with polymer loadings ranging from 0.5% to 12.7%. Hydraulic conductivity tests were conducted at an effective confining stress of 20 kPa. The hydraulic conductivity of the Na–B GCLs were >1 × 10−10 m/s when permeated with all six CCP leachates, whereas the B–P GCLs with sufficient polymer loading maintained low hydraulic conductivity to synthetic CCP leachates. All the B–P GCLs showed low hydraulic conductivity (<1 × 10−10 m/s) to low ionic strength leachates (TCCP-50, I = 50 mM and LRMD-96, I = 96 mM). B–P GCLs with P > 5% showed low hydraulic conductivity (<1 × 10−10 m/s) up to HI-3179 leachates. These results suggest that B–P GCLs with sufficient polymer loading can be used to manage aggressive CCP leachates. • Hydraulic conductivity of Na-B and six B-P GCLs to six synthetic CCP leachates. • Six synthetic leachates were created in this study to mimic typical to worst field scenarios. • B-P GCL with > 5% polymer loading can maintain low hydraulic conductivity to CCP leachates. • B-P GCL with sufficient polymer loading can be used to manage aggressive CCP leachates. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151632293&site=ehost-live
648,Radiation dose and antioxidant depletion in a HDPE geomembrane.,Kuo Tian,Geotextiles & Geomembranes,2661144,,Aug-18,46,4,426,10,129589460,10.1016/j.geotexmem.2018.03.003,Elsevier B.V.,Article,ANTIOXIDANTS; HIGH density polyethylene; GEOMEMBRANES; RADIOACTIVE wastes; LEACHATE; Waste treatment and disposal; Hazardous Waste Treatment and Disposal,Antioxidant depletion; Dose deposition; HDPE geomembrane; Low-level radioactive waste; Radiation; Service life,"The impact of α and β radiation on antioxidant depletion in smooth high-density polyethylene (HDPE) geomembranes (GMs) is described. Smooth HDPE GMs having different thickness (0.04-mm, 0.1-mm, 0.2-mm) were created by mechanically pulverizing sections of 2-mm-thick smooth HDPE GM and extruding the polymer at different thicknesses using a film blowing machine. The 2-mm-thick smooth HDPE GM was also used in the experiments. HDPE GM specimens were exposed to sealed sources of 241 Am and 99 Tc for 1–50 h to simulate the impact of α and β radiation from U and 99 Tc in low-level radioactive waste (LLW) leachate. Standard oxidative induction time (OIT) tests were conducted to determine antioxidant depletion. No change in OIT occurred in the 2-mm-thick HDPE GM after exposure to sealed sources of 241 Am and 99 Tc for 50 h. In much thinner GMs (e.g., 0.04 mm), however, significant antioxidant depletion occurred after exposure most likely due to penetration of α and β particles. Penetration depth of α and β particles and dose deposition in HDPE GMs were estimated with the GEometry ANd Tracking (GEANT4) program. Predictions from GEANT4 show that maximum dose deposition occurs at the surface of the HDPE GM and decreases with depth. A multilayer model is used to estimate antioxidant depletion in HDPE GMs for depth-dependent doses. These estimates suggest that radiation from LLW leachate has an insignificant effect on antioxidant depletion in HDPE GMs due to the low dose deposition (e.g., 2.42 Gy) expected over a 1000-yr service life, even if the level of activity in LLW leachate increases 10x to 100x the level typical of today. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129589460&site=ehost-live
649,Blast response comparison of multiple steel frame connections,Girum Urgessa,Finite Elements in Analysis & Design,0168874X,,Jul-11,47,7,668,8,60157303,10.1016/j.finel.2011.01.009,Elsevier B.V.,Article,FINITE element method; BLAST effect; STRAINS & stresses (Mechanics); STRUCTURAL steel; STRUCTURAL frames; STRUCTURAL plates; COMPUTATIONAL fluid dynamics; Plate Work Manufacturing; Structural Steel and Precast Concrete Contractors; Fabricated Structural Metal Manufacturing; Other plate work and fabricated structural product manufacturing; Framing Contractors; Finish Carpentry Contractors,Blast loads; Connections; Displacement; Explosions; Finite element method; Stress,"Abstract: When a structural steel frame is subjected to blast, the beam-to-column connections, which are responsible for load transfer between different members within the frame, play a major role in structural response. This paper presents results of a comparative finite element analysis of a steel frame subjected to a blast loading from a vehicular threat. The study compared three connection systems referred as standard, TA and SidePlateTM. Connection plate thickness variations were also considered. Strain rate effects were included in the material constitutive model. The pressure–time histories were determined using FEFLO, a general purpose computational fluid dynamics program developed by the Center for Computational Fluid Dynamics at George Mason University. The numerical results were used to compare both the structural response and the constructability of frames with the studied connection types. Three structural response evaluation criteria and three such constructability criteria were used to determine comparative advantages and disadvantages of the TA and SidePlateTM connection types with respect to the standard connection. [Copyright &y& Elsevier] Copyright of Finite Elements in Analysis & Design is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60157303&site=ehost-live
650,Fire performance of functionally-graded-material sheathed load bearing thin-walled structural framing.,Girum Urgessa,Fire Safety Journal,3797112,,Oct-21,125,,N.PAG,1,152606340,10.1016/j.firesaf.2021.103425,Elsevier B.V.,Article,STRUCTURAL frames; FUNCTIONALLY gradient materials; COLD-formed steel; MECHANICAL properties of condensed matter; FIRE prevention; THERMAL properties; FIRE testing; Finish Carpentry Contractors; Framing Contractors,Cold-formed steel; Fire-resistance rating; Functionally graded materials; Temperature distribution; Thin-walled structure,"This paper presents the fire rating performance of load-bearing thin-walled system sheathed with Functionally Graded Material (FGM) board under standard ISO834 fire through numerical simulation. FGMs are one of the new classes of advanced composite materials that possess continuous variation of material properties within a given direction. The composition of the FGM sheathing is defined by the volume fractions of the constituents' materials (metal/ceramic) based on the power-law (P-FGM) material function. The general rule of mixtures is then used to predict the thermo-mechanical properties of the FGM sheathing. Fire rating analyses for the Cold-Formed Steel (CFS) wall system were conducted under steady-state conditions where the elastic buckling load from bifurcation analysis under fire conditions was first obtained. Then using the RIKS ON algorithm, collapse analysis was performed in a time frame until failure deformation occurred. The effect of non-uniform temperature on mechanical and thermal properties of the wall stud was included at each time frame in both elastic and collapse analysis. From the FE analysis using ABAQUS, it was observed that the use of FGM as a sheathing material for fire protection increases the failure time for all load ratios compared with the traditional gypsum board. The increase in failure time has a significant implication for improving the safety of occupants during fire scenarios. The study also shows that the novel composite material (FGM board), if properly designed, can lead to an alternative fire protection material in the thin-walled system. [ABSTRACT FROM AUTHOR] Copyright of Fire Safety Journal is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152606340&site=ehost-live
651,"Internal Relative Humidity, Autogenous Shrinkage, and Strength of Cement Mortar Modified with Superabsorbent Polymers.",Girum Urgessa,Polymers (20734360),20734360,,Oct-18,10,10,1074,1,132686423,10.3390/polym10101074,MDPI,Article,SUPERABSORBENT polymers; HUMIDITY; THERMAL expansion; CROSSLINKING (Polymerization); SURFACE cracks; DEFORMATIONS (Mechanics),autogenous shrinkage; coefficient of thermal expansion; internal curing; internal relative humidity; strength; superabsorbent polymers,"Laboratory evaluations were performed to investigate the effect of internal curing (IC) by superabsorbent polymers (SAP) on the internal relative humidity (IRH), autogenous shrinkage, coefficient of thermal expansion (CTE), and strength characteristics of low water-cement ratio (w/c) mortars. Four types of SAP with different cross-linking densities and particle sizes were used. Test results showed that the SAP inclusion effectively mitigated the IRH drops due to self-desiccation and corresponding autogenous shrinkage, and the IC effectiveness tended to increase with an increased SAP dosage. The greater the cross-linking density and particle size of SAP, the less the IRH drop and autogenous shrinkage. The trend of autogenous shrinkage developments was in good agreement with that of IRH changes, with nearly linear relationships between them. Both immediate deformation (ID)-based and full response-based CTEs were rarely affected by SAP inclusions. There were no substantial losses in compressive and flexural strengths of SAP-modified mortar compared to reference plain mortar. The findings revealed that SAPs can be effectively used to reduce the shrinkage cracking potential of low w/c cement-based materials at early ages, without compromising mechanical and thermal characteristics. [ABSTRACT FROM AUTHOR] Copyright of Polymers (20734360) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=132686423&site=ehost-live
652,Thermal responses of concrete slabs containing microencapsulated low-transition temperature phase change materials exposed to realistic climate conditions.,Girum Urgessa,Cement & Concrete Composites,9589465,,Nov-19,104,,N.PAG,1,140984573,10.1016/j.cemconcomp.2019.103391,Elsevier B.V.,Article,PHASE change materials; CONCRETE slabs; CONCRETE pavements; TRANSITION temperature; FREEZE-thaw cycles; TEMPERATURE; SERVICE life; Structural Steel and Precast Concrete Contractors; All Other Specialty Trade Contractors; All other non-metallic mineral product manufacturing,Concrete slab; Freeze-thaw deterioration; Mechanical properties; Microencapsulated phase change materials; Service life prediction; Thermal response,"This study examines the effect of microencapsulated low-transition temperature phase change material (PCM) additions on the thermal response of concrete slabs subjected to long-term realistic environmental exposure. To prevent direct contact of PCM with cement hydration products and possible leakage upon liquefaction, an inert PCM was encapsulated with a melamine-formaldehyde resin via an emulsification process before being added in concrete mixtures. Temperature monitoring was performed on three 500 × 500 × 150 mm large-scale concrete slabs with and without PCM for about 14 months encompassing two cold winter seasons. Results indicated that the addition of microencapsulated PCM effectively reduced excessive temperature drop and the number of freeze-thaw cycles concrete slabs experience during winter seasons, which may lead to service life extension by up to 5.2%–35.9% based on a freeze-thaw deterioration model. In particular, the effectiveness of PCM was found to be pronounced when the ambient temperature varied around the transition temperature (mild-cold seasons) while it became insignificant under prolonged exposure to extreme climate conditions such as cold winter and summer. The result of a visual condition survey was consistent with that of the model predictions, which verified the potential benefits of low-transition temperature PCM technology in concrete applications. This study also investigated the influence of microencapsulated PCM pellet embedment on the compressive and flexural strength characteristics. [ABSTRACT FROM AUTHOR] Copyright of Cement & Concrete Composites is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140984573&site=ehost-live
653,Vibration properties of beams using frequency-domain system identification methods.,Girum Urgessa,Journal of Vibration & Control,10775463,,8/1/11,17,9,1287,8,63249045,10.1177/1077546310378431,"Sage Publications, Ltd.",Article,STRUCTURAL dynamics; SYSTEM identification; CIVIL engineering; STRUCTURAL engineering; FREQUENCIES of oscillating systems; DAMPING (Mechanics); MODE shapes; VIBRATION measurements; Engineering Services,frequency; frequency analysis; Identification; system analysis; vibration,"The application of system identification methods to a number of civil engineering structures is increasing in order to improve the understanding of actual structural behaviors and augment traditional analytical assessments. The identification methods are typically used to determine structural dynamics properties (frequencies, damping ratios and mode shapes) and detection of local structural damages and deterioration. This paper presents two frequency domain system identification methods for determining structural vibration properties: eigensystem realization algorithm and the McKelvey frequency domain subspace algorithm. The methods were used in order to formulate a mathematical model that closely matches the frequency response function obtained from a vibration experiment conducted on an uncontrolled cantilever plate. Natural frequencies and damping ratios were determined using the system identification methods and the results are compared with a finite element analysis. A pulse response for a step input is simulated based on the state-space model system matrices obtained from the identification. [ABSTRACT FROM PUBLISHER] Copyright of Journal of Vibration & Control is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=63249045&site=ehost-live
654,Emerging clinical applications of text analytics.,Özlem Uzuner,International Journal of Medical Informatics,13865056,,Feb-20,134,,N.PAG,1,140937648,10.1016/j.ijmedinf.2019.103974,Elsevier B.V.,editorial,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140937648&site=ehost-live
655,A Microfluidic Platform to Monitor Real-Time Effects of Extracellular Vesicle Exchange between Co-Cultured Cells across Selectively Permeable Barriers.,Remi Veneziano,International Journal of Molecular Sciences,16616596,,Apr-22,23,7,3534,15,156291938,10.3390/ijms23073534,MDPI,Article,EXTRACELLULAR vesicles; CELL populations; EXTRACELLULAR matrix; EXOSOMES; CELL separation; MICROFLUIDIC devices,exosomes; extracellular matrix; extracellular vesicles; functional EV assays; intercellular communication; lab-on-a-chip; Matrigel; microfluidic device; PEGDA,"Exosomes and other extracellular vesicles (EVs) play a significant yet poorly understood role in cell–cell communication during homeostasis and various pathological conditions. Conventional in vitro and in vivo approaches for studying exosome/EV function depend on time-consuming and expensive vesicle purification methods to obtain sufficient vesicle populations. Moreover, the existence of various EV subtypes with distinct functional characteristics and submicron size makes their analysis challenging. To help address these challenges, we present here a unique chip-based approach for real-time monitoring of cellular EV exchange between physically separated cell populations. The extracellular matrix (ECM)-mimicking Matrigel is used to physically separate cell populations confined within microchannels, and mimics tissue environments to enable direct study of exosome/EV function. The submicron effective pore size of the Matrigel allows for the selective diffusion of only exosomes and other smaller EVs, in addition to soluble factors, between co-cultured cell populations. Furthermore, the use of PEGDA hydrogel with a very small pore size of 1.2 nm in lieu of Matrigel allows us to block EV migration and, therefore, differentiate EV effects from effects that may be mediated by soluble factors. This versatile platform bridges purely in vitro and in vivo assays by enabling studies of EV-mediated cellular crosstalk under physiologically relevant conditions, enabling future exosome/EV investigations across multiple disciplines through real-time monitoring of vesicle exchange. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Molecular Sciences is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156291938&site=ehost-live
656,DNA Scaffolds for Nanophotonics: Utilizing the Organizational Power of DNA Scaffolds for New Nanophotonic Applications (Advanced Optical Materials 18/2019).,Remi Veneziano,Advanced Optical Materials,21951071,,9/18/19,7,18,N.PAG,1,138689676,10.1002/adom.201970067,Wiley-Blackwell,Article,OPTICAL materials; DNA nanotechnology; NANOPHOTONICS; FLUORESCENCE resonance energy transfer; DNA; SURFACE enhanced Raman effect,chiral properties; DNA scaffolds; Förster resonance energy transfer; metamaterials; nanoscale optical devices; optically active molecules; plasmonic nanomaterials,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138689676&site=ehost-live
657,Rapid DNA origami nanostructure detection and classification using the YOLOv5 deep convolutional neural network.,Qi Wei,Scientific Reports,20452322,,3/9/22,12,1,1,13,155683431,10.1038/s41598-022-07759-3,Springer Nature,Article,CONVOLUTIONAL neural networks; DNA folding; DNA structure; DNA nanotechnology; ATOMIC force microscopes,,"The intra-image identification of DNA structures is essential to rapid prototyping and quality control of self-assembled DNA origami scaffold systems. We postulate that the YOLO modern object detection platform commonly used for facial recognition can be applied to rapidly scour atomic force microscope (AFM) images for identifying correctly formed DNA nanostructures with high fidelity. To make this approach widely available, we use open-source software and provide a straightforward procedure for designing a tailored, intelligent identification platform which can easily be repurposed to fit arbitrary structural geometries beyond AFM images of DNA structures. Here, we describe methods to acquire and generate the necessary components to create this robust system. Beginning with DNA structure design, we detail AFM imaging, data point annotation, data augmentation, model training, and inference. To demonstrate the adaptability of this system, we assembled two distinct DNA origami architectures (triangles and breadboards) for detection in raw AFM images. Using the images acquired of each structure, we trained two separate single class object identification models unique to each architecture. By applying these models in sequence, we correctly identified 3470 structures from a total population of 3617 using images that sometimes included a third DNA origami structure as well as other impurities. Analysis was completed in under 20 s with results yielding an F1 score of 0.96 using our approach. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155683431&site=ehost-live
657,Rapid DNA origami nanostructure detection and classification using the YOLOv5 deep convolutional neural network.,Remi Veneziano,Scientific Reports,20452322,,3/9/22,12,1,1,13,155683431,10.1038/s41598-022-07759-3,Springer Nature,Article,CONVOLUTIONAL neural networks; DNA folding; DNA structure; DNA nanotechnology; ATOMIC force microscopes,,"The intra-image identification of DNA structures is essential to rapid prototyping and quality control of self-assembled DNA origami scaffold systems. We postulate that the YOLO modern object detection platform commonly used for facial recognition can be applied to rapidly scour atomic force microscope (AFM) images for identifying correctly formed DNA nanostructures with high fidelity. To make this approach widely available, we use open-source software and provide a straightforward procedure for designing a tailored, intelligent identification platform which can easily be repurposed to fit arbitrary structural geometries beyond AFM images of DNA structures. Here, we describe methods to acquire and generate the necessary components to create this robust system. Beginning with DNA structure design, we detail AFM imaging, data point annotation, data augmentation, model training, and inference. To demonstrate the adaptability of this system, we assembled two distinct DNA origami architectures (triangles and breadboards) for detection in raw AFM images. Using the images acquired of each structure, we trained two separate single class object identification models unique to each architecture. By applying these models in sequence, we correctly identified 3470 structures from a total population of 3617 using images that sometimes included a third DNA origami structure as well as other impurities. Analysis was completed in under 20 s with results yielding an F1 score of 0.96 using our approach. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155683431&site=ehost-live
658,Synthesis of DNA Origami Scaffolds: Current and Emerging Strategies.,Remi Veneziano,Molecules,14203049,,Aug-20,25,15,3386,1,144990075,10.3390/molecules25153386,MDPI,Article,DNA folding; DNA synthesis; SINGLE-stranded DNA; CIRCULAR DNA; CANCER treatment; DNA nanotechnology,DNA amplification; DNA origami; DNA scaffolds; DNA Synthesis; nucleic acid nanoparticles; single-stranded DNA,"DNA origami nanocarriers have emerged as a promising tool for many biomedical applications, such as biosensing, targeted drug delivery, and cancer immunotherapy. These highly programmable nanoarchitectures are assembled into any shape or size with nanoscale precision by folding a single-stranded DNA scaffold with short complementary oligonucleotides. The standard scaffold strand used to fold DNA origami nanocarriers is usually the M13mp18 bacteriophage's circular single-stranded DNA genome with limited design flexibility in terms of the sequence and size of the final objects. However, with the recent progress in automated DNA origami design—allowing for increasing structural complexity—and the growing number of applications, the need for scalable methods to produce custom scaffolds has become crucial to overcome the limitations of traditional methods for scaffold production. Improved scaffold synthesis strategies will help to broaden the use of DNA origami for more biomedical applications. To this end, several techniques have been developed in recent years for the scalable synthesis of single stranded DNA scaffolds with custom lengths and sequences. This review focuses on these methods and the progress that has been made to address the challenges confronting custom scaffold production for large-scale DNA origami assembly. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144990075&site=ehost-live
659,Utilizing the Organizational Power of DNA Scaffolds for New Nanophotonic Applications.,Remi Veneziano,Advanced Optical Materials,21951071,,9/18/19,7,18,N.PAG,1,138689669,10.1002/adom.201900562,Wiley-Blackwell,Article,FLUORESCENCE resonance energy transfer; NANOWIRES; DNA structure; DNA; NANOELECTROMECHANICAL systems; SOFT errors,chiral properties; DNA scaffolds; Förster resonance energy transfer; metamaterials; nanoscale optical devices; optically active molecules; plasmonic nanomaterials,"Rapid development of DNA technology has provided a feasible route to creating nanoscale materials. DNA acts as a self‐assembled nanoscaffold capable of assuming any three‐dimensional shape. The ability to integrate dyes and new optical materials such as quantum dots and plasmonic nanoparticles precisely onto these architectures provides new ways to exploit their near‐ and far‐field interactions. A fundamental understanding of these optical processes will help drive development of next‐generation photonic nanomaterials. This review is focused on latest progress in DNA‐based photonic materials and highlights DNA scaffolds for rapidly assembling and prototyping nanoscale optical devices. Three areas are discussed including intrinsically active DNA structures displaying chiral properties, DNA scaffolds hosting plasmonic nanomaterials, and fluorophore‐labeled DNAs that engage in Förster resonance energy transfer and give rise to complex molecular photonic wires. An explanation of what is desired from these optical processes when harnessed sets the tone for what DNA scaffolds are providing toward each focus. Examples from the literature illustrate current progress along with a discussion of challenges to overcome for further improvements. Opportunities to integrate diverse classes of optically active molecules including light‐generating enzymes, fluorescent proteins, nanoclusters, and metal–chelates in new structural combinations on DNA scaffolds are also highlighted. [ABSTRACT FROM AUTHOR] Copyright of Advanced Optical Materials is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138689669&site=ehost-live
660,Deriving performance measures for transportation planning using ITS archived data.,Mohan Venigalla,Civil Engineering & Environmental Systems,10286608,,Sep-05,22,3,171,18,18926515,10.1080/10286600500279998,Taylor & Francis Ltd,Article,INTELLIGENT transportation systems; ELECTRONICS in transportation; TRANSPORTATION; TRAFFIC flow; TRAFFIC surveys; DATABASES; ELECTRONIC information resources; Other support activities for transportation; All Other Support Activities for Transportation,Air quality; Archived data management system; Archived data management systems; Intelligent transportation systems; Transportation planning,"Various modern sensor technologies deployed under the auspices of intelligent transportation systems (ITS) for data collection and archiving have helped in accumulating a wealth of transportation data in the form of data archives. Archiving of transportation data obtained from intelligent sources is practiced in most parts of the US under the auspices of the states' departments of transportation. However, recently there is a shift in the focus of archived data management systems (ADMS) from data collection and archiving to data analysis and distribution to stakeholders. This article discusses the use of archived ITS data for the development of performance measures for transportation planning and air quality support service. This service is packaged within a larger ADMS effort called Traffic Management Centers (TMC) Applications of Archived Data, which is also known by its working title ‘ADMS Virginia’. Nine sub-services for computing various performance measures at different spatial and temporal levels of aggregation are available within the transportation planning and air quality service. The nine performance measures provided are traffic speed, volume, density, vehicle miles traveled (VMT), percent VMT by time of day, travel time, volume-to-capacity ratio, peak hour factor, and average daily traffic (ADT). The service integrates a subset of a regional transportation planning network with the traffic flow data in the archived databases. The performance measures developed in this study have a broad spectrum of uses ranging from long- and short-range transportation planning, transportation system monitoring, regional air quality monitoring and air quality conformity, development of forecasting and simulation models, and establishment of growth impact policies. The concepts of operations of this system are discussed along with functional requirements, data model, and algorithms for deriving the performance measures. The methodology and procedures discussed in this article are portable and can easily be adopted by other ADMS efforts. [ABSTRACT FROM AUTHOR] Copyright of Civil Engineering & Environmental Systems is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18926515&site=ehost-live
661,"Health effects of ambient levels of respirable particulate matter (PM) on healthy, young-adult population.",Mohan Venigalla,Atmospheric Environment,13522310,,Dec2015 Part A,123,,102,10,111527729,10.1016/j.atmosenv.2015.10.039,Elsevier B.V.,Article,PARTICULATE matter; ENVIRONMENTAL health; CLIMATE change; EMISSIONS (Air pollution); ATMOSPHERIC ozone,Health effects; Particulate matter; PM10; PM2.5; Upper respiratory diseases; Young healthy adults,"There is an absence of studies that define the relationship between ambient particulate matter (PM) levels and adverse health outcomes among the young and healthy adult sub-group. In this research, the relationship between exposures to ambient levels of PM in the 10 micron (PM 10 ) and 2.5 micron (PM 2.5 ) size fractions and health outcomes in members of the healthy, young-adult subgroup who are 18–39 years of age was examined. Active duty military personnel populations at three strategically selected military bases in the United States were used as a surrogate to the control group. Health outcome data, which consists of the number of diagnoses for each of nine International Classification of Diseases, 9th Revision (ICD-9) categories related to respiratory illness, were derived from outpatient visits at each of the three military bases. Data on ambient concentrations of particulate matter, specifically PM 10 and PM 2.5 , were obtained for these sites. The health outcome data were correlated and regressed with the PM 10 and PM 2.5 data, and other air quality and weather-related data on a daily and weekly basis for the period 1998 to 2004. Results indicate that at Fort Bliss, which is a US Environmental Protection Agency designated non-attainment area for PM 10 , a statistically significant association exists between the weekly-averaged number of adverse health effects in the young and healthy adult population and the corresponding weekly-average ambient PM 10 concentration. A least squares regression analysis was performed on the Fort Bliss data sets indicated that the health outcome data is related to several environmental parameters in addition to PM 10 . Overall, the analysis estimates a .6% increase in the weekly rate of emergency room visits for upper respiratory infections for every 10 μg/m 3 increase in the weekly-averaged PM 10 concentration above the mean. The findings support the development of policy and guidance opportunities that can be developed to mitigate exposures to particulate matter. [ABSTRACT FROM AUTHOR] Copyright of Atmospheric Environment is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=111527729&site=ehost-live
662,Innovations in Geographic Information Systems Applications for Civil Engineering.,Mohan Venigalla,Journal of Computing in Civil Engineering,8873801,,Nov-06,20,6,375,2,22741548,10.1061/(ASCE)0887-3801(2006)20:6(375),American Society of Civil Engineers,Article,GEOGRAPHIC information systems; CIVIL engineering; ENGINEERING; TRANSPORTATION; ENVIRONMENTAL engineering; WATER supply; Engineering Services; All Other Support Activities for Transportation; Other support activities for transportation; Water Supply and Irrigation Systems,,"The article focuses on the application of geographic information systems (GIS) for civil engineering. The article discusses GIS-based solutions for engineering problems such as transportation, environmental and water resources. It also presents topics on engineering for submission to the journal such as infrastructure security.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22741548&site=ehost-live
663,A statistical approach for evaluating the effectiveness of heartworm preventive drugs: what does 100% efficacy really mean?,Anand Vidyashankar,Parasites & Vectors,17563305,,11/9/17,10,,97,12,126152770,10.1186/s13071-017-2440-x,BioMed Central,Article,DOG diseases; DIROFILARIA immitis; BIOTYPES of protozoa; MOXIDECTIN; UNITED States. Food & Drug Administration; All Other Animal Production; Live animal merchant wholesalers,Canine heartworm; Dirofilaria immitis; Efficacy; Macrocyclic lactone; Parametric bootstrap; Statistical,"Background: Initial studies of heartworm preventive drugs all yielded an observed efficacy of 100% with a single dose, and based on these data the US Food and Drug Administration (FDA) required all products to meet this standard for approval. Those initial studies, however, were based on just a few strains of parasites, and therefore were not representative of the full assortment of circulating biotypes. This issue has come to light in recent years, where it has become common for studies to yield less than 100% efficacy. This has changed the landscape for the testing of new products because heartworm efficacy studies lack the statistical power to conclude that finding zero worms is different from finding a few worms. Methods: To address this issue, we developed a novel statistical model, based on a hierarchical modeling and parametric bootstrap approach that provides new insights to assess multiple sources of variability encountered in heartworm drug efficacy studies. Using the newly established metrics we performed both data simulations and analyzed actual experimental data. Results: Our results suggest that an important source of modeling variability arises from variability in the parasite establishment rate between dogs; not accounting for this can overestimate the efficacy in more than 40% of cases. We provide strong evidence that ZoeMo-2012 and JYD-34, which both were established from the same source dog, have differing levels of susceptibility to moxidectin. In addition, we provide strong evidence that the differences in efficacy seen in two published studies using the MP3 strain were not due to randomness, and thus must be biological in nature. Conclusion: Our results demonstrate how statistical modeling can improve the interpretation of data from heartworm efficacy studies by providing a means to identify the true efficacy range based on the observed data. Importantly, these new insights should help to inform regulators on how to move forward in establishing new statistically and scientifically valid requirements for efficacy in the registration of new heartworm preventative products. Furthermore, our results provide strong evidence that heartworm 'strains' can change their susceptibility phenotype over short periods of time, providing further evidence that a wide diversity of susceptibility phenotypes exists among naturally circulating biotypes of D. immitis. [ABSTRACT FROM AUTHOR] Copyright of Parasites & Vectors is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126152770&site=ehost-live
664,An inconvenient truth: Global worming and anthelmintic resistance,Anand Vidyashankar,Veterinary Parasitology,3044017,,May-12,186,2-Jan,70,9,73968683,10.1016/j.vetpar.2011.11.048,Elsevier B.V.,Article,WORMS as carriers of disease; ANTHELMINTICS; LIVESTOCK parasites; IVERMECTIN; VETERINARY parasitologists; ALTERNATIVE treatment of cattle diseases; NEW Zealand; Pharmaceutical and medicine manufacturing,Anthelmintic resistance; Cattle; Goats; Horses; Nematodes; Sheep,"Abstract: Over the past 10–15 years, we have witnessed a rapid increase in both the prevalence and magnitude of anthelmintic resistance, and this increase appears to be a worldwide phenomenon. Reports of anthelmintic resistance to multiple drugs in individual parasite species, and in multiple parasite species across virtually all livestock hosts, are increasingly common. In addition, since the introduction of ivermectin in 1981, no novel anthelmintic classes were developed and introduced for use in livestock until recently with the launch of monepantel in New Zealand. Thus, livestock producers are often left with few options for effective treatment against many important parasite species. While new anthelmintic classes with novel mechanisms of action could potentially solve this problem, new drugs are extremely expensive to develop, and can be expected to be more expensive than older drugs. Thus, it seems clear that the “Global Worming” approach that has taken hold over the past 40–50 years must change, and livestock producers must develop a new vision for parasite control and sustainability of production. Furthermore, parasitologists must improve methods for study design and data analysis that are used for diagnosing anthelmintic resistance, especially for the fecal egg count reduction test (FECRT). Currently, standards for diagnosis of anthelmintic resistance using FECRT exist only for sheep. Lack of standards in horses and cattle and arbitrarily defined cutoffs for defining resistance, combined with inadequate analysis of the data, mean that errors in assigning resistance status are common. Similarly, the lack of standards makes it difficult to compare data among different studies. This problem needs to be addressed, because as new drugs are introduced now and in the future, the lack of alternative treatments will make early and accurate diagnosis of anthelmintic resistance increasingly important. [Copyright &y& Elsevier] Copyright of Veterinary Parasitology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=73968683&site=ehost-live
665,Ancestral inference for branching processes in random environments and an application to polymerase chain reaction.,Anand Vidyashankar,Stochastic Models,15326349,,2019,35,3,318,20,138027164,10.1080/15326349.2019.1588133,Taylor & Francis Ltd,Article,BRANCHING processes; STOCHASTIC processes; POLYMERASE chain reaction; INTEGRATED software; REVERSE transcriptase polymerase chain reaction,62G05 60F05 g0G42 62G20 62M05; Ancestral inference; BPRE; relative quantitation; replicated BPRE; variance absolute quantitation,"Branching processes in random environments arise in a variety of applications such as biology, finance, and other contemporary scientific areas. Motivated by these applications, this article investigates the problem of ancestral inference. Specifically, the article develops point and interval estimates for the mean number of ancestors initiating a branching process in i.i.d. random environments and establishes their asymptotic properties when the number of replications diverges to infinity. These results are then used to quantitate the number of DNA molecules in a genetic material using data from polymerase chain reaction experiments. Numerical experiments and data analyses are included to support the proposed methods. An R software package for implementing the methods of this manuscript is also included. [ABSTRACT FROM AUTHOR] Copyright of Stochastic Models is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138027164&site=ehost-live
666,Assessing Genome-Wide Statistical Significance for Large p Small n Problems.,Anand Vidyashankar,Genetics,166731,,Jul-13,194,3,781,8,89336371,10.1534/genetics.113.150896,Oxford University Press / USA,Article,GENOMES; GENETICS; RESAMPLING (Statistics); NONPARAMETRIC statistics; ERROR rates,,"Assessing genome-wide statistical significance is an important issue in genetic studies. We describe a new resampling approach for determining the appropriate thresholds for statistical significance. Our simulation results demonstrate that the proposed approach accurately controls the genome-wide type I error rate even under the large p small n situations. [ABSTRACT FROM AUTHOR] Copyright of Genetics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89336371&site=ehost-live
667,Comparison of fecal egg counting methods in four livestock species.,Anand Vidyashankar,Veterinary Parasitology,3044017,,Jun-18,257,,21,7,130123750,10.1016/j.vetpar.2018.05.015,Elsevier B.V.,Article,"MCMASTER approach; DIAGNOSTIC tests (Education); CHEESECLOTH; PARASITOLOGY; RUMINANTS; Piece goods, notions and other dry goods merchant wholesalers",Diagnostic parasitology; McMaster; Mini-FLOTAC; Modified-Wisconsin; Quantitative diagnostic test,"Gastrointestinal nematode parasites are important pathogens of all domesticated livestock species. Fecal egg counts (FEC) are routinely used for evaluating anthelmintic efficacy and for making targeted anthelmintic treatment decisions. Numerous FEC techniques exist and vary in precision and accuracy. These performance characteristics are especially important when performing fecal egg count reduction tests (FECRT). The objective of this study was to compare the accuracy and precision of three commonly used FEC methods and determine if differences existed among livestock species. In this study, we evaluated the modified-Wisconsin, 3-chamber (high-sensitivity) McMaster, and Mini-FLOTAC methods in cattle, sheep, horses, and llamas in three phases. In the first phase, we performed an egg-spiking study to assess the egg recovery rate and accuracy of the different FEC methods. In the second phase, we examined clinical samples from four different livestock species and completed multiple replicate FEC using each method. In the last phase, we assessed the cheesecloth straining step as a potential source of egg loss. In the egg-spiking study, the Mini-FLOTAC recovered 70.9% of the eggs, which was significantly higher than either the McMaster (P = 0.002) or Wisconsin (P = 0.002). In the clinical samples from ruminants, Mini-FLOTAC consistently yielded the highest EPG, revealing a significantly higher level of egg recovery (P < 0.0001). For horses and llamas, both McMaster and Mini-FLOTAC yielded significantly higher EPG than Wisconsin (P < 0.0001, P < 0.0001, P < 0.001, and P = 0.024). Mini-FLOTAC was the most accurate method and was the most precise test for both species of ruminants. The Wisconsin method was the most precise for horses and McMaster was more precise for llama samples. We compared the Wisconsin and Mini-FLOTAC methods using a modified technique where both methods were performed using either the Mini-FLOTAC sieve or cheesecloth. The differences in the estimated mean EPG on log scale between the Wisconsin and mini-FLOTAC methods when cheesecloth was used (P < 0.0001) and when cheesecloth was excluded (P < 0.0001) were significant, providing strong evidence that the straining step is an important source of error. The high accuracy and precision demonstrated in this study for the Mini-FLOTAC, suggest that this method can be recommended for routine use in all host species. The benefits of Mini-FLOTAC will be especially relevant when high accuracy is important, such as when performing FECRT. [ABSTRACT FROM AUTHOR] Copyright of Veterinary Parasitology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=130123750&site=ehost-live
668,Hypothesis testing in finite mixture of regressions: Sparsity and model selection uncertainty.,Anand Vidyashankar,Canadian Journal of Statistics,3195724,,Sep-18,46,3,429,29,133167294,10.1002/cjs.11467,Wiley-Blackwell,Article,HYPOTHESIS; FINITE mixture models (Statistics); REGRESSION analysis; SIMULATION methods & models; DATA analysis,Adjusted p‐value; BIC‐enhanced tuning parameter; data splitting; family‐wise error rate; model selection consistency,"Sparse finite mixture of regression models arise in several scientific applications and testing hypotheses concerning regression coefficients in such models is fundamental to data analysis. In this article, we describe an approach for hypothesis testing of regression coefficients that take into account model selection uncertainty. The proposed methods involve (i) estimating the active predictor set of the sparse model using a consistent model selector and (ii) testing hypotheses concerning the regression coefficients associated with the estimated active predictor set. The methods asymptotically control the family wise error rate at a pre‐specified nominal level, while accounting for variable selection uncertainty. Additionally, we provide examples of consistent model selectors and describe methods for finite sample improvements. Performance of the methods is also illustrated using simulations. A real data analysis is included to illustrate the applicability of the methods. The Canadian Journal of Statistics 46: 429–457; 2018 © 2018 Statistical Society of Canada (English) [ABSTRACT FROM AUTHOR] Résumé: Les mélanges épars de modèles de régression surviennent dans plusieurs applications scientifiques, et il est fondamental pour l'analyse des données de tester des hypothèses à propos des coefficients de ces modèles. Les auteurs décrivent une approche pour tester les coefficients de régression en tenant compte de l'incertitude liée à la sélection de modèle. Les méthodes proposées comportent (i) l'estimation de l'ensemble des prédicteurs actifs d'un modèle épars à l'aide d'un sélecteur de modèle convergent, et (ii) le test d'hypothèses à propos des coefficients de régression associés à l'ensemble de prédicteurs actifs. Les méthodes contrôlent asymptotiquement le taux d'erreur par famille à un niveau prédéfini tout en tenant compte de l'incertitude issue de la sélection de variables. Les auteurs fournissent des exemples de sélecteurs de modèle convergent et décrivent des méthodes apportant des améliorations avec des échantillons finis. Ils illustrent la performance de leurs méthodes à l'aide de simulations et procèdent à l'analyse de données réelles pour montrer son applicabilité. La revue canadienne de statistique 46: 429–457; 2018 © 2018 Sociétéstatistique du Canada (French) [ABSTRACT FROM AUTHOR] Copyright of Canadian Journal of Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133167294&site=ehost-live
669,Impact of fenbendazole resistance in Ascaridia dissimilis on the economics of production in turkeys.,Anand Vidyashankar,Poultry Science,325791,,Nov-21,100,11,N.PAG,1,153293711,10.1016/j.psj.2021.101435,Elsevier B.V.,Article,TURKEYS; FEED utilization efficiency; PRODUCTION (Economic theory); TREATMENT failure; WEIGHT gain; INFECTION control; Turkey Production; Poultry Processing; Poultry Hatcheries,anthelmintic resistance; Ascaridia; benzimidazoles; feed conversion; turkey,"Feed conversion efficiency is among the most important factors affecting profitable production of poultry.Infections with parasitic nematodes can decrease efficiency of production, making parasite control through the use of anthelmintics an important component of health management. In ruminants and horses, anthelmintic resistance is highly prevalent in many of the most important nematode species, which greatly impacts their control. Recently, we identified resistance to fenbendazole in an isolate of Ascaridia dissimilis , the most common intestinal helminth of turkeys. Using this drug-resistant isolate, we investigated the impact that failure to control infections has on weight gain and feed conversion in growing turkeys. Birds were infected on D 0 with either a fenbendazole-susceptible or -resistant isolate, and then half were treated with fenbendazole (SafeGuard Aquasol) at 4- and 8-wk postinfection. Feed intake and bird weight were measured for each pen weekly throughout the study, and feed conversion rate was calculated. Necropsy was performed on birds from each treatment group to assess worm burdens at wk 7 and 9 postinfection. In the birds infected with the susceptible isolate, fenbendazole-treated groups had significantly better feed conversion as compared to untreated groups. In contrast, there were no significant differences in feed conversion between the fenbendazole-treated and untreated groups in the birds infected with the resistant isolate. At both wk 7 and 9, worm burdens were significantly different between the treated and untreated birds infected with the drug-susceptible isolate, but not in the birds infected with the drug-resistant isolate. These significant effects on feed conversion were seen despite having a rather low worm establishment in the birds. Overall, these data indicate that A. dissimilis can produce significant reductions in feed conversion, and that failure of treatment due to the presence of fenbendazole-resistant worms can have a significant economic impact on turkey production. Furthermore, given the low worm burdens and an abbreviated grow out period of this study, the levels of production loss we measured may be an underestimate of the true impact that fenbendazole-resistant worms may have on a commercial operation. [ABSTRACT FROM AUTHOR] Copyright of Poultry Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153293711&site=ehost-live
670,Inference for Quantitation Parameters in Polymerase Chain Reactions via Branching Processes With Random Effects.,Anand Vidyashankar,Journal of the American Statistical Association,1621459,,Jun-11,106,494,525,9,62255364,10.1198/jasa.2011.tm08601,Taylor & Francis Ltd,Article,POLYMERASE chain reaction; BRANCHING processes; ASYMPTOTIC theory in estimation theory; NUCLEOTIDE sequence; RESEARCH methodology,Between-reaction variability; Generalized method of moments; Martingale limits; Within-reaction variability,"The quantitative polymerase chain reaction (qPCR) is a widely used tool for gene quantitation and has been applied extensively in several scientific areas. The current methods used for analyzing qPCR data fail to account for multiple sources of variability present in the PCR dynamics, leading to biased estimates and incorrect inference. In this article, we introduce a branching process model with random effects to account for within-reaction and between-reaction variability in PCR experiments. We describe, in terms of the observed fluorescence data, new statistical methodology for gene quantitation. Using simulations, PCR experiments, and asymptotic theory we demonstrate the improvements achieved by our methodology compared to existing methods. This article has supplemental materials online. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Statistical Association is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=62255364&site=ehost-live
671,Rare Event Analysis for Minimum Hellinger Distance Estimators via Large Deviation Theory.,Anand Vidyashankar,Entropy,10994300,,Apr-21,23,4,386,1,150811443,10.3390/e23040386,MDPI,Article,LARGE deviations (Mathematics); LARGE deviation theory; MAXIMUM likelihood statistics; GENERATING functions; ASYMPTOTIC distribution; MAXIMA & minima; DISTANCES,divergence measures; Hellinger distance; large deviations; rare event probabilities,"Hellinger distance has been widely used to derive objective functions that are alternatives to maximum likelihood methods. While the asymptotic distributions of these estimators have been well investigated, the probabilities of rare events induced by them are largely unknown. In this article, we analyze these rare event probabilities using large deviation theory under a potential model misspecification, in both one and higher dimensions. We show that these probabilities decay exponentially, characterizing their decay via a ""rate function"" which is expressed as a convex conjugate of a limiting cumulant generating function. In the analysis of the lower bound, in particular, certain geometric considerations arise that facilitate an explicit representation, also in the case when the limiting generating function is nondifferentiable. Our analysis involves the modulus of continuity properties of the affinity, which may be of independent interest. [ABSTRACT FROM AUTHOR] Copyright of Entropy is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150811443&site=ehost-live
672,"Resistance to fenbendazole in Ascaridia dissimilis, an important nematode parasite of turkeys.",Anand Vidyashankar,Poultry Science,325791,,Nov-19,98,11,5412,4,138940664,10.3382/ps/pez379,Elsevier B.V.,Article,TURKEYS; NECROTIC enteritis; GASTROINTESTINAL contents; INTESTINAL parasites; HAEMONCHUS contortus; PARASITES; POULTRY; Meat Markets; Poultry and egg merchant wholesalers; Live animal merchant wholesalers; Other Poultry Production; Poultry and Poultry Product Merchant Wholesalers; Poultry Hatcheries; Turkey Production; Poultry Processing,Ascaridia; benzimidazoles; resistance; turkey,"An important factor in efficient production of poultry is management of parasites. Ascaridia dissimilis is the most prevalent small intestinal nematode parasite of turkeys with up to 100% of flocks infected. High worm burdens can cause necrotic enteritis leading to high mortality in flocks. Recently, we were made aware of multiple cases where high burdens were seen at slaughter despite the administration of anthelmintics at frequent intervals, suggesting that resistance may have evolved in A. dissimilis. To address this issue, we obtained eggs of A. dissimilis from 4 commercial turkey farms and performed controlled efficacy tests to determine if fenbendazole resistance was present. Three farms had histories of frequent use of fenbendazole and worms found at slaughter, suggesting they may have resistance, and one was an organic farm where we had no additional history other than the farm had transitioned to organic production a few years earlier. For each worm isolate there were 2 treated and 2 untreated groups containing 9 birds each, with all groups being replicated in 2 separate rooms. Birds were infected with approximately 200 infective eggs, and treated groups received fenbendazole in the water (SafeGuard Aquasol, 1 mg/kg) for 5 consecutive days starting on day 24 post-infection. One week after the last treatment birds were necropsied, intestinal contents were collected and worms enumerated. Three of the four isolates demonstrated greater than 99% efficacy, indicating they were fully susceptible to fenbendazole. However, the fourth isolate demonstrated a significantly reduced efficacy of 63.89%, indicating the presence of resistance. Interestingly, this was the organic farm, whereas the 3 farms with ""suspected"" resistance all turned out to be fully susceptible. Given that 1 randomly acquired isolate of A. dissimilis , out of 4 tested, demonstrated resistance in this study, fenbendazole resistance may be a much larger problem on turkey farms than is currently recognized. Additional studies are needed to determine the prevalence of resistance, as well as the economic impact that resistant A. dissimilis have on turkey production. [ABSTRACT FROM AUTHOR] Copyright of Poultry Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138940664&site=ehost-live
673,Robust Inference after Random Projections via Hellinger Distance for Location-Scale Family.,Anand Vidyashankar,Entropy,10994300,,Apr-19,21,4,348,1,136174402,10.3390/e21040348,MDPI,Article,RANDOM projection method; ROBUST control; MAXIMUM likelihood statistics; RANDOM variables; HEALTH care industry; All Other Health and Personal Care Stores,asymptotic normality; compressed data; consistency; Hellinger distance; influence function; iterated limits; location-scale family; representation formula,"Big data and streaming data are encountered in a variety of contemporary applications in business and industry. In such cases, it is common to use random projections to reduce the dimension of the data yielding compressed data. These data however possess various anomalies such as heterogeneity, outliers, and round-off errors which are hard to detect due to volume and processing challenges. This paper describes a new robust and efficient methodology, using Hellinger distance, to analyze the compressed data. Using large sample methods and numerical experiments, it is demonstrated that a routine use of robust estimation procedure is feasible. The role of double limits in understanding the efficiency and robustness is brought out, which is of independent interest. [ABSTRACT FROM AUTHOR] Copyright of Entropy is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136174402&site=ehost-live
674,Serum Strongylus vulgaris-specific antibody responses to anthelmintic treatment in naturally infected horses.,Anand Vidyashankar,Parasitology Research,9320113,,Feb-15,114,2,445,7,100576391,10.1007/s00436-014-4201-5,Springer Nature,Article,HELMINTHS; PARASITES; ENDARTERITIS; THROMBOEMBOLISM; INFARCTION; ENZYME-linked immunosorbent assay,ELISA; Ivermectin; Pyrantel tartrate; Strongylus vulgaris; SvSXP,"Strongylus vulgaris is the most pathogenic helminth parasite of horses, causing verminous endarteritis with thromboembolism and infarction. A serum enzyme-linked immunosorbent assay (ELISA) has been validated for detection of antibodies to an antigen produced by migrating larvae of this parasite. The aim was to evaluate ELISA responses to anthelmintic treatment in cohorts of naturally infected horses. Fifteen healthy horses harboring patent S. vulgaris infections were turned out for communal grazing in May 2013 (day 0). On day 55, horses were ranked according to ELISA titers and randomly allocated to the following three groups: no treatment followed by placebo pellets daily; ivermectin on day 60 followed by placebo pellets daily; or ivermectin on day 60 followed by daily pyrantel tartrate. Fecal and serum samples were collected at ∼28-day intervals until study termination on day 231. Increased ELISA values were observed for the first 53 days following ivermectin treatment. Titers were significantly reduced 80 days after ivermectin treatment. Horses receiving daily pyrantel tartrate maintained lower ELISA values from 137 days post ivermectin treatment until trial termination. These results illustrate that a positive ELISA result is indicative of either current or prior exposure to larval S. vulgaris infection within the previous 5 months. [ABSTRACT FROM AUTHOR] Copyright of Parasitology Research is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100576391&site=ehost-live
675,Tail estimates for stochastic fixed point equations via nonlinear renewal theory.,Anand Vidyashankar,Stochastic Processes & Their Applications,3044149,,Sep-13,123,9,3378,52,89137576,10.1016/j.spa.2013.04.015,Elsevier B.V.,Article,STOCHASTIC processes; FIXED point theory; NONLINEAR analysis; ESTIMATION theory; MATHEMATICAL sequences; MATHEMATICAL bounds,Cramér–Lundberg theory with stochastic investments; Extremal index; GARCH processes; Geometric ergodicity; Harris recurrent Markov chains; Large deviations; Letac’s principle; Nonlinear renewal theory; Random recurrence equations; Slowly changing functions,"Abstract: This paper introduces a new approach, based on large deviation theory and nonlinear renewal theory, for analyzing solutions to stochastic fixed point equations of the form , where for a random triplet . Our main result establishes the tail estimate as , providing a new, explicit probabilistic characterization for the constant . Our methods rely on a dual change of measure, which we use to analyze the path properties of the forward iterates of the stochastic fixed point equation. To analyze these forward iterates, we establish several new results in the realm of nonlinear renewal theory for these processes. As a consequence of our techniques, we develop a new characterization of the extremal index, as well as a Lundberg-type upper bound for . Finally, we provide an extension of our main result to random Lipschitz maps of the form , where and . [Copyright &y& Elsevier] Copyright of Stochastic Processes & Their Applications is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89137576&site=ehost-live
676,Validation of the FAMACHA© system in South American camelids.,Anand Vidyashankar,Veterinary Parasitology,3044017,,Aug-17,243,,85,7,124608542,10.1016/j.vetpar.2017.06.004,Elsevier B.V.,Article,HAEMONCHUS contortus; CAMELIDAE; ANTHELMINTICS; ANIMAL diseases; RUMINANTS; DRUG resistance in microorganisms; DISEASE prevalence; Pharmaceutical and medicine manufacturing,Alpaca; Anthelmintic resistance; Camelids; FAMACHA©; Haemonchosis; Haemonchus contortus; Llama; Selective treatment,"Haemonchus contortus resistant to multiple anthelmintics threaten the viability of the small ruminant industry in areas where this parasite is prevalent. In response to this situation, the FAMACHA© system was developed and validated for use with small ruminants as a way to detect clinical anemia associated with haemonchosis. Given that H. contortus and multiple anthelmintic resistance is a similar problem in camelids, the FAMACHA© system might also provide the same benefits. To address this need, a validation study of the FAMACHA© system was conducted on 21 alpaca and llama farms over a 2-year period. H. contortus was the predominant nematode parasite on 17 of the 21 farms (10 alpaca and 7 llama farms) enrolled in the study, based on fecal culture results. The FAMACHA© card was used to score the color of the lower palpebral (lower eye lid) conjunctiva on a 1–5 scale. Packed cell volume (PCV) values were measured and compared to FAMACHA© scores using FAMACHA© score cutoffs of ≥3 or ≥4 and with anemia defined as a PCV ≤15%, ≤17%, or ≤ 20%. PCV was significantly associated with FAMACHA© score, fecal egg count (FEC), and body condition score (BCS), regardless of the FAMACHA© cutoff score or the PCV% chosen to define clinical anemia (p < 0.01 in all cases). The use of FAMACHA© scores ≥3 and PCV ≥ 15% indicating anemia provided the best sensitivity (96.4% vs 92.9% for FAMACHA© ≥4), whereas FAMACHA scores ≥ 4 and PCV ≤20% provided the best specificity (94.2% vs 69.1% for FAMACHA© ≥3). The data from this study support the FAMACHA© system as a useful tool for detecting clinical anemia in camelids suffering from haemonchosis. Parameters for making treatment decisions based on FAMACHA© score in camelids should mirror those established for small ruminants. [ABSTRACT FROM AUTHOR] Copyright of Veterinary Parasitology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124608542&site=ehost-live
677,Efficient Doppler-Compensated Reiterative Minimum Mean-Squared-Error Processing.,Kathleen Wage,IEEE Transactions on Aerospace & Electronic Systems,189251,,Apr-17,53,2,562,13,122903914,10.1109/TAES.2017.2651480,IEEE,Article,DOPPLER effect; COVARIANCE matrices; PULSE compression (Signal processing); MEAN square algorithms; PULSE frequency modulation,Adaptation models; Adaptive pulse compression (APC); Covariance matrices; covariance matrix tapers (CMT); Distortion; Doppler compensation; Doppler effect; Indexes; reiterative minimum mean squared error (RMMSE); Robustness; Signal processing algorithms,"Doppler distortion degrades the performance of the reiterative minimum mean-squared-error (RMMSE) adaptive pulse compression (APC) algorithm. This paper presents a new approach for robust RMMSE Doppler compensation through the use of covariance matrix tapers (RMMSE-CMT). RMMSE-CMT delivers performance comparable to the existing approaches such as Doppler-compensated APC (DC-APC). RMMSE-CMT is simple to implement and offers considerable computational savings over DC-APC. RMMSE-CMT also provides improved Doppler robustness in the fast-APC algorithm. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Aerospace & Electronic Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122903914&site=ehost-live
678,Modal analysis of broadband acoustic receptions at 3515-km range in the North Pacific using short-time Fourier techniques.,Kathleen Wage,Journal of the Acoustical Society of America,14966,,Feb-03,113,2,801,17,19825880,10.1121/1.1530615,American Institute of Physics,Article,MODAL analysis; SOUND; FOURIER analysis; SIGNAL theory; WAVEGUIDES,,"In 1995–1996 the Acoustic Thermometry of Ocean Climate (ATOC) experiment provided an opportunity to study long-range broadband transmissions over a series of months using mode-resolving vertical arrays. A 75-Hz source off the California coast transmitted broadband pulses to receiving arrays in the North Pacific, located at ranges of 3515 and 5171 km. This paper develops a short-time Fourier transform (STFT) processor for estimating the signals propagating in the lowest modes of the ocean waveguide and applies it to analyze data from the ATOC experiment. The STFT provides a convenient framework for examining processing issues associated with broadband signals. In particular, this paper discusses the required frequency resolution for mode estimation, analyzes the broadband performance of two standard modal beamforming algorithms, and explores the time/frequency tradeoffs inherent in broadband mode processing. Short-time Fourier analysis of the ATOC receptions at 3515 km reveals a complicated arrival structure in modes 1–10. This structure is characterized by frequency-selective fading and a high degree of temporal variability. At this range the first ten modes have equal average powers, and the magnitude-squared coherence between the modes is effectively zero. The coherence times of the peaks in the STFT mode estimates are on the order of 5.5 min. An analysis of mean arrival times yields modal dispersion curves and indicates that there are statistically significant shifts in travel time over 5 months of ATOC transmissions. © 2003 Acoustical Society of America. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Acoustical Society of America is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=19825880&site=ehost-live
679,Multiplicative and min processing of experimental passive sonar data from thinned arrays.,Kathleen Wage,Journal of the Acoustical Society of America,14966,,Dec-18,144,6,3262,13,133810084,10.1121/1.5064458,American Institute of Physics,Article,ACOUSTIC arrays; DETECTORS; SONAR; BEAMFORMING; SIGNAL processing; WAVEGUIDES; INTERFERENCE (Sound),,"Sparse arrays reduce the number of sensors required to achieve a specific angular resolution by using sensor spacing greater than the half-wavelength. These undersampled sparse arrays require processing algorithms to eliminate aliasing ambiguities. Thinned arrays are sparse arrays whose sensor positions lie on an underlying equally spaced grid. Using data from a shallow water passive sonar experiment, this paper investigates two thinned array geometries (coprime and nested) along with two processing algorithms (multiplicative and min). Coprime and nested arrays consist of two interleaved Uniform Line Arrays (ULAs) where one or both of the ULAs are undersampled. Multiplicative and min processors combine the outputs of the conventionally-beamformed subarrays to estimate the spatial spectrum. While these nonlinear processors can suppress aliasing, they are often plagued by high sidelobes and cross term interference. This paper presents sparse array designs for a shallow waveguide that require 33% fewer sensors than a fully-sampled ULA and provide significant sidelobe attenuation. Experimental data analysis reveals that cross term interference dominates the spectral estimates for the coprime and nested multiplicative processors and the coprime min processor. The nested min processor outperforms its sparse counterparts due to its ability to contend with coherent multipath in the environment. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Acoustical Society of America is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133810084&site=ehost-live
680,Reduced rank models for travel time estimation of low order mode pulses.,Kathleen Wage,Journal of the Acoustical Society of America,14966,,Oct-13,134,4,3332,15,90559337,10.1121/1.4818847,American Institute of Physics,Article,"SOUND waves; SPEED of sound; ACOUSTIC emission; OCEANOGRAPHIC research; ORTHOGONAL functions; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences",,"Mode travel time estimation in the presence of internal waves (IWs) is a challenging problem. IWs perturb the sound speed, which results in travel time wander and mode scattering. A standard approach to travel time estimation is to pulse compress the broadband signal, pick the peak of the compressed time series, and average the peak time over multiple receptions to reduce variance. The peak-picking approach implicitly assumes there is a single strong arrival and does not perform well when there are multiple arrivals due to scattering. This article presents a statistical model for the scattered mode arrivals and uses the model to design improved travel time estimators. The model is based on an Empirical Orthogonal Function (EOF) analysis of the mode time series. Range-dependent simulations and data from the Long-range Ocean Acoustic Propagation Experiment (LOAPEX) indicate that the modes are represented by a small number of EOFs. The reduced-rank EOF model is used to construct a travel time estimator based on the Matched Subspace Detector (MSD). Analysis of simulation and experimental data show that the MSDs are more robust to IW scattering than peak picking. The simulation analysis also highlights how IWs affect the mode excitation by the source. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Acoustical Society of America is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=90559337&site=ehost-live
681,The North Pacific Acoustic Laboratory deep-water acoustic propagation experiments in the Philippine Sea.,Kathleen Wage,Journal of the Acoustical Society of America,14966,,Oct-13,134,4,3359,17,90559317,10.1121/1.4818887,American Institute of Physics,Article,"ACOUSTIC emission; NOISE measurement; OCEANOGRAPHIC research; EDDIES; PHILIPPINE Sea; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences",,"A series of experiments conducted in the Philippine Sea during 2009-2011 investigated deep-water acoustic propagation and ambient noise in this oceanographically and geologically complex region: (i) the 2009 North Pacific Acoustic Laboratory (NPAL) Pilot Study/Engineering Test, (ii) the 2010-2011 NPAL Philippine Sea Experiment, and (iii) the Ocean Bottom Seismometer Augmentation of the 2010-2011 NPAL Philippine Sea Experiment. The experimental goals included (a) understanding the impacts of fronts, eddies, and internal tides on acoustic propagation, (b) determining whether acoustic methods, together with other measurements and ocean modeling, can yield estimates of the time-evolving ocean state useful for making improved acoustic predictions, (c) improving our understanding of the physics of scattering by internal waves and spice, (d) characterizing the depth dependence and temporal variability of ambient noise, and (e) understanding the relationship between the acoustic field in the water column and the seismic field in the seafloor. In these experiments, moored and ship-suspended low-frequency acoustic sources transmitted to a newly developed distributed vertical line array receiver capable of spanning the water column in the deep ocean. The acoustic transmissions and ambient noise were also recorded by a towed hydrophone array, by acoustic Seagliders, and by ocean bottom seismometers. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Acoustical Society of America is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=90559317&site=ehost-live
682,The Signals and Systems Concept Inventory.,Kathleen Wage,IEEE Transactions on Education,189359,,Aug-05,48,3,448,14,18013061,10.1109/TE.2005.849746,IEEE,Article,DIGITAL signal processing; ELECTRONIC instruments; DIGITAL communications; TEACHING; INVENTORIES; STUDENTS; Instrument Manufacturing for Measuring and Testing Electricity and Electrical Signals,Active learning; assessment; conceptual learning; signal processing education,"The signal processing community needs quantitative standardized tools to assess student learning in order to improve teaching methods and satisfy accreditation requirements. The Signals and Systems Concept Inventory (SSCI) is a 25-question multiple-choice exam designed to measure students' understanding of fundamental concepts taught in standard signals and systems curricula. When administered as a pre- and postcourse assessment, the SSCI measures the gain in conceptual understanding as a result of instruction. This paper summarizes the three-year development of this new assessment instrument and presents results obtained from testing with a pool of over 900 students from seven schools. Initial findings from the SSCI study show that students in traditional lecture courses master approximately 20% of the concepts they do not know prior to the start of the course. Other results highlight the most common student misconceptions and quantify the correlation between signals and systems and prerequisite courses. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Education is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18013061&site=ehost-live
683,Vertical line array measurements of ambient noise in the North Pacific.,Kathleen Wage,Journal of the Acoustical Society of America,14966,,Mar-17,141,3,1571,11,122305763,10.1121/1.4976706,American Institute of Physics,Article,AMBIENT sounds; OCEAN sounds; ACOUSTIC arrays; UNDERWATER acoustics; NORTH Pacific Ocean,,"Shipping noise and wind are the dominant sources of ocean noise in the frequency band between 20 and 500 Hz. This paper analyzes noise in that band using data from the SPICEX experiment, which took place in the North Pacific in 2004-2005, and compares the results with other North Pacific experiments. SPICEX included vertical arrays with sensors above and below the surface conjugate depth, facilitating an analysis of the depth dependence of ambient noise. The paper includes several key results. First, the 2004-05 noise levels at 50 Hz measured in SPICEX had not increased relative to levels measured by Morris [(1978). J. Acoust. Soc. Am. 64, 581-590] at a nearby North Pacific site three decades earlier, but rather were comparable to those levels. Second, at 50 Hz the noise below the conjugate depth decreases at a rate of -9.9 dB/km, which is similar to the rate measured by Morris and much less than the rate measured by Gaul, Knobles, Shooter, and Wittenborn [(2007). IEEE J. Ocean. Eng. 32, 497-512] for the CHURCH OPAL experiment. Finally, the paper describes the seasonal trends in noise over the year-long time series of the measurements. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Acoustical Society of America is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122305763&site=ehost-live
684,High-throughput profiling of histone post-translational modifications and chromatin modifying proteins by reverse phase protein array.,Xuan Wang,Journal of Proteomics,18743919,,Jun-22,262,,N.PAG,1,156999945,10.1016/j.jprot.2022.104596,Elsevier B.V.,Article,PROTEIN microarrays; POST-translational modification; HUMAN biology; DEVELOPMENTAL biology; CELL culture; PLURIPOTENT stem cells; HISTONES,Breast cancer; Epigenetics; High-throughput; Induced pluripotent stem cells; Post-translational modifications; RPPA,"Epigenetic variation plays a significant role in normal development and human diseases including cancer, in part through post-translational modifications (PTMs) of histones. Identification and profiling of changes in histone PTMs, and in proteins regulating PTMs, are crucial to understanding diseases, and for discovery of epigenetic therapeutic agents. In this study, we have adapted and validated an antibody-based reverse phase protein array (RPPA) platform for profiling 20 histone PTMs and expression of 40 proteins that modify histones and other epigenomic regulators. The specificity of the RPPA assay for histone PTMs was validated with synthetic peptides corresponding to histone PTMs and by detection of histone PTM changes in response to inhibitors of histone modifier proteins in cell cultures. The useful application of the RPPA platform was demonstrated with two models: induction of pluripotent stem cells and a mouse mammary tumor progression model. Described here is a robust platform that includes a rapid microscale method for histone isolation and partially automated workflows for analysis of histone PTMs and histone modifiers that can be performed in a high-throughput manner with hundreds of samples. This RPPA platform has potential for translational applications through the discovery and validation of epigenetic states as therapeutic targets and biomarkers. Our study has established an antibody-based reverse phase protein array platform for global profiling of a wide range of post-translational modifications of histones and histone modifier proteins. The high-throughput platform provides comprehensive analyses of epigenetics for biological research and disease studies and may serve as screening assay for diagnostic purpose or therapy development. [Display omitted] • Reverse Phase Protein Array (RPPA) was adapted to profile histone PTMs. • RPPA simultaneously profiles expression level of proteins modifying histones. • RPPA is reproducible, high-throughput, and scalable for all histone modifications. • RPPA provides a valuable tool for study of developmental biology and human disease. [ABSTRACT FROM AUTHOR] Copyright of Journal of Proteomics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156999945&site=ehost-live
685,Adaptive modulation and coding in underwater acoustic communications: a machine learning perspective.,Yue Wang,EURASIP Journal on Wireless Communications & Networking,16871472,,10/17/20,2020,1,N.PAG,1,146494374,10.1186/s13638-020-01818-x,Springer Nature,Article,UNDERWATER acoustic communication; ADAPTIVE modulation; MODULATION coding; WIRELESS channels; MACHINE learning; MARINE resources,Adaptive modulation and coding (AMC); Harsh oceanic environment; Machine learning (ML); Underwater acoustic communication (UAC),"The increasing demand for exploring and managing the vast marine resources of the planet has underscored the importance of research on advanced underwater acoustic communication (UAC) technologies. However, owing to the severe characteristics of the oceanic environment, underwater acoustic (UWA) propagation experiences nearly the harshest wireless channels in nature. This article resorts to the perspective of machine learning (ML) to cope with the major challenges of adaptive modulation and coding (AMC) design in UACs. First, we present an ML AMC framework for UACs. Then, we propose an attention-aided k-nearest neighbor (A-kNN) algorithm with simplicity and robustness, based on which an ML AMC approach is designed with immunity to channel modeling uncertainty. Leveraging its online learning ability, such A-kNN-based AMC classifier offers salient capabilities of both sustainable self-enhancement and broad applicability to various operation scenarios. Next, aiming at higher implementation efficiency, we take strategies of complexity reduction and present a dimensionality-reduced and data-clustered A-kNN (DRDC-A-kNN) AMC classifier. Finally, we demonstrate that these proposed ML approaches have superior performance over traditional model-based methods by simulations using actual data collected from three lake experiments. [ABSTRACT FROM AUTHOR] Copyright of EURASIP Journal on Wireless Communications & Networking is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146494374&site=ehost-live
686,Efficient two-dimensional line spectrum estimation based on decoupled atomic norm minimization.,Yue Wang,Signal Processing,1651684,,Oct-19,163,,95,12,136840664,10.1016/j.sigpro.2019.04.024,Elsevier B.V.,Article,TOEPLITZ matrices; RADAR signal processing; RADIO astronomy; MAGNITUDE (Mathematics); MATHEMATICAL optimization; COMPUTATIONAL complexity,Atomic norm minimization; Decoupled ANM; Line spectrum estimation; Semi-definite programming; Two-dimensional,"This paper presents an efficient optimization technique for gridless 2-D line spectrum estimation, named decoupled atomic norm minimization (D-ANM). The framework of atomic norm minimization (ANM) is considered, which has been successfully applied in 1-D problems to allow super-resolution frequency estimation for correlated sources even when the number of snapshots is highly limited. The state-of-the-art 2-D ANM approach vectorizes the 2-D measurements to their 1-D equivalence, which incurs huge computational cost and may become too costly for practical applications. We develop a novel decoupled approach of 2-D ANM via semi-definite programming (SDP), which introduces a new matrix-form atom set to naturally decouple the joint observations in both dimensions without loss of optimality. Accordingly, the original large-scale 2-D problem is equivalently reformulated via two decoupled one-level Toeplitz matrices, which can be solved by simple 1-D frequency estimation with pairing. Compared with the conventional vectorized approach, the proposed D-ANM technique reduces the computational complexity by several orders of magnitude with respect to the problem size, at no loss of optimality. It also retains the benefits of ANM in terms of precise signal recovery, small number of required measurements, and robustness to source correlation. The complexity benefits are particularly attractive for large-scale antenna systems such as massive MIMO, radar signal processing and radio astronomy. [ABSTRACT FROM AUTHOR] Copyright of Signal Processing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136840664&site=ehost-live
687,Mice lacking the transcriptional regulator Bhlhe40 have enhanced neuronal excitability and impaired synaptic plasticity in the hippocampus.,Yue Wang,PLoS ONE,19326203,,1/5/18,13,5,1,22,129375500,10.1371/journal.pone.0196223,Public Library of Science,Article,TRANSCRIPTION factors; NEUROPLASTICITY; HIPPOCAMPUS physiology; EXCITATORY postsynaptic potential; DNA-protein interactions,Amniotes; Anatomy; Animal genomics; Animal models; Animals; Biochemistry; Biology and life sciences; Brain; Cellular neuroscience; Developmental neuroscience; Diabetic endocrinology; Endocrinology; Eukaryota; Experimental organism systems; Gene expression; Genetics; Genomics; Hippocampus; Hormones; Insulin; Mammalian genomics; Mammals; Medicine and health sciences; Mice; Model organisms; Mouse models; Neuronal plasticity; Neuroscience; Organisms; Research and analysis methods; Research Article; Rodents; Synaptic plasticity; Vertebrates,"Bhlhe40 is a transcription factor that is highly expressed in the hippocampus; however, its role in neuronal function is not well understood. Here, we used Bhlhe40 null mice on a congenic C57Bl6/J background (Bhlhe40 KO) to investigate the impact of Bhlhe40 on neuronal excitability and synaptic plasticity in the hippocampus. Bhlhe40 KO CA1 neurons had increased miniature excitatory post-synaptic current amplitude and decreased inhibitory post-synaptic current amplitude, indicating CA1 neuronal hyperexcitability. Increased CA1 neuronal excitability was not associated with increased seizure severity as Bhlhe40 KO relative to +/+ (WT) control mice injected with the convulsant kainic acid. However, significant reductions in long term potentiation and long term depression at CA1 synapses were observed in Bhlhe40 KO mice, indicating impaired hippocampal synaptic plasticity. Behavioral testing for spatial learning and memory on the Morris Water Maze (MWM) revealed that while Bhlhe40 KO mice performed similarly to WT controls initially, when the hidden platform was moved to the opposite quadrant Bhlhe40 KO mice showed impairments in relearning, consistent with decreased hippocampal synaptic plasticity. To investigate possible mechanisms for increased neuronal excitability and decreased synaptic plasticity, a whole genome mRNA expression profile of Bhlhe40 KO hippocampus was performed followed by a chromatin immunoprecipitation sequencing (ChIP-Seq) screen of the validated candidate genes for Bhlhe40 protein-DNA interactions consistent with transcriptional regulation. Of the validated genes identified from mRNA expression analysis, insulin degrading enzyme (Ide) had the most significantly altered expression in hippocampus and was significantly downregulated on the RNA and protein levels; although Bhlhe40 did not occupy the Ide gene by ChIP-Seq. Together, these findings support a role for Bhlhe40 in regulating neuronal excitability and synaptic plasticity in the hippocampus and that indirect regulation of Ide transcription may be involved in these phenotypes. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129375500&site=ehost-live
688,Performance bounds of compressive classification under perturbation.,Yue Wang,Signal Processing,1651684,,Mar-21,180,,N.PAG,1,147405032,10.1016/j.sigpro.2020.107855,Elsevier B.V.,Article,CLASSIFICATION; MOTIVATION (Psychology); GAUSSIAN distribution,Classification; Compressive sensing; Performance bound; Perturbation,"• The upper bound and the lower bound on the possibility of misclassification of the compressive classification under measurement perturbation are derived. • Our analysis reveals that the performance of compressive classification is dependent on the number of measurements, the variance of the noise, and the measurement perturbation. • When the measurement perturbations for different hypotheses are different, the performance of compressive classification is better than the one with the same measurement perturbation. • We propose an improved sparse representation classification framework, which models the perturbation factor in the training samples as the perturbation term in the dictionary. Recently, compressive sensing based classification, which is called compressive classification, has drawn a lot of attention, since it works directly in the compressive domain with low complexity. However, existing literatures assume perfectly known measurement matrix during compressive classification, which is impossible in many practical situations. In this paper, we focus on studying the performance of classification based on the compressive measurements under the perturbation, where the perturbation models the uncertainty of the measurement matrix. The upper and the lower bounds on the probability of misclassification of the compressive classification are evaluated by utilizing the Kullback-Leibler and Chernoff distances. Our results indicate that the performance depends on the variance of the perturbation when the perturbation obeys Gaussian distribution with zero mean. Moreover, compared with the one without perturbation, the performance of the compressive classification can be improved when the perturbation in each hypothesis is different from each other. Motivated by these observations, we design an improved sparse representation classification (SRC) framework by incorporating the perturbation item into the SRC framework and propose several enhanced SRC schemes for performance improvement. The experiments on MNIST datasets validate that the proposed SRC schemes outperform the existing standard SRC scheme. [ABSTRACT FROM AUTHOR] Copyright of Signal Processing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147405032&site=ehost-live
689,Performance limits of one-bit compressive classification.,Yue Wang,Signal Processing,1651684,,Jan-21,178,,N.PAG,1,146398367,10.1016/j.sigpro.2020.107808,Elsevier B.V.,Article,PROBABILITY density function; SIGNAL classification; SIGNAL processing; GAUSSIAN distribution,Compressive classification (CC); Compressive sensing (CS); One-bit CC; One-bit CS; Performance limits,"• When the signals are not necessarily sparse, we derive the upper bound on the probability of misclassification in the high-dimensional setting. • For the signals that are not necessarily sparse, we derive the lower bound on the probability of misclassification in the high-dimensional setting. • For high-dimensional sparse signals, we derive the performance limits on the probability of misclassification when the nonzero entries follow the first order Gaussian distribution. Classification is an important task in the fields of signal processing and machine learning. Recently, compressive classification (CC) appears to enable signal classification directly with the compressive measurements, which are acquired by the technique of compressive sensing (CS). However, the existing works of CC ignore the practical quantization operation. This paper studies the one-bit CC based on one-bit quantized compressive measurements, which is appealing in saving the transmission and storage bits in many applications when the bandwidth and energy are constrained. For the signals that are not necessarily sparse as well as exactly sparse, we provide the performance limits of one-bit CC. We first analyze the Chernoff distance and Kullback–Leibler (KL) distance between two probability density functions under any two hypotheses. The results are then used to derive the upper and lower bounds on the probability of misclassification in the high-dimensional setting. The analytical results show that one-bit quantized CC has the same performance trend as conventional not-quantized CC when the number of measurements increases. [ABSTRACT FROM AUTHOR] Copyright of Signal Processing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146398367&site=ehost-live
690,A Martingale Framework for Detecting Changes in Data Streams by Testing Exchangeability.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,1628828,,Dec-10,32,12,2113,0,54885885,10.1109/TPAMI.2010.48,IEEE,Article,,Change detection; classification; clustering; Data models; data stream; exchangeability; Games; hypothesis testing; Manganese; martingale; Random variables; regression; support vector machine; Support vector machines; Testing; Training,"In a data streaming setting, data points are observed sequentially. The data generating model may change as the data are streaming. In this paper, we propose detecting this change in data streams by testing the exchangeability property of the observed data. Our martingale approach is an efficient, nonparametric, one-pass algorithm that is effective on the classification, cluster, and regression data generating models. Experimental results show the feasibility and effectiveness of the martingale methodology in detecting changes in the data generating model for time-varying data streams. Moreover, we also show that: 1) An adaptive support vector machine (SVM) utilizing the martingale methodology compares favorably against an adaptive SVM utilizing a sliding window, and 2) a multiple martingale video-shot change detector compares favorably against standard shot-change detection algorithms. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=54885885&site=ehost-live
691,Biometric Security and Privacy Using Smart Identity Management and Interoperability: Validation and Vulnerabilities of Various Techniques.,Harry Wechsler,Review of Policy Research,1541132X,,Jan-12,29,1,63,24,70469404,10.1111/j.1541-1338.2011.00538.x,Wiley-Blackwell,Article,ETHICS; BIOMETRIC identification; DATA mining; RIGHT of privacy; ANONYMITY; IDENTIFICATION; FORENSIC sciences; RELIABILITY (Personality trait); MASS surveillance,,"The central position of this article is that validation and interoperability are paramount for the effective and ethical use of biometrics. Illuminating the relevance for policymakers of the science underlying the security and privacy aspects of biometrics, this article calls for adequate and enforceable performance metrics that can be independently corroborated. Accordingly, the article considers biometrics and forensics for the dual challenges of addressing security and privacy using smart identity management. The discussion revolves around the concepts of 'personally identifiable information' (PII) and interoperability with emphasis on quantitative performance analysis and validation for uncontrolled operational settings, variable demographics, and distributed and federated operations. Validation metrics includes expected rates of identification/misidentification, precision, and recall. The complementary concepts of identity and anonymity are addressed in terms of expected performance, functionality, law and ethics, forensics, and statistical learning. Biometrics encompasses appearance, behavior, and cognitive state or intent. Modes of deployment and performance evaluation for biometrics are detailed, with operational and adversarial challenges for both security and privacy described in terms of trustworthiness, vulnerabilities, functional creep, and feasibility of safeguards. The article underscores how lack of interoperability is mostly due to overfitting and tuning to well-controlled settings, so that validation merely confirms 'teaching to the test' rather than preparation for real-world deployment. Most important for validation is reproducibility of results including full information on the experimental design used, that forensic exclusion is allowed, and that scientific methods for performance evaluation are followed. The article concludes with expected developments regarding technology use and advancements that bear on security and privacy, including data streams and video, de-anonymization and reidentification, social media analytics and cyber security, and smart camera networks and surveillance. [ABSTRACT FROM AUTHOR] Copyright of Review of Policy Research is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=70469404&site=ehost-live
692,Biometric surveillance using visual question answering.,Harry Wechsler,Pattern Recognition Letters,1678655,,Sep-19,126,,111,8,138726453,10.1016/j.patrec.2018.02.013,Elsevier B.V.,Article,EYE; QUESTIONING; TURING test; QUERY (Information retrieval system); VIDEO compression,Biometrics; Deep learning; Forensics; Question relevance; Surveillance; Visual question answering; Visual turing test,"• Novel biometric surveillance system based on Visual Question Answering. • Reuse of pre-trained components without fine tuning to demonstrate system robustness. • Introduction of novel models for biometric-focused question image relevance. • New datasets for biometric-based surveillance tasks. Surveillance of individuals using visual data requires human-level capabilities for understanding the characteristics that differentiate one person from another. However, because the influx of both video and imagery is increasing at a greater rate than humans can cope with, biometric-based surveillance systems are required to assist with the triage of information based on human-generated queries. Unfortunately, current systems are not robust enough to tackle new tasks, as they involve specialized models that do not leverage existing, pre-trained components. To mitigate these issues, we propose a novel system for biometric-based surveillance that utilizes models that are relevance-aware to triage images and videos based on interaction with single or multiple users. As the system is initially focused on detection of people via their appearance and clothing, we have named the system Context and Collaborative (C2) Visual Question Answering (VQA) for Biometric Object-Attribute Relevance and Surveillance (C2VQA-BOARS). To validate the usefulness of C2VQA-BOARS in real-world scenarios, we provide an implementation of two novel components (Relevance and Triage) and apply them in tasks against two datasets created for biometric surveillance. Our results outperform baseline approaches, proving that a system with a minimal amount of fine-tuned components can robustly handle new datasets and problems as needed. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138726453&site=ehost-live
694,Eye movement analysis for human authentication: a critical survey.,Harry Wechsler,Pattern Recognition Letters,1678655,,Dec-16,84,,272,12,119777393,10.1016/j.patrec.2016.11.002,Elsevier B.V.,Article,EYE tracking; BIOMETRIC identification; INDIVIDUALIZED medicine; MEDICAL care; PERFORMANCE evaluation,Active and dynamic biometrics; Eye movements; GANT; Gaze analysis; Gaze scan paths; HCI; Identity management; Performance evaluation; Uncontrolled settings,"This paper addresses the active and dynamic nature of biometrics, in general, and gaze analysis, in particular, including motivation and background. The paper includes a critical survey of existing gaze analysis methods, challenges due to uncontrolled settings and lack of standards, and outlines promising future R&D directions. Criteria for performance evaluation are proposed, and state-of-the art gaze analysis methods are compared on the same database set. Performance improvement would come from richer stimuli including task dependent user profiles, with applications going much beyond identity management to include personalized medical care and rehabilitation, privacy, marketing, and education. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119777393&site=ehost-live
696,Linguistics and face recognition,Harry Wechsler,Journal of Visual Languages & Computing,1045926X,,Jun-09,20,3,145,11,39776716,10.1016/j.jvlc.2009.01.001,Academic Press Inc.,Article,FACE perception; VISUAL perception; BIOMETRY; SEMANTICS,Authentication; Biometrics; Boosting; Clustering; Cross-validation; Data fusion; Face recognition; Feature selection; FERET; Forensics; FRGC; ICA; k Nearest neighbor; Likelihood ratio; Linguistics; Margin; MDL; Multimodal integration; Neyman–Pearson; Occlusion; p-Values; Parsing; Random deficiency; Ranking; Recognition; Recognition-by-parts; Segmentation; SIFT; Strangeness; Surveillance; Transduction; Typicality,"Abstract: We describe in this paper a novel biometric methodology for face recognition suitable to address pose, illumination, and expression (PIE) image variability, temporal change, flexible matching, and last but not least occlusion and disguise that are usually referred to as denial and deception. The adverse conditions listed above affect the scope and performance of biometric analysis vis-à-vis both training and testing. The conceptual framework proposed here draws support from discriminative methods using likelihood ratios. At the conceptual level it links forensics and biometrics, while at the implementation level it links the Bayesian framework and statistical learning theory. As many of the concerns listed usually affect only parts of the face, a non-parametric recognition-by-part approach is advanced here for the purpose of reliable face recognition. Recognition-by-parts facilitates authentication because it does not seek for explicit invariance. Instead, it handles variability using component-based configurations that are flexible enough to compensate among others for limited pose changes, if any, and limited occlusion and disguise. The recognition-by-parts approach proposed here supports incremental and progressive processing. It is similar in nature to modern linguistics and practical intelligence with the emphasis on semantics and pragmatics. Layered categorization starts with face detection using implicit rather than explicit segmentation. It proceeds with face authentication that involves feature selection of local patch instances including dimensionality reduction, exemplar-based clustering of patches into parts, and data fusion for matching using boosting driven by parts that play the role of weak learners. The implementation, driven by transduction, employs proximity and typicality (ranking) realized using strangeness and random deficiency p-values, respectively. The feasibility and reliability of the proposed architecture has been validated using FERET and FRGC data. The paper concludes with suggestions for augmenting and enhancing the scope and utility of the recognition-by-parts architecture. [Copyright &y& Elsevier] Copyright of Journal of Visual Languages & Computing is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=39776716&site=ehost-live
697,"Micro-Doppler Effect in Radar: Phenomenon, Model, and Simulation Study.",Harry Wechsler,IEEE Transactions on Aerospace & Electronic Systems,189251,,Jan-06,42,1,2,20,20383039,10.1109/TAES.2006.1603402,IEEE,Article,DOPPLER radar; DOPPLER effect; RADAR targets; ELECTRONIC modulation; ELECTRONIC pulse techniques; RADAR simulation,,"When, in addition to the constant Doppler frequency shift induced by the bulk motion of a radar target, the target or any structure on the target undergoes micro-motion dynamics, such as mechanical vibrations or rotations, the micro-motion dynamics induce Doppler modulations on the returned signal, referred to as the micro-Doppler effect. We introduce the micro-Doppler phenomenon in radar, develop a model of Doppler modulations, derive formulas of micro-Doppler induced by targets with vibration, rotation, tumbling and coning motions, and verify them by simulation studies, analyze time-varying micro-Doppler features using high-resolution time-frequency transforms, and demonstrate the micro-Doppler effect observed in real radar data. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Aerospace & Electronic Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20383039&site=ehost-live
698,"Mobile Iris Challenge Evaluation (MICHE)-I, biometric iris dataset and protocols.",Harry Wechsler,Pattern Recognition Letters,1678655,,May-15,57,,17,7,102208391,10.1016/j.patrec.2015.02.009,Elsevier B.V.,Article,BIOMETRIC identification; DATABASES; COMPUTER network protocols; COMPUTER simulation; IRIS recognition,Iris biometric; Iris challenge; Mobile devices,"We introduce and describe here MICHE-I, a new iris biometric dataset captured under uncontrolled settings using mobile devices. The key features of the MICHE-I dataset are a wide and diverse population of subjects, the use of different mobile devices for iris acquisition, realistic simulation of the acquisition process (including noise), several data capture sessions separated in time, and image annotation using metadata. The aim of MICHE-I dataset is to make up the starting core of a wider dataset that we plan to collect, with the further aim to address interoperability, both in the sense of matching samples acquired with different devices and of assessing the robustness of algorithms to the use of devices with different characteristics. We discuss throughout the merits of MICHE-I with regard to biometric dimensions of interest including uncontrolled settings, demographics, interoperability, and real-world applications. We also consider the potential for MICHE-I to assist with developing continuous authentication aimed to counter adversarial spoofing and impersonation, when the bar for uncontrolled settings raises even higher for proper and effective defensive measures. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=102208391&site=ehost-live
699,Modern art challenges face detection.,Harry Wechsler,Pattern Recognition Letters,1678655,,Sep-19,126,,3,8,138726454,10.1016/j.patrec.2018.02.014,Elsevier B.V.,Article,MODERN art; POP art; HUMAN facial recognition software; TURING test; COMPUTER vision; FUSIFORM gyrus,Biometrics; Face detection; Forensics; Interoperability; Modern art; Visual Turing test,"• Creation of a new face detection dataset (MAFD-150) based on modern art. • Describes face detection on modern art as a challenging task for Visual Turing test. • Analysis of the characteristics of modern art that make it challenging for algorithms. • Challenge to claims of ""human-level performance"" on face detection and recognition. • Performance evaluation for face detection on modern art. There is a widely held belief that computer vision, in general, and face authentication, in particular, are to a large extent solved problems. This paper challenges this belief regarding face authentication using examples from modern art that significantly confound face detection. The challenges are made concrete using a new MAFD-150 dataset (M odern A rt F ace D etection) composed mostly of modern art examples that cover much diversity in style and artists. MAFD-150 challenges the belief that singleton and crowd face detection is an almost solved problem, and provides baselines and preliminary results that highlight the inadequacy of current expertise and methods to address face detection. In particular, we show that well-known face detection algorithms are only able to achieve an F1 score of less than 35% overall across the new dataset. Additionally, we discuss the performance of the selected face detectors on varying art categories (such as Impressionism, Pop Art, et al.) to show how style and face representation may impact these algorithms. The paper concludes with suggestions on how to advance face processing by leveraging the complementarity between Show-and-Tell-like methods and a context and cooperative driven visual question answering framework using relevance-based triage. The very challenges detailed throughout are then shown to be helpful with developing novel, robust, and secure access protocols that combine text and modern art images using the visual question answering framework. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138726454&site=ehost-live
700,Motion Estimation Using Statistical Learning Theory.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,1628828,,Apr-04,26,4,466,13,12798701,10.1109/TPAMI.2004.1265862,IEEE,Article,MOTION; THEORY; LEARNING; STATISTICS; MATHEMATICAL models; IMAGING systems; Photographic and Photocopying Equipment Manufacturing; Computer Terminal and Other Computer Peripheral Equipment Manufacturing,,"This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12798701&site=ehost-live
701,Open Set Face Recognition Using Transduction.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,1628828,,Nov-05,27,11,1686,12,18530864,10.1109/TPAMI.2005.224,IEEE,Article,"BIOMETRY; KOLMOGOROV complexity; NUMERICAL analysis; MATHEMATICAL statistics; ELECTRONIC data processing; IDENTIFICATION documents; Data Processing, Hosting, and Related Services",(multiclass) transduction; Biometrics; clustering; confidence; credibility; data fusion; face recognition; face surveillance; information quality; Kolmogorov complexity; open set recognition; outlier detection; performance evaluation; PSEI (pattern specific error inhomogeneities); randomness deficiency; strangeness; watch list,"This paper motivates and describes a novel realization of transductive inference that can address the Open Set face recognition task. Open Set operates under the assumption that not all the test probes have mates in the gallery. It either detects the presence of some biometric signature within the gallery and finds its identity or rejects it, i.e., it provides for the ""none of the above"" answer. The main contribution of the paper is Open Set TCM-kNN (Transduction Confidence Machine-k Nearest Neighbors), which is suitable for multiclass authentication operational scenarios that have to include a rejection option for classes never enrolled in the gallery. Open Set TCM-kNN, driven by the relation between transduction and Kolmogorov complexity, provides a local estimation of the likelihood ratio needed for detection tasks. We provide extensive experimental data to show the feasibility, robustness, and comparative advantages of Open Set TCM-kNN on Open Set identification and watch list (surveillance) tasks using challenging FERET data. Last, we analyze the error structure driven by the fact that most of the errors in identification are due to a relatively small number of face patterns. Open Set TCM-kNN is shown to be suitable for PSEI (pattern specific error inhomogeneities) error analysis in order to identify difficult to recognize faces. PSEI analysis improves biometric performance by removing a small number of those difficult to recognize faces responsible for much of the original error in performance and/or by using data fusion. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18530864&site=ehost-live
702,Pattern recognition: Historical perspective and future directions.,Harry Wechsler,International Journal of Imaging Systems & Technology,8999457,,Mar-00,11,2,101,16,13509961,10.1002/1098-1098(2000)11:2<101::AID-IMA1>3.0.CO;2-J,Wiley-Blackwell,Article,"PATTERN recognition systems; PATTERN perception; FORM perception; ARTIFICIAL intelligence; COMPUTER science; ELECTRICAL engineering; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Engineering Services",categorization; classification; feature extraction; feature selection; functional approximation; induction; pattern recognition; performance evaluation; predictive learning; representation,"“What being walks sometimes on two feet, sometimes on three, and sometimes on four, and is weakest when it has the most?” —The Sphinx's Riddle Pattern recognition is one of the most important functionalities for intelligent behavior and is displayed by both biological and artificial systems. Pattern recognition systems have four major components: data acquisition and collection, feature extraction and representation, similarity detection and pattern classifier design, and performance evaluation. In addition, pattern recognition systems are successful to the extent that they can continuously adapt and learn from examples; the underlying framework for building such systems is predictive learning. The pattern recognition problem is a special case of the more general problem of statistical regression; it seeks an approximating function that minimizes the probability of misclassification. In this framework, data representation requires the specification of a basis set of approximating functions. Classification requires an inductive principle to design and model the classifier and an optimization or learning procedure for classifier parameter estimation. Pattern recognition also involves categorization: making sense of patterns not previously seen. The sections of this paper deal with the categorization and functional approximation problems; the four components of a pattern recognition system; and trends in predictive learning, feature selection using “natural” bases, and the use of mixtures of experts in classification. © 2000 John Wiley & Sons, Inc. Int J Imaging Syst Technol 11, 101–116, 2000 [ABSTRACT FROM AUTHOR] Copyright of International Journal of Imaging Systems & Technology is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=13509961&site=ehost-live
703,Query by transduction.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,1628828,,Sep-08,30,9,1557,15,34230536,10.1109/TPAMI.2007.70811,IEEE,Article,INFORMATION storage & retrieval systems; EXPERIENTIAL learning; CLASSIFICATION; COMPUTER algorithms; INFORMATION retrieval; COMPUTER systems; QUERY (Information retrieval system); Computer Systems Design Services; Computer systems design and related services (except video game design and development),Active learning; hypothesis testing; Kolmogorov complexity; support vector machine; transductive inference,"There has recently been a growing interest in the use of transductive inference for learning. We expand here the scope of transductive inference to active learning in a stream-based setting. Toward that end, this paper proposes Query-by-Transduction (QBT) as a novel active learning algorithm. QBT queries the label of an example based on the p-values obtained using transduction. We show that QBT is closely related to Query-by-Committee (QBC) using relations between transduction, Bayesian statistical testing, Kuliback-Leibler divergence, and Shannon information. The feasibility and utility of QBT is shown on both binary and multiclass classification tasks using a support vector machine (SVM) as the choice classifier. Our experimental results show that QBT compares favorably, in terms of mean generalization, against random sampling, committee-based active learning, margin-based active learning, and QBC in the stream-based setting. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34230536&site=ehost-live
704,Robust face recognition after plastic surgery using region-based approaches.,Harry Wechsler,Pattern Recognition,313203,,Apr-15,48,4,1257,16,100157071,10.1016/j.patcog.2014.10.004,Elsevier B.V.,Article,HUMAN facial recognition software; PLASTIC surgery; ROBUST control; FACIAL expression; CODING theory; COMPUTER reliability,Beautification; Face recognition; Local features; Plastic surgery; Regions of interest (ROI); Response reliability,"This paper advances the use of region-based strategies for addressing the problem of face recognition after plastic surgery. The proposed methods implement the region-based approach in several ways. FARO (FAce Recognition against Occlusions and Expression Variations) divides the face into relevant regions (left eye, right eye, nose and mouth) and then codes them independently using Partitioned Iterated Function System (PIFS) processing. FACE (Face Analysis for Commercial Entities) applies a localized version of image correlation index. Finally, the Split Face Architecture (SFA), adaptive and integrative in nature, can leverage any known recognition method, from PCA to most recent ones (including FARO and FACE), provided that it is possible to divide the face into regions. Experimental results, compared with those available from recent experiments reported in literature, show that our methods yield much better performance than state-of-the art algorithms, both holistic and region based. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100157071&site=ehost-live
705,Robust human authentication using appearance and holistic anthropometric features,Harry Wechsler,Pattern Recognition Letters,1678655,,Nov-10,31,15,2425,11,53405253,10.1016/j.patrec.2010.07.011,Elsevier B.V.,Article,BIOMETRIC identification; ANTHROPOMETRY; DECISION making; STATISTICAL matching; FEATURE extraction; HUMAN facial recognition software; ROBUST control,Anthropometry; Biometrics; Face recognition; Feature selection; Occlusion and disguise; Soft biometrics,"Abstract: We propose here decision-level fusion using neural networks and feature-level fusion using boosting for the purpose of robust human authentication vis-à-vis face occlusion and disguise. Holistic anthropometric and appearance-based features feed the data fusion stage. In addition to standard head and face geometric measurements, the proposed holistic anthropometric features include additional measurements below the face to describe the neck and shoulder and their contextual relations to head and face. The appearance-based features include standard PCA or Fisherfaces. Experimental data shows the feasibility and utility of the proposed hybrid (extended geometry+appearance) approach for robust human authentication vis-à-vis occluded and/or degraded face biometrics. The authentication results presented compare favorably against both appearance-based methods and hybrid methods with anthropometric features confined to face and head. The methods proposed can train on clean data and authenticate on corrupt data, or train on corrupt data and authenticate on clean data. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53405253&site=ehost-live
706,Robust re-identification using randomness and statistical learning: Quo vadis,Harry Wechsler,Pattern Recognition Letters,1678655,,Oct-12,33,14,1820,8,79559551,10.1016/j.patrec.2012.02.005,Elsevier B.V.,Article,"ROBUST control; STATISTICS; BIOMETRIC identification; ELECTRONIC data processing; COMBINATORIAL analysis; COMPUTER architecture; Data Processing, Hosting, and Related Services",Biometrics; Evidence-based management; Face recognition; Identity management; Re-identification; Statistical learning theory,"Abstract: The re-identification problem is to match objects across multiple but possibly disjoint fields of view for the purpose of sequential authentication over space and time. Detection and seeding for initialization do not presume known identity and allow for re-identification of objects and/or faces whose identity might remain unknown. Specific functionalities involved in re-identification include clustering and selection, recognition-by-parts, anomaly and change detection, sampling and tracking, fast indexing and search, sensitivity analysis, and their integration for the purpose of identity management. As re-identification processes data streams and involves change detection and on-line adaptation three complementary statistical learning frameworks, driven by randomness for the purpose of robust prediction, are advanced here to support the functionalities listed earlier and their combination thereof. The intertwined learning frameworks employed are those of (a) semi-supervised learning (SSL); (b) transduction; and (c) conformal prediction. The overall architecture proposed is data-driven and modular, on one side, and discriminative and progressive, on the other side. The architecture is built around autonomic computing and W5+. Autonomic computing or self-management provides for closed-loop control. W5+ answers questions related to What data to consider for sampling and collection, When to capture the data and from Where, and How to best process the data. The Who (is) query is about identity for biometrics, and the Why question for explanation purposes. The challenge addressed throughout is that of evidence-based management to progressively collect and add value to data in order to generate knowledge that leads to purposeful and gainful action including active learning for the overall purpose of re-identification. A venue for future research includes adversarial learning when re-identification is possibly “distracted” using deliberate corrupt information. [Copyright &y& Elsevier] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=79559551&site=ehost-live
707,Spam detection using Random Boost,Harry Wechsler,Pattern Recognition Letters,1678655,,Jul-12,33,10,1237,8,76157688,10.1016/j.patrec.2012.03.012,Elsevier B.V.,Article,RANDOM variables; DETECTORS; SPAM email; ROBUST control; PERFORMANCE evaluation; COMPUTATIONAL complexity; RANDOM projection method,Logit Boost; Random Boost; Random Forest; Random projection; Robust learning; Spam detection,"Abstract: This paper proposes two alternative methods of random projections and compares their performance for robust and efficient spam detection when trained using a small number of examples. Robustness refers to learning and adaptation leading to a high level of performance despite data variability, while efficiency is concerned with (i) the complexity of the detection method employed; and (ii) the amount of training resources used for training and retraining. The first method, Random Project, employs a random projection matrix to produce linear combinations of input features, while the second method, Random Boost, employs random feature selection to enhance the performance of the Logit Boost algorithm. Random Boost is, in fact, a combination of Logit Boost and Random Forest. Experimental results, using TREC and CEAS as challenging spam benchmark sets, show that the Random Boost method significantly improves the performance of the spam filter compared to the Logit Boost algorithm (e.g., a 5% increase in AUC, which is the area under the Receiver Operating Characteristic curve), and yields similar classification accuracy compared to the Random Forest method but using only one fourth the runtime complexity of the Random Forest algorithm. Additionally, the Random Boost algorithm also reduces training time by two orders of magnitude compared to Logit Boost, which becomes important during retraining on the ever changing data streams, including adapting to adversarial tactics and “noise” injected by spammers. [Copyright &y& Elsevier] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=76157688&site=ehost-live
708,Towards demographic categorization using gaze analysis.,Harry Wechsler,Pattern Recognition Letters,1678655,,Oct2016 Part 2,82,,226,6,119158561,10.1016/j.patrec.2015.08.018,Elsevier B.V.,Article,EYE tracking; MEANS of communication for people with disabilities; AMYOTROPHIC lateral sclerosis; BIOMETRIC eye scanning systems; DEMOGRAPHIC surveys; SUPPORT vector machines; FEATURE extraction,Adaboost; GANT; Gaze analysis; Gender and age categorization; K-fold cross validation; SVM,"Current use of gaze analysis, which is mostly restricted to eye gaze tracking for augmentative and alternative communication (AAC) medium, can benefit people afflicted with amyotrophic lateral sclerosis (ALS). This paper advances the use of gaze analysis for biometrics purposes related to gender and age demographics to benefit applications related to retail space for targeted advertising, behavioral biometrics to benefit health care, and surveillance applications. Towards that end, this paper expands on the recently introduced Gaze ANalysis Technique (GANT) for human identification to combine the length of time spent on observing patterns of interest and the scanning patterns for biometric representation with AdaBoost and super vector machines (SVM) subsequently used for biometric categorization. The experiments conducted show that while the initial results are promising further innovation and development is required to make gaze analysis a viable alternative for demographics categorization, on its own, or together with other biometrics. Further improvements on performance are expected from the derivation, extraction, and use of alternative and novel gaze driven features. This will include among others additional information that is already available about the arc features connecting the fixation points and the dynamics they encode about the roving gaze. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119158561&site=ehost-live
709,Biomechanical modeling of actively controlled rectus extraocular muscle pulleys.,Qi Wei,Scientific Reports,20452322,,4/6/22,12,1,1,8,156155241,10.1038/s41598-022-09220-x,Springer Nature,Article,"PULLEYS; EYE muscles; MAGNETIC resonance imaging; GAZE; AXIOMS; ROTATIONAL motion; Diagnostic Imaging Centers; Overhead Traveling Crane, Hoist, and Monorail System Manufacturing; Material handling equipment manufacturing",,"The Active Pulley Hypothesis (APH) is based on modern functional anatomical descriptions of the oculomotor plant, and postulates behaviors of the orbital pulleys proposed to be positioned by the extraocular muscles (EOMs). A computational model is needed to understand this schema quantitatively. We developed and evaluated a novel biomechanical model of active horizontal rectus pulleys. The orbital (OL) and global (GL) layers of the horizontal rectus EOMs were implemented as separate musculoskeletal strands. Pulley sleeves were modeled as tube-like structures receiving the OL insertion and suspended by elastic strands. Stiffnesses and orientations of pulley suspensions were determined empirically to limit horizontal rectus EOM side-slip while allowing anteroposterior pulley travel. Independent neural drives of the OL greater than GL were assumed. The model was iteratively refined in secondary gazes to implement realistic behavior using the simplest mechanical configuration and neural control strategy. Simulated horizontal rectus EOM paths and pulley positions during secondary gazes were consistent with published MRI measurements. Estimated EOM tensions were consistent with the range of experimentally measured tensions. This model is consistent with postulated bilaminar activity of the EOMs, and the separate roles of the GL in ocular rotation, and OL in pulley positioning. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156155241&site=ehost-live
710,Correlation between pelvic floor ultrasound parameters and vaginal pressures in nulliparous women: a subanalysis of the SUM-AN study.,Qi Wei,International Urogynecology Journal,9373462,,Jun-22,33,6,1481,7,157529319,10.1007/s00192-022-05117-5,Springer Nature,Article,PELVIC floor; ULTRASONIC imaging; PUBIC symphysis; LOGISTIC regression analysis,Levator ani muscle hiatus; Nulliparous; Pelvic floor hiatus; Three-dimensional endovaginal manometry; Three-dimensional endovaginal ultrasound; Vaginal pressures,"Introduction and hypothesis: Pelvic floor ultrasound is used as a validated technique for measuring levator ani dimensions. Vaginal manometry has been used in the past as a method to assess levator ani muscle (LAM) strength. Whether the combination of both methods can contribute to our understanding of pelvic floor pathophysiology has not yet been described. We hypothesized that as female pelvic floor muscular hiatus increases, the vaginal pressure and strength decrease. Methods: We recruited 20 asymptomatic nulliparous women ages 18–85 years. Minimal levator hiatus (MLH) area, anteroposterior/left-right (AP/LR) diameter ratio, the distance between levator plate and the pubic symphysis (LP-PS) while at rest and squeeze were measured using endovaginal ultrasound (US). Vaginal pressure at rest, squeeze (Kegel) and Valsalva were measured using 3D manometry. Logistic and linear regression analysis was performed to assess correlations. Results: MLH area was negatively correlated with the sum of all the squeeze pressures produced on the four walls of the vagina (p = 0.049, R2 = 0.197). There was also a borderline negative correlation between MLH and the sum of rest pressures (p = 0.09, R2 = 0.15). AP/LR ratio was negatively correlated with the sum of squeeze pressures (p = 0.056, R2 = 0.197). LP-PS distances, both while at rest and during squeeze, were negatively correlated with the vaginal squeeze pressure (p = 0.046, R2 = 0.21; p = 0.011, R2 = 0.31, respectively). LP-V distance, both at rest and during squeeze, was negatively correlated with the sum of squeeze pressures on four vaginal walls (p = 0.02, R2 = 0.25; p = 0.005, R2 = 0.36, respectively). Conclusions: Stronger levator ani muscles, smaller MLH area and a more oval shape of pelvic floor hiatus as assessed by pelvic floor ultrasound are associated with higher squeeze vaginal pressures as assessed by 3D manometry. [ABSTRACT FROM AUTHOR] Copyright of International Urogynecology Journal is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157529319&site=ehost-live
711,Fast ray-tracing of human eye optics on Graphics Processing Units.,Qi Wei,Computer Methods & Programs in Biomedicine,1692607,,May-14,114,3,302,13,95713419,10.1016/j.cmpb.2014.02.003,Elsevier B.V.,Article,VISION disorders; RETINAL anatomy; DIAGNOSTIC imaging; MEDICAL screening; COMPUTER simulation; GRAPHICS processing units; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance; All Other Miscellaneous Ambulatory Health Care Services,Computational simulation; GPU programming; Human eye optics; Patient specific modeling and simulation; Ray tracing; Vision defects,"Abstract: We present a new technique for simulating retinal image formation by tracing a large number of rays from objects in three dimensions as they pass through the optic apparatus of the eye to objects. Simulating human optics is useful for understanding basic questions of vision science and for studying vision defects and their corrections. Because of the complexity of computing such simulations accurately, most previous efforts used simplified analytical models of the normal eye. This makes them less effective in modeling vision disorders associated with abnormal shapes of the ocular structures which are hard to be precisely represented by analytical surfaces. We have developed a computer simulator that can simulate ocular structures of arbitrary shapes, for instance represented by polygon meshes. Topographic and geometric measurements of the cornea, lens, and retina from keratometer or medical imaging data can be integrated for individualized examination. We utilize parallel processing using modern Graphics Processing Units (GPUs) to efficiently compute retinal images by tracing millions of rays. A stable retinal image can be generated within minutes. We simulated depth-of-field, accommodation, chromatic aberrations, as well as astigmatism and correction. We also show application of the technique in patient specific vision correction by incorporating geometric models of the orbit reconstructed from clinical medical images. [Copyright &y& Elsevier] Copyright of Computer Methods & Programs in Biomedicine is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95713419&site=ehost-live
712,Levator ani muscle volume and architecture in normal vs. muscle damage patients using 3D endovaginal ultrasound: a pilot study.,Qi Wei,International Urogynecology Journal,9373462,,Sep-22,,,1,,159395461,10.1007/s00192-022-05366-4,Springer Nature,Article,,3D endovaginal ultrasound; Avulsion; Levator ani; Muscle damage,"Introduction and hypothesis: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group.Methods: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group.Results: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group.Conclusions: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group. [ABSTRACT FROM AUTHOR] Copyright of International Urogynecology Journal is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159395461&site=ehost-live
713,"Posterior perineal translevator hernia: application of MRI, 3D ultrasound, and computerized modeling.",Qi Wei,International Urogynecology Journal,9373462,,Oct-18,29,10,1559,3,131820451,10.1007/s00192-018-3714-3,Springer Nature,Article,PERINEAL care; PELVIC floor; PELVIC surgery; ULTRASONIC imaging; MAGNETIC resonance imaging; DISEASES; Diagnostic Imaging Centers,Pelvic floor MRI; Pelvic floor ultrasound; Translevator hernia,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131820451&site=ehost-live
714,Uncertainty in Limb Configuration Makes Minimal Contribution to Errors Between Observed and Predicted Forces in a Musculoskeletal Model of the Rat Hindlimb.,Qi Wei,IEEE Transactions on Biomedical Engineering,189294,,Feb-18,65,2,469,8,127409149,10.1109/TBME.2017.2775598,IEEE,Article,MUSCULOSKELETAL system; LOCOMOTION; CALIBRATION; GROUND reaction forces (Biomechanics); ROBOTICS; MONTE Carlo method,Biomechanical simulation; Biomedical measurement; Force; Force measurement; Monte Carlo simulation; Muscles; musculoskeletal model; Predictive models; rat hindlimb; Rats,"Subject-specific musculoskeletal models are increasingly used in biomedical applications to predict endpoint forces due to muscle activation, matching predicted forces to experimentally observed forces at a specific limb configuration. However, it is difficult to precisely measure the limb configuration at which these forces are observed. The consequent uncertainty in limb configuration might contribute to errors in model predictions. We therefore evaluated how uncertainties in limb configuration measurement contributed to errors in force prediction, using data from in vivo measurements in the rat hindlimb. We used a data-driven approach to estimate the uncertainty in estimated limb configuration and then used this configuration uncertainty to evaluate the consequent uncertainty in force predictions, using Monte Carlo simulations. We used subject-specific models of joint structures (i.e., centers and axes of rotation) in order to estimate limb configurations for each animal. The standard deviation of the distribution of predicted force directions resulting from configuration uncertainty was small, ranging between 0.27° and 3.05° across muscles. For most muscles, this standard deviation was considerably smaller than the error between observed and predicted forces (between 0.57° and 70.96°), suggesting that uncertainty in limb configuration could not explain inaccuracies in model predictions. Instead, our results suggest that inaccuracies in muscle model parameters, most likely in parameters specifying muscle moment arms, are the main source of prediction errors by musculoskeletal models in the rat hindlimb. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Biomedical Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127409149&site=ehost-live
715,Zebrafish larvae heartbeat detection from body deformation in low resolution and low frequency video.,Qi Wei,Medical & Biological Engineering & Computing,1400118,,Dec-18,56,12,2353,13,133056270,10.1007/s11517-018-1863-7,Springer Nature,journal article,ZEBRA danio; FISH larvae; FISH genetics; HEART beat; DEFORMATIONS (Mechanics),Dense optical flow; Motion tracking; Multiresolution; Principal component analysis; Zebrafish heartbeat,"Zebrafish (Danio rerio) is a powerful animal model used in many areas of genetics and disease research. Despite its advantages for cardiac research, the heartbeat pattern of zebrafish larvae under different stress conditions is not well documented quantitatively. Several effective automated heartbeat detection methods have been developed to reduce the workload for larva heartbeat analysis. However, most require complex experimental setups and necessitate direct observation of the larva heart. In this paper, we propose the Zebrafish Heart Rate Automatic Method (Z-HRAM), which detects and tracks the heartbeats of immobilized, ventrally positioned zebrafish larvae without direct larva heart observation. Z-HRAM tracks localized larva body deformation that is highly correlated with heart movement. Multiresolution dense optical flow-based motion tracking and principal component analysis are used to identify heartbeats. Here, we present results of Z-HRAM on estimating heart rate from video recordings of seizure-induced larvae, which were of low resolution (1024 × 760) and low frame rate (3 to 4 fps). Heartbeats detected from Z-HRAM were shown to correlate reliably with those determined through corresponding electrocardiogram and manual video inspection. We conclude that Z-HRAM is a robust, computationally efficient, and easily applicable tool for studying larva cardiac function in general laboratory conditions. Graphical abstract Flowchart of the automatic zebrafish heartbeat detection. [ABSTRACT FROM AUTHOR] Copyright of Medical & Biological Engineering & Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133056270&site=ehost-live
716,An Attack Vector Taxonomy for Mobile Telephony Security Vulnerabilities.,Duminda Wijesekera,Computer (00189162),189162,,Apr-21,54,4,76,9,149806637,10.1109/MC.2021.3057059,IEEE,Article,TAXONOMY; SECURITY management; CELL phone systems; Wireless Telecommunications Carriers (except Satellite),,"A simplified cybersecurity threat matrix may provide a unifying way to define the security risk posed by current and future generations of mobile telephony. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149806637&site=ehost-live
717,An authorization model for multimedia digital libraries.,Duminda Wijesekera,International Journal on Digital Libraries,14325012,,2004,4,3,139,17,14910124,10.1007/s00799-004-0080-1,Springer Nature,Article,MULTIMEDIA systems; DIGITAL libraries; ACCESS control; METADATA; SEMANTICS; Libraries and Archives,Access control; Continuous media; Digital library; Metadata; Semantics,"In this paper we present ageneralized authorization modelfor multimedia digital libraries. Our aim is to support the enforcement of access control requirements of the original data sources without the need to create a new, unified model for the digital library. We integrate the three most widely used access control models (i.e., mandatory, discretionary, and role-based) within a single framework, allowing seamless accesses to data protected by these security models. In particular, we address the access control needs ofcontinuous media datawhile supporting quality of service (QoS) requirements and preserving operational semantics. The technical core of the paper focuses on the development ofmetadataand the correspondingmetastructureto represent authorization policies and QoS requirements and shows their applicabilty to continuous media. We define our security objects based on the Synchronized Multimedia Integration Language (SMIL), which controls multimedia presentations. Following the synchronization constructs <par> and <seq> of SMIL, we define a normal form for multimedia streams, calledSMIL normal form. SMIL normal form provides a syntax-independent representation of semantically equivalent multimedia data. SMIL normal form compositions are extended (decorated) with RDF statements, representing security and QoS metadata. Interpretation of these statements and, therefore, the authorization and QoS requirements of the decorated multimedia object are defined by the metastructure, represented as a DAML+OIL ontology. We propose the concept ofgeneralized subjectthat encompasses all access permissions of a given user regardless of the multiple permissions in different access control models. Finally, we develop methods to generate secure views for each generalized subject and retrieve them using a secure multimedia server. [ABSTRACT FROM AUTHOR] Copyright of International Journal on Digital Libraries is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14910124&site=ehost-live
718,An ontology-based distributed whiteboard to determine legal responses to online cyber attacks.,Duminda Wijesekera,Internet Research,10662243,,2006,16,5,475,16,24638211,10.1108/10662240610710969,Emerald Publishing Limited,Article,DATA protection; DECISION making; CYBERSPACE; COMPUTER crimes; COMPUTER systems; CYBERTERRORISM; LEGAL documents; INFORMATION services; PEACE officers; All Other Information Services; Computer systems design and related services (except video game design and development); Computer Systems Design Services,Decision support systems; Law; Systems theory; Worldwide web,"Purpose — This paper aims to assist investigators and attorneys addressing the legal aspects of cyber incidents, and allow them to determine the legality of a response to cyber attacks by using the Worldwide web securely. Design/methodology/approach — Develop a decision support legal whiteboard that graphically constructs legal arguments as a decision tree. The tree is constructed using a tree of questions and appending legal documents to substantiate the answers that are known to hold in anticipated legal challenges. Findings — The tool allows participating group of attorneys to meet in cyberspace in real time and construct a legal argument graphically by using a decision tree. They can construct sub-parts of the tree from their own legal domains. Because diverse legal domains use different nomenclatures, this tool provides the user the capability to index and search legal documents using a complex international legal ontology that goes beyond the traditional LexisNexis-like legal databases. This ontology itself can be created using the tool from distributed locations. Originality/value — This tool has been fine-tuned through numerous interviews with attorneys teaching and practicing in the area of cyber crime, cyber espionage, and military operations in cyberspace. It can be used to guide forensic experts and law enforcement personnel during their active responses and off-line examinations. [ABSTRACT FROM AUTHOR] Copyright of Internet Research is the property of Emerald Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24638211&site=ehost-live
719,Can You Trust Zero Trust?,Duminda Wijesekera,Computer (00189162),189162,,Aug-22,55,8,103,3,158333510,10.1109/MC.2022.3178813,IEEE,Article,TRUST; COMPUTER architecture,Computer architecture,"Developing and sustaining a ""zero-trust architecture"" is essentially impossible today. The concept is currently a moving target, and its meaning is in the eye of the beholder. One thing we know for certain is that it's a misnomer. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158333510&site=ehost-live
720,Detecting VoIP Floods Using the Hellinger Distance.,Duminda Wijesekera,IEEE Transactions on Parallel & Distributed Systems,10459219,,Jun-08,19,6,794,12,32522747,10.1109/TPDS.2007.70786,IEEE,Article,INTERNET telephony; COMPUTER network protocols; MARKET share; INTERNET; DISTRIBUTION (Probability theory); EXPERIMENTS; TRAFFIC flow; DIFFERENCES; DATA packeting; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; All Other Telecommunications,flooding attacks; Hellinger distance; VOIP,"Voice over IP (VoIP), also known as Internet telephony, is gaining market share rapidly and now competes favorably as one of the visible applications of the Internet. Nevertheless, being an application running over the TCP/IP suite, it is susceptible to flooding attacks. If flooded, as a time-sensitive service, VoIP may show noticeable service degradation and even encounter sudden service disruptions. Because multiple protocols are involved in a VoIP service and most of them are susceptible to flooding, an effective solution must be able to detect and overcome hybrid floods. As a solution, we offer the VoIP Flooding Detection System (vFDS)—an online statistical anomaly detection framework that generates alerts based on abnormal variations in a selected hybrid collection of traffic flows. It does so by viewing collections of related packet streams as evolving probability distributions and measuring abnormal variations in their relationships based on the Hellinger distance—a measure of variability between two probability distributions. Experimental results show that vFDS is fast and accurate in detecting flooding attacks, without noticeably increasing call setup times or introducing jitter into the voice streams. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=32522747&site=ehost-live
721,Modeling Human-in-the-Loop Security Analysis and Decision-Making Processes.,Duminda Wijesekera,IEEE Transactions on Software Engineering,985589,,Jan-14,40,2,154,13,94842622,10.1109/TSE.2014.2302433,IEEE,Article,MATHEMATICAL models of decision making; WORKFLOW management; SOFTWARE engineering; COMPUTER security; PRODUCTION engineering,Analytical models; Business; Formal methods; Formal specifications; information assurance; Object oriented modeling; process modeling; Runtime; Software; software engineering; statechart assertions; Unified modeling language; verification and validation,"This paper presents a novel application of computer-assisted formal methods for systematically specifying, documenting, statically and dynamically checking, and maintaining human-centered workflow processes. This approach provides for end-to-end verification and validation of process workflows, which is needed for process workflows that are intended for use in developing and maintaining high-integrity systems. We demonstrate the technical feasibility of our approach by applying it on the development of the US government's process workflow for implementing, certifying, and accrediting cross-domain computer security solutions. Our approach involves identifying human-in-the-loop decision points in the process activities and then modeling these via statechart assertions. We developed techniques to specify and enforce workflow hierarchies, which was a challenge due to the existence of concurrent activities within complex workflow processes. Some of the key advantages of our approach are: it results in development of a model that is executable, supporting both upfront and runtime checking of process-workflow requirements; aids comprehension and communication among stakeholders and process engineers; and provides for incorporating accountability and risk management into the engineering of process workflows. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94842622&site=ehost-live
722,Reasoning with advanced policy rules and its application to access control.,Duminda Wijesekera,International Journal on Digital Libraries,14325012,,2004,4,3,156,15,14910126,10.1007/s00799-004-0078-8,Springer Nature,Article,ACCESS control; COMPUTER security; INFORMATION storage & retrieval systems; ELECTRONIC information resources; DIGITAL libraries; Libraries and Archives,Access control; Obligations; Policies; Policy rule evaluation; Provisions,"This paper presents a formal framework to represent and manage advanced policy rules, which incorporate the notions ofprovisionandobligation. Provisions are those conditions that need to be satisfied or actions that must be performed by a user or an agent before a decision is rendered, while obligations are those conditions or actions that must be fulfilled by either the user or agent or by the system itself within a certain period of time after the decision. This paper proposes a specific formalism to express provisions and obligations within a policy and investigates a reasoning mechanism within this framework. A policy decision may be supported by more than one rule-based derivation, each associated with a potentially different set of provisions and obligations (called a global PO set). The reasoning mechanism can derive all the global PO sets for each specific policy decision and facilitates the selection of the best one based on numerical weights assigned to provisions and obligations as well as on semantic relationships among them. The formal results presented in the paper hold for many applications requiring the specification of policies, but this paper illustrates the use of the proposed policy framework in the security domain only. [ABSTRACT FROM AUTHOR] Copyright of International Journal on Digital Libraries is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14910126&site=ehost-live
723,Removing Permissions in the Flexible Authorization Framework.,Duminda Wijesekera,ACM Transactions on Database Systems,3625915,,Sep-03,28,3,209,21,10885768,10.1145/937598.937599,Association for Computing Machinery,Article,COMPUTER security; SECURITY systems; DATA protection; LOGIC programming; COMPUTER programming; ACCESS control; Computer systems design and related services (except video game design and development); Other Computer Related Services; Custom Computer Programming Services; Security Systems Services (except Locksmiths),Access control policy; authorization; logic programming,"The Flexible Authorization Framework (FAF) defined by Jajodia et al. [2001] provides a stratified-logic-programming-based framework for specifying access control policies that is expressive enough to specify many known access control policies. Although the original formulation of FAF indicated how rules could be added to or deleted from a FAF specification, it did not address the removal of access permissions from users. We present two options for removing permissions in FAF and provide details on the option which is representation independent. [ABSTRACT FROM AUTHOR] Copyright of ACM Transactions on Database Systems is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=10885768&site=ehost-live
724,Securing the ZigBee Protocol in the Smart Grid.,Duminda Wijesekera,Computer (00189162),189162,,Apr-12,45,4,92,0,74133618,10.1109/MC.2012.146,IEEE,Article,"COMPUTER network protocols; CYBERTERRORISM; COMPUTER security; SMART power grids; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Authentication; Educational institutions; formal methods; NIST; protocol verification; Protocols; security; smart grid; Smart grids; Zigbee,"The design-implement-fix process is sufficient for previously unknown attack vectors. However, engineers should use established knowledge, such as known attack patterns, in the analysis of security protocols prior to their acceptance and implementation. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=74133618&site=ehost-live
725,SPUTERS: An Integrated Traffic Surveillance and Emergency Response Architecture.,Duminda Wijesekera,Journal of Intelligent Transportation Systems,15472450,,Jan-Mar2005,9,1,11,12,16908858,10.1080/15472450590912538,Taylor & Francis Ltd,Article,"INFORMATION storage & retrieval systems; TRAFFIC accidents; TRAFFIC surveys; TRAFFIC accident investigation; EMERGENCY communication systems; EMERGENCY management; TRAFFIC engineering; COMMUNICATIONS industries; Wired Telecommunications Carriers; Wireless Telecommunications Carriers (except Satellite); Satellite Telecommunications; Telecommunications Resellers; All Other Telecommunications; Other federal protective services; Other provincial protective services; Other municipal protective services; Emergency and Other Relief Services; Other Justice, Public Order, and Safety Activities; Engineering Services",Incident Response; Privacy; Real-Time Multimedia; Security; Traffic Surveillance,"Secure Progressively Updatable Traffic Emergency Response System (SPUTERS) is a framework for collecting traffic-surveillance data in crash-prone areas of roadways. SPUTERS receives as input video, audio, and text-based data from an integrated collection of distributed cameras and sensors, both vehicle-borne and those embedded in the roadway infrastructure. Under nominal roadway operating conditions, the multimedia data are used to develop incident countermeasures. During degraded modes of roadway operation, SPUTERS generates progressive updates to create a shared situational awareness among the people who orchestrate responses to traffic incidents and emergencies. In addition to addressing performance-oriented quality-of-service requirements for interactive display of data on handheld devices, SPUTERS relies on role-based access control to safeguard the privacy of roadway users under surveillance, preventing privileged information collected and managed by the system from being leaked to unauthorized persons. In order to enhance security, SPUTERS uses symmetric key encryption to guard against the unauthorized alteration of the surveillance data. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent Transportation Systems is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=16908858&site=ehost-live
726,SS7 Over IP: Signaling Interworking Vulnerabilities.,Duminda Wijesekera,IEEE Network,8908044,,Nov/Dec2006,20,6,32,10,23194297,10.1109/MNET.2006.273119,IEEE,Article,SWITCHING systems (Telecommunication); INTERNETWORKING devices; INTERNET telephony signaling; TELEPHONE signaling; COMPUTER networks; DIGITAL telephone systems; INTERNET; COMPUTER network architectures; COMPUTER security; INTERNETWORKING; Computer Systems Design Services; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,,"The article discusses the solution on how to avoid exploitation of intersignaling potential provided by SIGTRAN, a signaling interface which provides seamless interconnectivity between public telephony and Internet Protocol telephony. The proposed solution is based on access control, signaling screening and detecting anomalous signaling. It argues that syntactic correctness, semantic validity of the signal content and the appropriateness of a particular signal in the context of earlier exchanged messages should be considered.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23194297&site=ehost-live
727,Tableaux for constructive concurrent dynamic logic,Duminda Wijesekera,Annals of Pure & Applied Logic,1680072,,Sep-05,135,3-Jan,1,72,18150453,10.1016/j.apal.2004.12.001,Elsevier B.V.,Article,"INFORMATION theory; COMPUTER science; LOGIC design; LOGIC circuits; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"Abstract: This is the first paper on constructive concurrent dynamic logic (CCDL). For the first time, either for concurrent or sequential dynamic logic, we give a satisfactory treatment of what statements are forced to be true by partial information about the underlying computer. Dynamic logic was developed by Pratt [V. Pratt, Semantical considerations on Floyd–Hoare logic, in: 17th Annual IEEE Symp. on Found. Comp. Sci., New York, 1976, pp. 109–121, V. Pratt, Applications of modal logic to programming, Studia Logica 39 (1980) 257–274] for nondeterministic sequential programs, and by Peleg [D. Peleg, Concurrent dynamic logic, Journal of the Association for Computing Machinery 34 (2) (1987), D. Peleg, Communication in concurrent dynamic logic, Journal of Computer and System Sciences 35 (1987)] for concurrent programs, for the purpose of proving properties of programs such as correctness. Here we define what it means for a dynamic logic formula to be forced to be true knowing only partial information about the results of assignments and tests. This informal CCDL semantics is formalized by intuitionistic Kripke frames modeling this partial information, and each such frame is interpreted as an idealized concurrent machine (a concurrent transition system). In CCDL, proofs and deductions are -height, -branching, well-founded labeled subtrees of . These are a generalization of the signed tableaux of Nerode [A. Nerode, Some lectures in modal logic, Technical Report, M.S.I. Cornell University, 1989, CIME Logic and Computer Science Montecatini Volume, Springer-Verlag Lecture Notes, 1990, A. Nerode, Some lectures in intuitionistic logic, Technical Report, M.S.I. Cornell University, 1988, Marktoberdorf Logic and Computation NATO Summer School Volume, NATO Science Series, 1990 (in press)] stemming from the prefix tableaux of Fitting [M.C. Fitting, Proof Methods for Modal and Intuitionistic Logic, Reidel, 1983]. We demonstrate the correctness of our tableau proofs, define consistency properties, prove that consistency properties yield models, construct systematic tableaux, prove that systematic tableaux yield a consistency property, and conclude that CCDL is complete. This infinitary semantics and proof procedure will be the primary guide for defining, in a sequel, the correct finitary CCDL (FCCDL) based on induction principles. FCCDL is suitable for implementation in constructive logic software systems such as Constable’s NUPRL or Huet-Coquand’s CONSTRUCTIONS. Our goal is to develop a constructive logic programming tool for specification and modular verification of programs in any imperative concurrent language, and for the extraction of concurrent programs from constructive proofs. Subsequent papers will introduce analogous logics for declarative and functional concurrent languages. [Copyright &y& Elsevier] Copyright of Annals of Pure & Applied Logic is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18150453&site=ehost-live
728,A Comparison Study of Validity Indices on Swarm-Intelligence-Based Clustering.,Jie Xu,"IEEE Transactions on Systems, Man & Cybernetics: Part B",10834419,,Aug-12,42,4,1243,14,77875159,10.1109/TSMCB.2012.2188509,IEEE,Article,COMPARATIVE studies; CLUSTER analysis (Statistics); SWARM intelligence; PARTICLE swarm optimization; PARALLEL algorithms; DIFFERENTIAL evolution,Clustering; Clustering algorithms; Context; differential evolution (DE); Encoding; Indexes; Particle swarm optimization; particle swarm optimization (PSO); Partitioning algorithms; swarm intelligence; validity index; Vectors,"Swarm intelligence has emerged as a worthwhile class of clustering methods due to its convenient implementation, parallel capability, ability to avoid local minima, and other advantages. In such applications, clustering validity indices usually operate as fitness functions to evaluate the qualities of the obtained clusters. However, as the validity indices are usually data dependent and are designed to address certain types of data, the selection of different indices as the fitness functions may critically affect cluster quality. Here, we compare the performances of eight well-known and widely used clustering validity indices, namely, the Caliński–Harabasz index, the CS index, the Davies–Bouldin index, the Dunn index with two of its generalized versions, the I index, and the silhouette statistic index, on both synthetic and real data sets in the framework of differential-evolution–particle-swarm-optimization (DEPSO)-based clustering. DEPSO is a hybrid evolutionary algorithm of the stochastic optimization approach (differential evolution) and the swarm intelligence method (particle swarm optimization) that further increases the search capability and achieves higher flexibility in exploring the problem space. According to the experimental results, we find that the silhouette statistic index stands out in most of the data sets that we examined. Meanwhile, we suggest that users reach their conclusions not just based on only one index, but after considering the results of several indices to achieve reliable clustering structures. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part B is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77875159&site=ehost-live
729,An Optimal Computing Budget Allocation Tree Policy for Monte Carlo Tree Search.,Jie Xu,IEEE Transactions on Automatic Control,189286,,Jun-22,67,6,2685,15,157192214,10.1109/TAC.2021.3088792,IEEE,Article,"STOCHASTIC control theory; MONTE Carlo method; BUDGET cuts; TREES; Flower, Nursery Stock, and Florists' Supplies Merchant Wholesalers",Dynamic programming; Games; Heuristic algorithms; Machine learning; Machine learning algorithms; Monte Carlo methods; Monte Carlo tree search (MCTS); optimization algorithms; Search problems; Space exploration; stochastic optimal control,"We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to widely used upper confidence bound (UCB) tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation tradeoff when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption. Numerical experiments demonstrate the efficiency of our algorithm in selecting the best action at the root. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157192214&site=ehost-live
730,Multi-fidelity sampling for efficient simulation-based decision making in manufacturing management.,Jie Xu,IISE Transactions,24725854,,Jul-19,51,7,792,14,136690083,10.1080/24725854.2019.1576951,Taylor & Francis Ltd,Article,"DECISION making; PRODUCTION planning; PRODUCTION management (Manufacturing); RESOURCE allocation; ROBUST control; Process, Physical Distribution, and Logistics Consulting Services",convergence rate; multi-fidelity models; optimal sampling; production planning; resource allocation; robust manufacturing; Simulation-based decision making,"Today's manufacturers operate in highly dynamic and uncertain market environments. Process-level disturbances present further challenges. Consequently, it is of strategic importance for a manufacturing company to develop robust manufacturing capabilities that can quickly adapt to varying customer demands in the presence of external and internal uncertainty and stochasticity. Discrete-event simulations have been used by manufacturing managers to conduct ""look-ahead"" analysis and optimize resource allocation and production plan. However, simulations of complex manufacturing systems are time-consuming. Therefore, there is a great need for a highly efficient procedure to allocate a limited number of simulations to improve a system's performance. In this article, we propose a multi-fidelity sampling algorithm that greatly increases the efficiency of simulation-based robust manufacturing management by utilizing ordinal estimates obtained from a low-fidelity, but fast, approximate model. We show that the multi-fidelity optimal sampling policy minimizes the expected optimality gap of the selected solution, and thus optimally uses a limited simulation budget. We derive an upper bound for the multi-fidelity sampling policy and compare it with other sampling policies to illustrate the efficiency improvement. We demonstrate its computational efficiency improvement and validate the convergence results derived using both benchmark test functions and two robust manufacturing management case studies. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136690083&site=ehost-live
731,Optimal selection of media vehicles using customer databases,Jie Xu,Expert Systems with Applications,9574174,,Dec-12,39,17,13035,11,78431424,10.1016/j.eswa.2012.05.095,Elsevier B.V.,Article,DATABASES; RANKING (Statistics); MARKETING; RATE of return; DIRECT costing; DECISION making; PROFIT maximization; CONSTRAINT satisfaction; Marketing Consulting Services,Advertising; Depth of purchase; Dynamic programing; Internet marketing; Mailing list; Media vehicle; Optimization; Revenue-to-cost ratio,"Abstract: This paper investigates the problem where an organization must select among multiple media vehicles for a marketing campaign, and determine how many names from each vehicle to impress (the contact depth). The organization can estimate the return from contacting each prospective customer, and this return decreases as depth increases. Different vehicles have different marginal costs per impression, and may have minimum-spend requirements or activation costs to use the vehicle. Decisions are to be made to maximize profit subject to a constraint on the total amount spent. We proposed an optimization model and two computationally efficient methods that often lead to global optimal solutions under practical assumptions. The model is illustrated with two data sets. [Copyright &y& Elsevier] Copyright of Expert Systems with Applications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=78431424&site=ehost-live
732,Stochastic Control Framework for Determining Feasible Alternatives in Sampling Allocation.,Jie Xu,IEEE Transactions on Automatic Control,189286,,Jun-20,65,6,2647,7,143576746,10.1109/TAC.2019.2942005,IEEE,Article,DYNAMIC programming; STATISTICAL decision making; GAUSSIAN distribution; NETWORK hubs,Approximate dynamic programming (ADP); Bayes methods; Dynamic programming; Dynamic scheduling; feasibility determination (FD); Gaussian distribution; multiarmed bandit (MAB); Optimization; ranking and selection (R&S); Resource management; Standards; stochastic control problem (SCP); value function approximation (VFA),"We formulate the optimal dynamic sampling allocation decision problem for feasibility determination as a stochastic control problem in a Bayesian setting. This new formulation addresses the limitations of previous static optimization formulations. In an approximate dynamic programming paradigm, we propose an approximately optimal allocation policy that maximizes a single feature of the value function one step ahead. Numerical results demonstrate the efficiency of the proposed method. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143576746&site=ehost-live
733,VMSA: a performance preserving online VM splitting and placement algorithm in dynamic cloud environments.,Jie Xu,Journal of Supercomputing,9208542,,Aug-16,72,8,3169,25,117321475,10.1007/s11227-015-1590-x,Springer Nature,Article,"CLOUD computing; ONLINE algorithms; VIRTUAL machine systems; SOFTWARE as a service; CLOUD storage; VIRTUAL storage (Computer science); ONLINE data processing; Data Processing, Hosting, and Related Services",Cloud computing; Dynamic resource demand; Online algorithm; Requirement splitting; Virtual machine placement,"Server consolidation schemes whereby each server is replaced with a virtual machine (VM) and multiple such VMs are run on a single physical server can reduce the number of physical servers needed, and in turn, both the cost and energy consumption in data centers. However, existing schemes have not fully exploited the flexibility in the usage and allocation of virtualization resources, so as to allow one application originally deployed on a single large VM (LVM) to be split and hosted by multiple smaller VMs (SVM). Using multiple SVMs instead of an LVM enables resource allocation at a smaller granularity and thus may further increase the utilization and reduce the number of physical servers. However, a major challenge to overcome when deploying multiple SVMs for one application is to preserve the performance of the application in terms of response delay. In this paper, we show through theoretical analysis and experiments that in order to preserve the performance of the application, one needs to allocate sufficient resources to each SVM, and the total amount of resources required by all the SVMs will exceed that required by the LVM. Nevertheless, we also show that by using the proposed heuristic algorithm called VM splitting and assignment (VMSA), we can substantially improve the utilization and reduce the number of physical servers. [ABSTRACT FROM AUTHOR] Copyright of Journal of Supercomputing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117321475&site=ehost-live
734,Dissecting Latency in 360° Video Camera Sensing Systems †.,Zhisheng Yan,Sensors (14248220),14248220,,Aug-22,22,16,6001,25,158948175,10.3390/s22166001,MDPI,Article,CAMCORDERS; COMPUTER systems; WIRELESS sensor networks; Audio and Video Equipment Manufacturing; Computer Systems Design Services; Computer systems design and related services (except video game design and development),360° video camera sensing; latency; measurement study; wireless multimedia sensor networks,"360° video camera sensing is an increasingly popular technology. Compared with traditional 2D video systems, it is challenging to ensure the viewing experience in 360° video camera sensing because the massive omnidirectional data introduce adverse effects on start-up delay, event-to-eye delay, and frame rate. Therefore, understanding the time consumption of computing tasks in 360° video camera sensing becomes the prerequisite to improving the system's delay performance and viewing experience. Despite the prior measurement studies on 360° video systems, none of them delves into the system pipeline and dissects the latency at the task level. In this paper, we perform the first in-depth measurement study of task-level time consumption for 360° video camera sensing. We start with identifying the subtle relationship between the three delay metrics and the time consumption breakdown across the system computing task. Next, we develop an open research prototype Zeus to characterize this relationship in various realistic usage scenarios. Our measurement of task-level time consumption demonstrates the importance of the camera CPU-GPU transfer and the server initialization, as well as the negligible effect of 360° video stitching on the delay metrics. Finally, we compare Zeus with a commercial system to validate that our results are representative and can be used to improve today's 360° video camera sensing systems. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158948175&site=ehost-live
735,Embedding Pose Information for Multiview Vehicle Model Recognition.,Zhisheng Yan,IEEE Transactions on Circuits & Systems for Video Technology,10518215,,Aug-22,32,8,5467,14,158333584,10.1109/TCSVT.2022.3151116,IEEE,Article,POSE estimation (Computer vision); VEHICLE models; CONVOLUTIONAL neural networks; COMPUTER vision; CLASSIFICATION algorithms; VISUAL fields,Computational modeling; Convolutional neural network; Data mining; Feature extraction; fine-grained classification; Integrated circuit modeling; Measurement; pose estimation; scale-aware features; Task analysis; Urban areas; vehicle model recognition,"Vehicle model recognition is a typical fine-grained classification task that has a wide range of application prospects in safe cities and constitutes a research hotspot in the field of computer vision. Vehicles in images can appear at various angles, resulting in large differences in appearance. The existence of “multiviews” renders vehicle model recognition challenging. Recent research on vehicle model recognition has not fully explored the pose information of vehicles in different images, resulting in low model performance. In this study, we use vehicle pose information to solve the multiview vehicle model recognition (MV-VMR) problem and design a convolutional neural network (CNN) model with embedded vehicle pose information, known as the embedding pose CNN (EP-CNN). The proposed model includes two subnetworks: the pose estimation subnetwork (PE-SubNet) and vehicle model classification subnetwork (VMC-SubNet). PE-SubNet extracts the vehicle pose information, including the pose features and vehicle viewpoint. In VMC-SubNet, considering the scale variation of vehicles, an improved squeeze-and-excitation (SE) block, named the MultiSE block is implemented. We embed the vehicle viewpoint into the MultiSE block, which reweighs each channel such that the extracted features elicit different responses to different viewpoints. Subsequently, the pose features and classification features are integrated for classification. Experiments are conducted on the benchmark CompCars web-nature and Stanford Cars datasets. The results demonstrate that the proposed EP-CNN method can achieve higher recognition accuracy than most classic CNN models and several state-of-the-art fine-grained vehicle model classification algorithms. Code has been made available at: https://github.com/HFUT-CV/EP-CNN. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Circuits & Systems for Video Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158333584&site=ehost-live
736,Targeted delivery of lopinavir to HIV reservoirs in the mesenteric lymphatic system by lipophilic ester prodrug approach.,Lei Yang,Journal of Controlled Release,1683659,,Jan-21,329,,1077,13,148861576,10.1016/j.jconrel.2020.10.036,Elsevier B.V.,Article,PRODRUGS; LYMPHATICS; RESERVOIRS; ESTERS; ANTIRETROVIRAL agents; LYMPH nodes; Water and Sewer Line and Related Structures Construction,Chylomicrons; HIV reservoirs; Intestinal lymphatic transport; Lipophilic ester prodrug approach; Lopinavir; Mesenteric lymphatic system,"The combined antiretroviral therapy (cART) can efficiently suppress HIV replication, but the cessation of cART usually results in viral rebound, mostly due to the presence of viral reservoirs. The mesenteric lymphatic system, including mesenteric lymph nodes (MLNs), is an important viral reservoir into which antiretroviral drugs poorly penetrate. In this work, we proposed a novel lipophilic ester prodrug approach, combined with oral lipid-based formulation, to efficiently deliver lopinavir (LPV) to the mesenteric lymph and MLNs. A series of prodrugs was designed using an in-silico model for prediction of affinity to chylomicrons (CMs), and then synthesized. The potential for mesenteric lymphatic targeting and bioconversion to LPV in physiologically relevant media was assessed in vitro and ex vivo. Subsequently, LPV and selected prodrug candidates were evaluated for their in vivo pharmacokinetics and biodistribution in rats. Oral co-administration of lipids alone could not facilitate the delivery of unmodified LPV to the mesenteric lymphatic system and resulted in undetectable levels of LPV in these tissues. However, a combination of the lipophilic prodrug approach with lipid-based formulation resulted in efficient targeting of LPV to HIV reservoirs in mesenteric lymph and MLNs. The maximum levels of LPV in mesenteric lymph were 1.6- and 16.9-fold higher than protein binding-adjusted IC 90 (PA-IC 90) of LPV for HIV-1 (140 ng/mL) following oral administration of simple alkyl ester prodrug and activated ester prodrug, respectively. Moreover, the concentrations of LPV in MLNs were 1.1- and 7.2-fold higher than PA-IC 90 following administration of simple alkyl ester prodrug and activated ester prodrug, respectively. Furthermore, the bioavailability of LPV was also substantially increased following oral administration of activated ester prodrug compared to unmodified LPV. This approach, especially if can be translated to other antiretroviral drugs, has potential for reducing the size of HIV reservoirs within the mesenteric lymphatic system. Unlabelled Image • Antiretrovirals fail to penetrate into HIV reservoirs in mesenteric lymphatic system. • Delivery of prodrugs and lopinavir to HIV reservoirs in lymphatic system. • Activated ester prodrug approach achieved the most efficient delivery of lopinavir. • This approach also improved the systemic exposure to lopinavir. [ABSTRACT FROM AUTHOR] Copyright of Journal of Controlled Release is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148861576&site=ehost-live
737,Real-Time Task Scheduling for Machine Perception in Intelligent Cyber-Physical Systems.,Shuochao Yao,IEEE Transactions on Computers,189340,,Aug-22,71,8,1770,14,157931348,10.1109/TC.2021.3106496,IEEE,Article,"CYBER physical systems; ARTIFICIAL intelligence; ARTIFICIAL neural networks; PARTITIONS (Building); RESOURCE allocation; NVIDIA Corp.; Other Building Finishing Contractors; Showcase, Partition, Shelving, and Locker Manufacturing",algorithmic priority inversion; Computer architecture; Cyber-physical systems; cyber-physical systems (CPS); Distance measurement; machine intelligence; Neural networks; Pipelines; Real-time scheduling; Real-time systems; Resource management,"This paper explores criticality-based real-time scheduling of neural-network-based machine inference pipelines in cyber-physical systems (CPS) to mitigate the effect of algorithmic priority inversion. We specifically focus on the perception subsystem, an important subsystem feeding other components (e.g., planning and control). In general, priority inversion occurs in real-time systems when computations that are of lower priority are performed together with or ahead of those that are of higher priority. In current machine perception software, significant priority inversion occurs because resource allocation to the underlying neural network models does not differentiate between critical and less critical data within a scene. To remedy this problem, in recent work, we proposed an architecture to partition the input data into regions of different criticality, then formulated a utility-based optimization problem to batch and schedule their processing in a manner that maximizes confidence in perception results, subject to criticality-based time constraints. This journal extension matures the work in several directions: (i) We extend confidence maximization to a generalized utility optimization formulation that accounts for criticality in the utility function itself, offering finer-grained control over resource allocation within the perception pipeline; (ii) we further instantiate and compare two different criticality metrics (distance-based and relative velocity-based) to understand their relative advantages; and (iii) we explore the limitations of the approach, specifically how inaccuracies in criticality-based attention cueing affect performance. All experiments are conducted on the NVIDIA Jetson AGX Xavier platform with a real-world driving dataset. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157931348&site=ehost-live
738,A Practical Downlink NOMA Scheme for Wireless LANs.,Kai Zeng,IEEE Transactions on Communications,906778,,Apr-20,68,4,2236,15,143316039,10.1109/TCOMM.2020.2965520,IEEE,Article,WIRELESS LANs; CHANNEL estimation; ALGORITHMS; CELL analysis,Antennas; Downlink; experimentation; Interference; NOMA; Receivers; Resource management; Silicon carbide; successive interference cancellation (SIC); WLAN,"Non-orthogonal multiple access (NOMA) has emerged as a new multiple access paradigm for wireless networks. Although many results have been produced for NOMA, most of them are limited to theoretical exploration and performance analysis in cellular networks. Very limited progress has been made so far in the design of practical NOMA schemes for wireless local area networks (WLANs). In this paper, we propose a practical downlink NOMA scheme for WLANs and evaluate its performance in real-world wireless environments. Our NOMA scheme has three key components: precoder design, user grouping, and successive interference cancellation (SIC). On the transmitter side, we first formulate the precoding design problem as an optimization problem and then devise an efficient algorithm to construct precoders for downlink NOMA transmissions. We further propose a lightweight user grouping algorithm to ensure the success of SIC at the receivers. On the receiver side, we propose a new SIC method to decode the desired signal in the presence of strong interference. In contrast to existing SIC methods, our SIC method does not require channel estimation to decode the signals, thereby improving its resilience to interference. We have built a prototype of the proposed NOMA scheme on a wireless testbed. Experimental results show that, compared to orthogonal multiple access (OMA), the proposed NOMA scheme can significantly improve the weak user’s date rate (93% on average) and considerably improve WLAN’s weighted sum rate (36% on average). [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Communications is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143316039&site=ehost-live
739,Delay Analysis of Physical-Layer Key Generation in Dynamic Roadside-to-Vehicle Networks.,Kai Zeng,IEEE Transactions on Vehicular Technology,189545,,Mar-17,66,3,2526,10,121854216,10.1109/TVT.2016.2582853,IEEE,Article,VEHICULAR ad hoc networks; WIRELESS communications; MARKOV processes; SIGNAL quantization; POISSON distribution; SAFETY; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,Channel probing; delay analysis; Delays; dynamic wireless networks; Physical layer; physical layer key generation; Probes; roadside unit; Vehicle dynamics; Vehicles; vehicular networks; Wireless networks,"Secret key generation by extracting the shared randomness in a wireless fading channel is a promising way to ensure wireless communication security. Previous studies only consider key generation in static networks, but real-world key establishments are usually dynamic. In this paper, for the first time, we investigate the pairwise key generation in dynamic wireless networks with a center node and random arrival users (e.g., roadside units (RSUs) with vehicles). We establish the key generation model for these kinds of networks. We propose a method based on discrete Markov chain to calculate the average time a user will spend on waiting and completing the key generation, called average key generation delay (AKGD). Our method can tackle both serial and parallel key generation scheduling under various conditions. We propose a novel scheduling method, which exploits wireless broadcast characteristic to reduce AKGD and probing energy. We conduct extensive simulations to show the effectiveness of our model and method. The analytical and simulation results match each other. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Vehicular Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121854216&site=ehost-live
740,Friendly spectrum jamming against MIMO eavesdropping.,Kai Zeng,Wireless Networks (10220038),10220038,,Aug-22,28,6,2437,17,157629771,10.1007/s11276-022-02967-1,Springer Nature,Article,INDEPENDENT component analysis; WIRELESS communications; EAVESDROPPING; AMPLITUDE modulation; PHASE modulation; RADAR interference; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Energy modulation; Friendly jamming; MIMO technology,"Friendly spectrum jamming is a flexible scheme to establish secure communications among heterogeneous wireless devices without the need of encryption. Previous works have indicated that this scheme however has weak security strength against multiple antenna eavesdropper in today's wireless communication systems, which limits its wide applicability. To tackle this challenge, we propose a novel modulation method, called energy modulation. The basic idea of our method is to keep the secrecy of the channel state information in modulation, so as to bring high uncertainty to the MIMO's separation and the eavesdropper's decoding. As a result, the security strength of friendly jamming notably increases facing multiple antenna eavesdropper. To demonstrate the effectiveness of our method, we perform independent component analysis to decouple the components of the measured signals with maximum likelihood separation. We find that our solution dramatically decreases the eavesdropper's partial information and has much less bits being compromised comparing with common amplitude and phase modulation. [ABSTRACT FROM AUTHOR] Copyright of Wireless Networks (10220038) is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157629771&site=ehost-live
741,Impact of Jamming Attacks on Vehicular Cooperative Adaptive Cruise Control Systems.,Kai Zeng,IEEE Transactions on Vehicular Technology,189545,,Nov-20,69,11,12679,15,147041807,10.1109/TVT.2020.3030251,IEEE,Article,MONTE Carlo method; ADAPTIVE control systems; WIRELESS channels; CRUISE control; WIRELESS communications; EQUATIONS of state; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Motor Vehicle Electrical and Electronic Equipment Manufacturing,Cooperative adaptive cruise control (CACC) security; Couplings; Jamming; Mathematical model; Numerical stability; Safety; Stability analysis; string stability; vehicle-to-vehicle (v2v) communication; Wireless communication; wireless jamming attacks,"Cooperative Adaptive Cruise Control (CACC) is considered as a key enabling technology to automatically regulate the inter-vehicle distances in a vehicle string, and improve the traffic throughput efficiency. In the existing CACC systems, the coupling between wireless communication uncertainty, and system states is not well modeled. In this paper, we integrate the jamming attacks, and wireless channel fading effects into the CACC state space equations such that it effectively captures the coupling impact. Then, we propose a novel time domain approach to analyze the mean string stability (MSS) of such a model. Based on the proposed model, we analyze the impact of the jammer's location on the string stability. We derive a sufficient condition for the packet successful delivery probability which indicates that the jammer has a higher probability to destabilize the string when it is closer to the first vehicle following the lead vehicle. We also propose a methodology to compute the upper, and lower bounds of the inter-vehicle distance trajectories between the lead vehicle, and its follower. Furthermore, string safety is investigated by numerically estimating the collision probability across the string. We conduct comprehensive Monte Carlo simulations to evaluate the stability, and safety of the string in various scenarios. We identify that string stability, and safety are highly influenced by the jamming attacks signal, and jammer's location. We show the consistency between the main results achieved by MSS analysis, and the Monte Carlo simulations. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Vehicular Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147041807&site=ehost-live
742,Physical layer key generation in wireless networks: challenges and opportunities.,Kai Zeng,IEEE Communications Magazine,1636804,,Jun-15,53,6,33,7,103222661,10.1109/MCOM.2015.7120014,IEEE,Article,WIRELESS channels; RECIPROCITY theorems; WIRELESS communications; TELECOMMUNICATION channels; INTERNET of things; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Communication system security; Jamming; Network security; Physical layer; Wireless communication; Wireless sensor networks,"Physical layer key generation that exploits reciprocity and randomness of wireless fading channels has attracted considerable research attention in recent years. Although theoretical study has shown its potential to generate information- theoretic secure keys, great challenges remain when transforming the theory into practice. This article provides an overview of the physical layer key generation process and discusses its practical challenges. Different passive and active attacks are analyzed and evaluated through numerical study. A new key generation scheme using random probing signals, and combining user generated randomness and channel randomness, is introduced as a countermeasure against active attacks. The numerical results show that the proposed scheme achieves higher security strength than existing schemes using constant probing signals under active attacks. Future research topics on physical layer key generation are discussed. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Communications Magazine is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103222661&site=ehost-live
743,Physical layer multi-user key generation in wireless networks.,Kai Zeng,Wireless Networks (10220038),10220038,,May-18,24,4,1043,12,128888434,10.1007/s11276-016-1389-6,Springer Nature,Article,WIRELESS sensor networks; MULTIUSER computer systems; WIRELESS communications; INTERFERENCE (Telecommunication); WIRELESS sensor nodes; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Lower bound; Multiple users; Parallel probing; Physical layer key generation; Waiting time,"Secret key generation by extracting the shared randomness in the wireless fading channel from physical layer is an interesting topic of practical value. Previous works have focused on the study of physical layer key generation with two nodes from the view point of key generation rate (KGR). Information theoretic limits and the KGRs in implementation have been derived. However, in real-world applications, the physical layer key generation problem involving multiple nodes is the common case, which lacks sufficient study so far. Multi-node case differs from two-node case in that there are two more important considerations: (1) the trade-off between KGR and probing efficiency at individual node pair; (2) channel probing schedule among multiple node pairs. This paper aims at minimizing the <italic>Overall Waiting Time of physical layer key generation with multiple users</italic> (shorten as OWT) through the optimization of probing rates at individual node pair and channel probing schedule. The theoretical lower bound of OWT is derived first, then a practical method (MUKEM) is proposed to compute reasonable probing rates and channel probing schedule for multiple node pairs to obtain a short OWT. Simulations are conducted to evaluate the effectiveness of our method. The results show that 70 % of OWT can be reduced by using our method comparing with one-by-one key generations; while it is only about 8 % longer than the lower bound of OWT. [ABSTRACT FROM AUTHOR] Copyright of Wireless Networks (10220038) is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128888434&site=ehost-live
744,Predicting locality phases for dynamic memory optimization,Yutao Zhong,Journal of Parallel & Distributed Computing,7437315,,Jul-07,67,7,783,14,25255293,10.1016/j.jpdc.2007.01.010,Academic Press Inc.,Article,DIGITAL signal processing; COMPUTER storage devices; DIGITAL electronics; COMPUTER input-output equipment; Computer Terminal and Other Computer Peripheral Equipment Manufacturing; Computer and peripheral equipment manufacturing; Computer Storage Device Manufacturing,Dynamic optimization; Locality analysis and optimization; Phase hierarchy; Program phase prediction; Reconfigurable architecture,"Abstract: Dynamic data, cache, and memory adaptation can significantly improve program performance when they are applied on long continuous phases of execution that have dynamic but predictable locality. To support phase-based adaptation, this paper defines the concept of locality phases and describes a four-component analysis technique. Locality-based phase detection uses locality analysis and signal processing techniques to identify phases from the data access trace of a program; frequency-based phase marking inserts code markers that mark phases in all executions of the program; phase hierarchy construction identifies the structure of multiple phases; and phase-sequence prediction predicts the phase sequence from program input parameters. The paper shows the accuracy and the granularity of phase and phase-sequence prediction as well as its uses in dynamic data packing, memory remapping, and cache resizing. [Copyright &y& Elsevier] Copyright of Journal of Parallel & Distributed Computing is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=25255293&site=ehost-live